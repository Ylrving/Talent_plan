title,publisher,citation,abstract,keywords,author_university
PATHWISE COORDINATE OPTIMIZATION FOR SPARSE LEARNING: ALGORITHM AND THEORY,ANNALS OF STATISTICS,37,"The pathwise coordinate optimization is one of the most important computational frameworks for high dimensional convex and nonconvex sparse learning problems. It differs from the classical coordinate optimization algorithms in three salient features: warm start initialization, active set updating and strong rule for coordinate preselection. Such a complex algorithmic structure grants superior empirical performance, but also poses significant challenge to theoretical analysis. To tackle this long lasting problem, we develop a new theory showing that these three features play pivotal roles in guaranteeing the outstanding statistical and computational performance of the pathwise coordinate optimization framework. Particularly, we analyze the existing pathwise coordinate optimization algorithms and provide new theoretical insights into them. The obtained insights further motivate the development of several modifications to improve the pathwise coordinate optimization framework, which guarantees linear convergence to a unique sparse local optimum with optimal statistical properties in parameter estimation and support recovery. This is the first result on the computational and statistical guarantees of the pathwise coordinate optimization framework in high dimensions. Thorough numerical experiments are provided to support our theory.","Nonconvex sparse learning,pathwise coordinate optimization,global linear convergence,optimal statistical rates of convergence,oracle property,active set,strong rule","Zhao, Tuo@University System of Georgia@Georgia Institute of Technology::Liu, Han@Princeton University::Zhang, Tong@Tencent"
CONDITIONAL MEAN AND QUANTILE DEPENDENCE TESTING IN HIGH DIMENSION,ANNALS OF STATISTICS,31,"Motivated by applications in biological science, we propose a novel test to assess the conditional mean dependence of a response variable on a large number of covariates. Our procedure is built on the martingale difference divergence recently proposed in Shao and Zhang [J. Amer. Statist. Assoc. 109 (2014) 1302-1318], and it is able to detect certain type of departure from the null hypothesis of conditional mean independence without making any specific model assumptions. Theoretically, we establish the asymptotic normality of the proposed test statistic under suitable assumption on the eigenvalues of a Hermitian operator, which is constructed based on the characteristic function of the covariates. These conditions can be simplified under banded dependence structure on the covariates or Gaussian design. To account for heterogeneity within the data, we further develop a testing procedure for conditional quantile independence at a given quantile level and provide an asymptotic justification. Empirically, our test of conditional mean independence delivers comparable results to the competitor, which was constructed under the linear model framework, when the underlying model is linear. It significantly outperforms the competitor when the conditional mean admits a nonlinear form.","Large-p-small-n,martingale difference divergence,simultaneous test,U-statistics","Zhang, Xianyang@Texas A&M University College Station@Texas A&M University System::Yao, Shun@University of Illinois Urbana-Champaign@University of Illinois System::Shao, Xiaofeng@University of Illinois Urbana-Champaign@University of Illinois System"
HIGH-DIMENSIONAL ASYMPTOTICS OF PREDICTION: RIDGE REGRESSION AND CLASSIFICATION,ANNALS OF STATISTICS,61,"We provide a unified analysis of the predictive risk of ridge regression and regularized discriminant analysis in a dense random effects model. We work in a high-dimensional asymptotic regime where p, n -> infinity and p/n -> gamma > 0, and allow for arbitrary covariance among the features. For both methods, we provide an explicit and efficiently computable expression for the limiting predictive risk, which depends only on the spectrum of the feature-covariance matrix, the signal strength and the aspect ratio.. Especially in the case of regularized discriminant analysis, we find that predictive accuracy has a nuanced dependence on the eigenvalue distribution of the covariance matrix, suggesting that analyses based on the operator norm of the covariance matrix may not be sharp. Our results also uncover an exact inverse relation between the limiting predictive risk and the limiting estimation risk in high-dimensional linear models. The analysis builds on recent advances in random matrix theory.","High-dimensional asymptotics,ridge regression,regularized discriminant analysis,prediction error,random matrix theory","Dobriban, Edgar@University of Pennsylvania::Wager, Stefan@Stanford University"
TESTING INDEPENDENCE IN HIGH DIMENSIONS WITH SUMS OF RANK CORRELATIONS,ANNALS OF STATISTICS,34,"We treat the problem of testing independence between m continuous variables when m can be larger than the available sample size n. We consider three types of test statistics that are constructed as sums or sums of squares of pairwise rank correlations. In the asymptotic regime where both m and n tend to infinity, a martingale central limit theorem is applied to show that the null distributions of these statistics converge to Gaussian limits, which are valid with no specific distributional or moment assumptions on the data. Using the framework of U-statistics, our result covers a variety of rank correlations including Kendall's tau and a dominating term of Spearman's rank correlation coefficient (rho), but also degenerate U-statistics such as Hoeffding's D, or the tau* of Bergsma and Dassios [Bernoulli 20 (2014) 1006-1028]. As in the classical theory for U-statistics, the test statistics need to be scaled differently when the rank correlations used to construct them are degenerate U-statistics. The power of the considered tests is explored in rate-optimality theory under a Gaussian equicorrelation alternative as well as in numerical experiments for specific cases of more general alternatives.","High-dimensional statistics,independence,U-statistics,minimax optimality,rank correlations","Leung, Dennis@Chinese University of Hong Kong::Drton, Mathias@University of Washington@University of Washington Seattle"
LOCAL M-ESTIMATION WITH DISCONTINUOUS CRITERION FOR DEPENDENT AND LIMITED OBSERVATIONS,ANNALS OF STATISTICS,44,"We examine the asymptotic properties of local M-estimators under three sets of high-level conditions. These conditions are sufficiently general to cover the minimum volume predictive region, the conditional maximum score estimator for a panel data discrete choice model and many other widely used estimators in statistics and econometrics. Specifically, they allow for discontinuous criterion functions of weakly dependent observations which may be localized by kernel smoothing and contain nuisance parameters with growing dimension. Furthermore, the localization can occur around parameter values rather than around a fixed point and the observations may take limited values which lead to set estimators. Our theory produces three different nonparametric cube root rates for local M-estimators and enables valid inference building on novel maximal inequalities for weakly dependent observations. The standard cube root asymptotics is included as a special case. The results are illustrated by various examples such as the Hough transform estimator with diminishing bandwidth, the maximum score-type set estimator and many others.","Cube root asymptotics,maximal inequality,mixing process,partial identification,parameter-dependent localization","Seo, Myung Hwan@Seoul National University::Otsu, Taisuke@London School Economics & Political Science@University of London"
MIXTURE INNER PRODUCT SPACES AND THEIR APPLICATION TO FUNCTIONAL DATA ANALYSIS,ANNALS OF STATISTICS,38,"We introduce the concept of mixture inner product spaces associated with a given separable Hilbert space, which feature an infinite-dimensional mixture of finite-dimensional vector spaces and are dense in the underlying Hilbert space. Any Hilbert valued random element can be arbitrarily closely approximated by mixture inner product space valued random elements. While this concept can be applied to data in any infinite-dimensional Hilbert space, the case of functional data that are random elements in the L-2 space of square integrable functions is of special interest. For functional data, mixture inner product spaces provide a new perspective, where each realization of the underlying stochastic process falls into one of the component spaces and is represented by a finite number of basis functions, the number of which corresponds to the dimension of the component space. In the mixture representation of functional data, the number of included mixture components used to represent a given random element in L2 is specifically adapted to each random trajectory and may be arbitrarily large. Key benefits of this novel approach are, first, that it provides a new perspective on the construction of a probability density in function space under mild regularity conditions, and second, that individual trajectories possess a trajectory-specific dimension that corresponds to a latent random variable, making it possible to use a larger number of components for less smooth and a smaller number for smoother trajectories. This enables flexible and parsimonious modeling of heterogeneous trajectory shapes. We establish estimation consistency of the functional mixture density and introduce an algorithm for fitting the functional mixture model based on a modified expectation-maximization algorithm. Simulations confirm that in comparison to traditional functional principal component analysis the proposed method achieves similar or better data recovery while using fewer components on average. Its practical merits are also demonstrated in an analysis of egg-laying trajectories for medflies.","Basis,functional data analysis,infinite mixture,probability density,trajectory representation","Lin, Zhenhua@University of Toronto::Mueller, Hans-Georg@University of California Davis@University of California System::Yao, Fang@University of Toronto@Peking University"
ON THE ASYMPTOTIC THEORY OF NEW BOOTSTRAP CONFIDENCE BOUNDS,ANNALS OF STATISTICS,13,"We propose a new method, based on sample splitting, for constructing bootstrap confidence bounds for a parameter appearing in the regular smooth function model. It has been demonstrated in the literature, for example, by Hall [Ann. Statist. 16 (1988) 927-985; The Bootstrap and Edgeworth Expansion (1992) Springer], that the well-known percentile-t method for constructing bootstrap confidence bounds typically incurs a coverage error of order O(n(-1)), with n being the sample size. Our version of the percentile-t bound reduces this coverage error to order O(n(-3/2)) and in some cases to O(n(-2)). Furthermore, whereas the standard percentile bounds typically incur coverage error of O(n(-1/2)), the new bounds have reduced error of O(n(-1)). In the case where the parameter of interest is the population mean, we derive for each confidence bound the exact coefficient of the leading term in an asymptotic expansion of the coverage error, although similar results may be obtained for other parameters such as the variance, the correlation coefficient, and the ratio of two means. We show that equal-tailed confidence intervals with coverage error at most O(n(-2)) may be obtained from the newly proposed bounds, as opposed to the typical error O(n(-1)) of the standard intervals. It is also shown that the good properties of the new percentile-t method carry over to regression problems. Results of independent interest are derived, such as a generalisation of a delta method by Cramer [Mathematical Methods of Statistics (1946) Princeton Univ. Press] and Hurt [Apl. Mat. 21 (1976) 444-456], and an expression for a polynomial appearing in an Edgeworth expansion of the distribution of a Studentised statistic for the slope parameter in a regression model. A small simulation study illustrates the behavior of the confidence bounds for small to moderate sample sizes.","Confidence bounds,sample splitting,coverage error,smooth function model,Edgeworth polynomials,Cornish-Fisher expansion,regression","Pretorius, Charl@North West University - South Africa::Swanepoel, Jan W. H.@North West University - South Africa"
DESIGNS WITH BLOCKS OF SIZE TWO AND APPLICATIONS TO MICROARRAY EXPERIMENTS,ANNALS OF STATISTICS,35,"Designs with blocks of size two have numerous applications. In experimental situations where observation loss is common, it is important for a design to be robust against breakdown. For designs with one treatment factor and a single blocking factor, with blocks of size two, conditions for connectivity and robustness are obtained using combinatorial arguments and results from graph theory. Lower bounds are given for the breakdown number in terms of design parameters. For designs with equal or near equal treatment replication, the concepts of treatment and block partitions, and of linking blocks, are used to obtain information on the number of blocks required to guarantee various levels of robustness. The results provide guidance for construction of designs with good robustness properties.

Robustness conditions are also established for row column designs in which one of the blocking factors involves blocks of size two. Such designs are particularly relevant for microarray experiments, where the high risk of observation loss makes robustness important. Disconnectivity in row column designs can be classified as three types. Techniques are given to assess design robustness according to each type, leading to lower bounds for the breakdown number. Guidance is given for robust design construction.

Cyclic designs and interwoven loop designs are shown to have good robustness properties.","Breakdown number,connectivity,incomplete block design,microarray experiment,observation loss,robustness,row column design","Godolphin, Janet@University of Surrey"
LOCAL ROBUST ESTIMATION OF THE PICKANDS DEPENDENCE FUNCTION,ANNALS OF STATISTICS,30,"We consider the robust estimation of the Pickands dependence function in the random covariate framework. Our estimator is based on local estimation with the minimum density power divergence criterion. We provide the main asymptotic properties, in particular the convergence of the stochastic process, correctly normalized, towards a tight centered Gaussian process. The finite sample performance of our estimator is evaluated with a simulation study involving both uncontaminated and contaminated samples. The method is illustrated on a dataset of air pollution measurements.","Conditional Pickands dependence function,robustness,stochastic convergence","Escobar-Bach, Mikael@University of Southern Denmark::Goegebeur, Yuri@University of Southern Denmark::Guillou, Armelle@CNRS - National Institute for Mathematical Sciences (INSMI)@Universite de Strasbourg@Centre National de la Recherche Scientifique (CNRS)@Universites de Strasbourg Etablissements Associes"
STRONG IDENTIFIABILITY AND OPTIMAL MINIMAX RATES FOR FINITE MIXTURE ESTIMATION,ANNALS OF STATISTICS,36,"We study the rates of estimation of finite mixing distributions, that is, the parameters of the mixture. We prove that under some regularity and strong identifiability conditions, around a given mixing distribution with m(0) components, the optimal local minimax rate of estimation of a mixing distribution with m components is n(-1/(4(m-m0)+2)). This corrects a previous paper by Chen [Ann. Statist. 23 (1995) 221-233].

By contrast, it turns out that there are estimators with a (nonuniform) pointwise rate of estimation of n(-1/2) for all mixing distributions with a finite number of components.","Local asymptotic normality,convergence of experiments,maximum likelihood estimate,Wasserstein metric,mixing distribution,mixture model,rate of convergence,strong identifiability,pointwise rate,superefficiency","Heinrich, Philippe@Universite de Lille@Fondation I-SITE ULNE@Universite Lille-Nord-de-France (ComUE)::Kahn, Jonas@Universite de Toulouse@Universite Toulouse III - Paul Sabatier"
SUB-GAUSSIAN ESTIMATORS OF THE MEAN OF A RANDOM MATRIX WITH HEAVY-TAILED ENTRIES,ANNALS OF STATISTICS,44,"Estimation of the covariance matrix has attracted a lot of attention of the statistical research community over the years, partially due to important applications such as principal component analysis. However, frequently used empirical covariance estimator, and its modifications, is very sensitive to the presence of outliers in the data. As P. Huber wrote [Ann. Math. Stat. 35 (1964) 73-101], ""... This raises a question which could have been asked already by Gauss, but which was, as far as I know, only raised a few years ago (notably by Tukey): what happens if the true distribution deviates slightly from the assumed normal one? As is now well known, the sample mean then may have a catastrophically bad performance...."" Motivated by Tukey's question, we develop a new estimator of the (element-wise) mean of a random matrix, which includes covariance estimation problem as a special case. Assuming that the entries of a matrix possess only finite second moment, this new estimator admits sub-Gaussian or sub-exponential concentration around the unknown mean in the operator norm. We explain the key ideas behind our construction, and discuss applications to covariance estimation and matrix completion problems.","Random matrix,heavy tails,concentration inequality,covariance estimation,matrix completion","Minsker, Stanislav@University of Southern California"
CAUSAL INFERENCE IN PARTIALLY LINEAR STRUCTURAL EQUATION MODELS,ANNALS OF STATISTICS,31,"We consider identifiability of partially linear additive structural equation models with Gaussian noise (PLSEMs) and estimation of distributionally equivalent models to a given PLSEM. Thereby, we also include robustness results for errors in the neighborhood of Gaussian distributions. Existing identifiability results in the framework of additive SEMs with Gaussian noise are limited to linear and nonlinear SEMs, which can be considered as special cases of PLSEMs with vanishing nonparametric or parametric part, respectively. We close the wide gap between these two special cases by providing a comprehensive theory of the identifiability of PLSEMs by means of (A) a graphical, (B) a transformational, (C) a functional and (D) a causal ordering characterization of PLSEMs that generate a given distribution P. In particular, the characterizations (C) and (D) answer the fundamental question to which extent nonlinear functions in additive SEMs with Gaussian noise restrict the set of potential causal models, and hence influence the identifiability.

On the basis of the transformational characterization (B) we provide a score-based estimation procedure that outputs the graphical representation (A) of the distribution equivalence class of a given PLSEM. We derive its (high-dimensional) consistency and demonstrate its performance on simulated datasets.","Causal inference,distribution equivalence class,graphical model,high-dimensional consistency,partially linear structural equation model","Rothenhaeusler, Dominik@ETH Zurich::Ernest, Jan@ETH Zurich::Buehlmann, Peter@ETH Zurich"
ON MSE-OPTIMAL CROSSOVER DESIGNS,ANNALS OF STATISTICS,14,"In crossover designs, each subject receives a series of treatments one after the other. Most papers on optimal crossover designs consider an estimate which is corrected for carryover effects. We look at the estimate for direct effects of treatment, which is not corrected for carryover effects. If there are carryover effects, this estimate will be biased. We try to find a design that minimizes the mean square error, that is, the sum of the squared bias and the variance. It turns out that the designs which are optimal for the corrected estimate are highly efficient for the uncorrected estimate.","Optimal design,crossover design,MSE-optimality","Neumann, Christoph@Dortmund University of Technology::Kunert, Joachim@Dortmund University of Technology"
TESTING FOR PERIODICITY IN FUNCTIONAL TIME SERIES,ANNALS OF STATISTICS,33,"We derive several tests for the presence of a periodic component in a time series of functions. We consider both the traditional setting in which the periodic functional signal is contaminated by functional white noise, and a more general setting of a weakly dependent contaminating process. Several forms of the periodic component are considered. Our tests are motivated by the likelihood principle and fall into two broad categories, which we term multivariate and fully functional. Generally, for the functional series that motivate this research, the fully functional tests exhibit a superior balance of size and power. Asymptotic null distributions of all tests are derived and their consistency is established. Their finite sample performance is examined and compared by numerical studies and application to pollution data.","Functional data,time series data,periodicity,spectral analysis,testing,asymptotics","Hoermann, Siegfried@Graz University of Technology::Kokoszka, Piotr@Colorado State University::Nisol, Gilles@Universite Libre de Bruxelles"
LIMITING BEHAVIOR OF EIGENVALUES IN HIGH-DIMENSIONAL MANOVA VIA RMT,ANNALS OF STATISTICS,38,"In this paper, we derive the asymptotic joint distributions of the eigenvalues under the null case and the local alternative cases in the MANOVA model and multiple discriminant analysis when both the dimension and the sample size are large. Our results are obtained by random matrix theory (RMT) without assuming normality in the populations. It is worth pointing out that the null and nonnull distributions of the eigenvalues and invariant test statistics are asymptotically robust against departure from normality in high-dimensional situations. Similar properties are pointed out for the null distributions of the invariant tests in multivariate regression model. Some new formulas in RMT are also presented.","Asymptotic distribution,eigenvalues,discriminant analysis,high-dimensional case,MANOVA,nonnormality,RMT,test statistics","Bai, Zhidong@Northeast Normal University - China::Choi, Kwok Pui@National University of Singapore::Fujikoshi, Yasunori@Hiroshima University"
OVERCOMING THE LIMITATIONS OF PHASE TRANSITION BY HIGHER ORDER ANALYSIS OF REGULARIZATION TECHNIQUES,ANNALS OF STATISTICS,46,"We study the problem of estimating a sparse vector beta is an element of R-p from the response variables y = X beta + omega, where omega similar to N(0, sigma(2)(omega) I-nxn), under the following high-dimensional asymptotic regime: given a fixed number delta, p -> infinity, while n/p -> delta. We consider the popular class of l(q)-regularized least squares (LQLS), a.k.a. bridge estimators, given by the optimization problem

(beta) over cap(lambda, q) is an element of arg min(beta) 1/2 parallel to y - X beta parallel to(2)(2) + lambda parallel to beta parallel to(q)(q),

and characterize the almost sure limit of 1/p parallel to(beta) over cap(lambda, q) - beta parallel to(2)(2), and call it asymptotic mean square error (AMSE). The expression we derive for this limit does not have explicit forms, and hence is not useful in comparing LQLS for different values of delta, or providing information in evaluating the effect of d or sparsity level of beta. To simplify the expression, researchers have considered the ideal ""error-free"" regime, that is, omega = 0, and have characterized the values of d for which AMSE is zero. This is known as the phase transition analysis.

In this paper, we first perform the phase transition analysis of LQLS. Our results reveal some of the limitations and misleading features of the phase transition analysis. To overcome these limitations, we propose the small error analysis of LQLS. Our new analysis framework not only sheds light on the results of the phase transition analysis, but also describes when phase transition analysis is reliable, and presents a more accurate comparison among different regularizers.","Bridge regression,phase transition,comparison of estimators,small error regime,second-order term,asymptotic mean square error,optimal tuning","Weng, Haolei@Columbia University::Maleki, Arian@Columbia University::Zheng, Le@Columbia University"
TWO-SAMPLE KOLMOGOROV-SMIRNOV-TYPE TESTS REVISITED: OLD AND NEW TESTS IN TERMS OF LOCAL LEVELS,ANNALS OF STATISTICS,29,"From a multiple testing viewpoint, Kolmogorov-Smirnov (KS)-type tests are union-intersection tests which can be redefined in terms of local levels. The local level perspective offers a new viewpoint on ranges of sensitivity of KS-type tests and the design of new tests. We study the finite and asymptotic local level behavior of weighted KS tests which are either tail, intermediate or central sensitive. Furthermore, we provide new tests with approximately equal local levels and prove that the asymptotics of such tests with sample sizes m and n coincides with the asymptotics of one-sample higher criticism tests with sample size min(m, n). We compare the overall power of various tests and introduce local powers that are in line with local levels. Finally, suitably parameterized local level shape functions can be used to design new tests. We illustrate how to combine tests with different sensitivity in terms of local levels.","Goodness-of-fit,higher criticism test,local levels,multiple hypotheses testing,nonparametric two-sample tests,order statistics,weighted Brownian bridge","Finner, Helmut@Heinrich Heine University Dusseldorf@Deutsches Diabetes-Zentrum (DDZ)::Gontscharuk, Veronika@Heinrich Heine University Dusseldorf@Deutsches Diabetes-Zentrum (DDZ)"
ROBUST GAUSSIAN STOCHASTIC PROCESS EMULATION,ANNALS OF STATISTICS,41,"We consider estimation of the parameters of a Gaussian Stochastic Process (GaSP), in the context of emulation (approximation) of computer models for which the outcomes are real-valued scalars. The main focus is on estimation of the GaSP parameters through various generalized maximum likelihood methods, mostly involving finding posterior modes; this is because full Bayesian analysis in computer model emulation is typically prohibitively expensive.

The posterior modes that are studied arise from objective priors, such as the reference prior. These priors have been studied in the literature for the situation of an isotropic covariance function or under the assumption of separability in the design of inputs for model runs used in the GaSP construction. In this paper, we consider more general designs (e.g., a Latin Hypercube Design) with a class of commonly used anisotropic correlation functions, which can be written as a product of isotropic correlation functions, each having an unknown range parameter and a fixed roughness parameter. We discuss properties of the objective priors and marginal likelihoods for the parameters of the GaSP and establish the posterior propriety of the GaSP parameters, but our main focus is to demonstrate that certain parameterizations result in more robust estimation of the GaSP parameters than others, and that some parameterizations that are in common use should clearly be avoided. These results are applicable to many frequently used covariance functions, for example, power exponential, Matern, rational quadratic and spherical covariance. We also generalize the results to the GaSP model with a nugget parameter. Both theoretical and numerical evidence is presented concerning the performance of the studied procedures.","Anisotropic covariance,emulation,Gaussian stochastic process,objective priors,posterior propriety,robust parameter estimation","Gu, Mengyang@Johns Hopkins University::Wang, Xiaojing@University of Connecticut::Berger, James O.@Duke University"
CONVERGENCE OF CONTRASTIVE DIVERGENCE ALGORITHM IN EXPONENTIAL FAMILY,ANNALS OF STATISTICS,44,"The Contrastive Divergence ( CD) algorithm has achieved notable success in training energy-based models including Restricted Boltzmann Machines and played a key role in the emergence of deep learning. The idea of this algorithm is to approximate the intractable term in the exact gradient of the log-likelihood function by using short Markov chain Monte Carlo (MCMC) runs. The approximate gradient is computationally-cheap but biased. Whether and why the CD algorithm provides an asymptotically consistent estimate are still open questions. This paper studies the asymptotic properties of the CD algorithm in canonical exponential families, which are special cases of the energy-based model. Suppose the CD algorithm runs m MCMC transition steps at each iteration t and iteratively generates a sequence of parameter estimates {theta(t)}(t >= 0) given an i.i.d. data sample {X-i}(i=1)(n) similar to p(theta star). Under conditions which are commonly obeyed by the CD algorithm in practice, we prove the existence of some bounded m such that any limit point of the time average Sigma(t-1)(s=0) theta(s)/t as t -> infinity is a consistent estimate for the true parameter theta(star). Our proof is based on the fact that {theta(t)}(t >= 0) is a homogenous Markov chain conditional on the data sample {X-i}(i=1)(n). This chain meets the Foster-Lyapunov drift criterion and converges to a random walk around the maximum likelihood estimate. The range of the random walk shrinks to zero at rate O(1/(3)root n) as the sample size n -> infinity.","Contrastive Divergence,exponential family,convergence rate","Jiang, Bai@Stanford University::Wu, Tung-Yu@Stanford University::Jin, Yifan@Stanford University::Wong, Wing H.@Stanford University"
OPTIMAL ADAPTIVE ESTIMATION OF LINEAR FUNCTIONALS UNDER SPARSITY,ANNALS OF STATISTICS,12,"We consider the problem of estimation of a linear functional in the Gaussian sequence model where the unknown vector theta is an element of R-d belongs to a class of s-sparse vectors with unknown s. We suggest an adaptive estimator achieving a nonasymptotic rate of convergence that differs from the minimax rate at most by a logarithmic factor. We also show that this optimal adaptive rate cannot be improved when s is unknown. Furthermore, we address the issue of simultaneous adaptation to s and to the variance sigma(2) of the noise. We suggest an estimator that achieves the optimal adaptive rate when both s and sigma(2) are unknown.","Nonasymptotic minimax estimation,adaptive estimation,linear functional,sparsity,unknown noise variance","Collier, Olivier@Universite Paris Saclay (ComUE)@Universite de Versailles Saint-Quentin-En-Yvelines@Universite Paris Nanterre@Universite Paris Saclay::Comminges, Laetitia@PSL Research University Paris (ComUE)@Universite Paris-Dauphine::Tsybakov, Alexandre B.@Universite Paris Saclay (ComUE)@ENSAE ParisTech::Verzelen, Nicolas@Institut National de la Recherche Agronomique (INRA)"
HIGH-DIMENSIONAL CONSISTENCY IN SCORE-BASED AND HYBRID STRUCTURE LEARNING,ANNALS OF STATISTICS,59,"Main approaches for learning Bayesian networks can be classified as constraint-based, score-based or hybrid methods. Although high-dimensional consistency results are available for constraint-based methods like the PC algorithm, such results have not been proved for score-based or hybrid methods, and most of the hybrid methods have not even shown to be consistent in the classical setting where the number of variables remains fixed and the sample size tends to infinity. In this paper, we show that consistency of hybrid methods based on greedy equivalence search (GES) can be achieved in the classical setting with adaptive restrictions on the search space that depend on the current state of the algorithm. Moreover, we prove consistency of GES and adaptively restricted GES (ARGES) in several sparse high-dimensional settings. ARGES scales well to sparse graphs with thousands of variables and our simulation study indicates that both GES and ARGES generally outperform the PC algorithm.","Bayesian network,directed acyclic graph (DAG),linear structural equation model (linear SEM),structure learning,greedy equivalence search (GES),score-based method,hybrid method,high-dimensional data,consistency","Nandy, Preetam@ETH Zurich::Hauser, Alain@Unknow::Maathuis, Marloes H.@ETH Zurich"
ESTIMATION OF A MONOTONE DENSITY IN s-SAMPLE BIASED SAMPLING MODELS,ANNALS OF STATISTICS,36,"We study the nonparametric estimation of a decreasing density function go in a general s-sample biased sampling model with weight (or bias) functions w(i )for i = 1, ...,s. The determination of the monotone maximum likelihood estimator (g) over cap (n) and its asymptotic distribution, except for the case when s = 1, has been long missing in the literature due to certain nonstandard structures of the likelihood function, such as nonseparability and a lack of strictly positive second order derivatives of the negative of the log-likelihood function. The existence, uniqueness, self-characterization, consistency of (g) over cap (n) and its asymptotic distribution at a fixed point are established in this article. To overcome the barriers caused by nonstandard likelihood structures, for instance, we show the tightness of (g) over cap (n) via a purely analytic argument instead of an intrinsic geometric one and propose an indirect approach to attain the root n-rate of convergence of the linear functional integral w(i )(g) over cap (n).","Density estimation,empirical process theory,Karush-Kuhn-Tucker conditions,nonparametric estimation,order statistics from multiple samples,self-induced characterization,shape-constrained problem,s-sample biased sampling","Chan, Kwun Chuen Gary@University of Washington@University of Washington Seattle::Ling, Hok Kan@Columbia University::Sit, Tony@Chinese University of Hong Kong::Yam, Sheung Chi Phillip@Chinese University of Hong Kong"
A NEW PERSPECTIVE ON ROBUST M-ESTIMATION: FINITE SAMPLE THEORY AND APPLICATIONS TO DEPENDENCE-ADJUSTED MULTIPLE TESTING,ANNALS OF STATISTICS,51,"Heavy-tailed errors impair the accuracy of the least squares estimate, which can be spoiled by a single grossly outlying observation. As argued in the seminal work of Peter Huber in 1973 [Ann. Statist. 1 (1973) 799-821], robust alternatives to the method of least squares are sorely needed. To achieve robustness against heavy-tailed sampling distributions, we revisit the Huber estimator from a new perspective by letting the tuning parameter involved diverge with the sample size. In this paper, we develop nonasymptotic concentration results for such an adaptive Huber estimator, namely, the Huber estimator with the tuning parameter adapted to sample size, dimension and the variance of the noise. Specifically, we obtain a sub-Gaussian-type deviation inequality and a nonasymptotic Bahadur representation when noise variables only have finite second moments. The nonasymptotic results further yield two conventional normal approximation results that are of independent interest, the Berry-Esseen inequality and Cramer-type moderate deviation. As an important application to large-scale simultaneous inference, we apply these robust normal approximation results to analyze a dependence-adjusted multiple testing procedure for moderately heavy-tailed data. It is shown that the robust dependence-adjusted procedure asymptotically controls the overall false discovery proportion at the nominal level under mild moment conditions. Thorough numerical results on both simulated and real datasets are also provided to back up our theory.","Approximate factor model,Bahadur representation,false discovery proportion,heavy-tailed data,Huber loss,large-scale multiple testing,M-estimator","Zhou, Wen-Xin@University of California San Diego@University of California System::Bose, Koushiki@Princeton University::Fan, Jianqing@Fudan University@Princeton University::Liu, Han@Princeton University"
VARIABLE SELECTION WITH HAMMING LOSS,ANNALS OF STATISTICS,28,"We derive nonasymptotic bounds for the minimax risk of variable selection under expected Hamming loss in the Gaussian mean model in R-d for classes of at most s-sparse vectors separated from 0 by a constant a > 0. In some cases, we get exact expressions for the nonasymptotic minimax risk as a function of d, s, a and find explicitly the minimax selectors. These results are extended to dependent or non-Gaussian observations and to the problem of crowdsourcing. Analogous conclusions are obtained for the probability of wrong recovery of the sparsity pattern. As corollaries, we derive necessary and sufficient conditions for such asymptotic properties as almost full recovery and exact recovery. Moreover, we propose data-driven selectors that provide almost full and exact recovery adaptively to the parameters of the classes.","Adaptive variable selection,almost full recovery,exact recovery,Hamming loss,minimax selectors,nonasymptotic minimax selection bounds,phase transitions","Butucea, Cristina@Universite Paris-Est Marne-la-Vallee@Universite Paris Saclay@Universite Paris-Est-Creteil-Val-de-Marne (UPEC)@ENSAE ParisTech@CNRS - National Institute for Mathematical Sciences (INSMI)@Centre National de la Recherche Scientifique (CNRS)@Universite Paris Saclay (ComUE)@Universite Paris-Est (ComUE)::Ndaoud, Mohamed@Universite Paris Saclay (ComUE)@Universite Paris Saclay@ENSAE ParisTech::Stepanova, Natalia A.@Carleton University::Tsybakov, Alexandre B.@Universite Paris Saclay (ComUE)@Universite Paris Saclay@ENSAE ParisTech"
RANDOMIZATION-BASED CAUSAL INFERENCE FROM SPLIT-PLOT DESIGNS,ANNALS OF STATISTICS,32,"Under the potential outcomes framework, we propose a randomization based estimation procedure for causal inference from split-plot designs, with special emphasis on 2(2) designs that naturally arise in many social, behavioral and biomedical experiments. Point estimators of factorial effects are obtained and their sampling variances are derived in closed form as linear combinations of the between- and within-group covariances of the potential outcomes. Results are compared to those under complete randomization as measures of design efficiency. Conservative estimators of these sampling variances are proposed. Connection of the randomization-based approach to inference based on the linear mixed effects model is explored. Results on sampling variances of point estimators and their estimators are extended to general split-plot designs. The superiority over existing model-based alternatives in frequency coverage properties is reported under a variety of simulation settings for both binary and continuous outcomes.","Between-whole-plot additivity,model-based inference,Neymanian inference,potential outcomes framework,projection matrix,within-whole-plot additivity","Zhao, Anqi@Harvard University::Ding, Peng@University of California System@University of California Berkeley::Mukerjee, Rahul@Indian Institute of Management Calcutta::Dasgupta, Tirthankar@Rutgers State University New Brunswick"
ROBUST COVARIANCE AND SCATTER MATRIX ESTIMATION UNDER HUBER'S CONTAMINATION MODEL,ANNALS OF STATISTICS,60,"Covariance matrix estimation is one of the most important problems in statistics. To accommodate the complexity of modern datasets, it is desired to have estimation procedures that not only can incorporate the structural assumptions of covariance matrices, but are also robust to outliers from arbitrary sources. In this paper, we define a new concept called matrix depth and then propose a robust covariance matrix estimator by maximizing the empirical depth function. The proposed estimator is shown to achieve minimax optimal rate under Huber's epsilon-contamination model for estimating covariance/scatter matrices with various structures including bandedness and sparsity.","Data depth,Minimax rate,high-dimensional statistics,outliers,contamination model,breakdown point","Chen, Mengjie@University of Chicago::Gao, Chao@University of Chicago::Ren, Zhao@Pennsylvania Commonwealth System of Higher Education (PCSHE)@University of Pittsburgh"
EMPIRICAL BEST PREDICTION UNDER A NESTED ERROR MODEL WITH LOG TRANSFORMATION,ANNALS OF STATISTICS,21,"In regression models involving economic variables such as income, log transformation is typically taken to achieve approximate normality and stabilize the variance. However, often the interest is predicting individual values or means of the variable in the original scale. Under a nested error model for the log transformation of the target variable, we show that the usual approach of back transforming the predicted values may introduce a substantial bias. We obtain the optimal (or ""best"") predictors of individual values of the original variable and of small area means under that model. Empirical best predictors are defined by estimating the unknown model parameters in the best predictors. When estimation is desired for subpopulations with small sample sizes (small areas), nested error models are widely used to ""borrow strength"" from the other areas and obtain estimators with greater efficiency than direct estimators based on the scarce area-specific data. We show that naive predictors of small area means obtained by back-transformation under the mentioned model may even underperform direct estimators. Moreover, assessing the uncertainty of the considered predictor is not straightforward. Exact mean squared errors of the best predictors and second-order approximations to the mean squared errors of the empirical best predictors are derived. Estimators of the mean squared errors that are second-order correct are also obtained. Simulation studies and an example with Mexican data on living conditions illustrate the procedures.","Empirical best estimator,mean squared error,parametric bootstrap","Molina, Isabel@Universidad Carlos III de Madrid::Martin, Nirian@Complutense University of Madrid"
BACKWARD NESTED DESCRIPTORS ASYMPTOTICS WITH INFERENCE ON STEM CELL DIFFERENTIATION,ANNALS OF STATISTICS,42,"For sequences of random backward nested subspaces as occur, say, in dimension reduction for manifold or stratified space valued data, asymptotic results are derived. In fact, we formulate our results more generally for backward nested families of descriptors (BNFD). Under rather general conditions, asymptotic strong consistency holds. Under additional, still rather general hypotheses, among them existence of a.s. local twice differentiable charts, asymptotic joint normality of a BNFD can be shown. If charts factor suitably, this leads to individual asymptotic normality for the last element, a principal nested mean or a principal nested geodesic, say. It turns out that these results pertain to principal nested spheres (PNS) and principal nested great subsphere (PNGS) analysis by Jung, Dryden and Marron [Biometrika 99 (2012) 551-568] as well as to the intrinsic mean on a first geodesic principal component (IMolGPC) for manifolds and Kendall's shape spaces. A nested bootstrap two-sample test is derived and illustrated with simulations. In a study on real data, PNGS is applied to track early human mesenchymal stem cell differentiation over a coarse time grid and, among others, to locate a change point with direct consequences for the design of further studies.","Frechet means,dimension reduction on manifolds,principal nested spheres,asymptotic consistency and normality,geodesic principal component analysis,Kendall's shape spaces,flags of subspaces","Huckemann, Stephan F.@University of Gottingen::Eltzner, Benjamin@University of Gottingen"
CHANGE-POINT DETECTION IN MULTINOMIAL DATA WITH A LARGE NUMBER OF CATEGORIES,ANNALS OF STATISTICS,30,"We consider a sequence of multinomial data for which the probabilities associated with the categories are subject to abrupt changes of unknown magnitudes at unknown locations. When the number of categories is comparable to or even larger than the number of subjects allocated to these categories, conventional methods such as the classical Pearson's chi-squared test and the deviance test may not work well. Motivated by high-dimensional homogeneity tests, we propose a novel change-point detection procedure that allows the number of categories to tend to infinity. The null distribution of our test statistic is asymptotically normal and the test performs well with finite samples. The number of change-points is determined by minimizing a penalized objective function based on segmentation, and the locations of the change-points are estimated by minimizing the objective function with the dynamic programming algorithm. Under some mild conditions, the consistency of the estimators of multiple change-points is established. Simulation studies show that the proposed method performs satisfactorily for identifying change-points in terms of power and estimation accuracy, and it is illustrated with an analysis of a real data set.","Asymptotic normality,categorical data,high-dimensional homogeneity test,multiple change-point detection,sparse contingency table","Wang, Guanghui@Nankai University::Zou, Changliang@Nankai University::Yin, Guosheng@University of Hong Kong"
LOCAL ASYMPTOTIC NORMALITY PROPERTY FOR FRACTIONAL GAUSSIAN NOISE UNDER HIGH-FREQUENCY OBSERVATIONS,ANNALS OF STATISTICS,26,"Local Asymptotic Normality (LAN) property for fractional Gaussian noise under high-frequency observations is proved with nondiagonal rate matrices depending on the parameter to be estimated. In contrast to the LAN families in the literature, nondiagonal rate matrices are inevitable. As consequences of the LAN property, a maximum likelihood sequence of estimators is shown to be asymptotically efficient and the likelihood ratio test on the Hurst parameter is shown to be an asymptotically uniformly most powerful unbiased test for two-sided hypotheses.","Locally asymptotically normal families,fractional Brownian motion,high-frequency data,maximum likelihood estimators,likelihood ratio test","Brouste, Alexandre@Unknow::Fukasawa, Masaaki@Osaka University"
GLOBAL TESTING AGAINST SPARSE ALTERNATIVES UNDER ISING MODELS,ANNALS OF STATISTICS,39,"In this paper, we study the effect of dependence on detecting sparse signals. In particular, we focus on global testing against sparse alternatives for the means of binary outcomes following an Ising model, and establish how the interplay between the strength and sparsity of a signal determines its detectability under various notions of dependence. The profound impact of dependence is best illustrated under the Curie-Weiss model where we observe the effect of a ""thermodynamic"" phase transition. In particular, the critical state exhibits a subtle ""blessing of dependence"" phenomenon in that one can detect much weaker signals at criticality than otherwise. Furthermore, we develop a testing procedure that is broadly applicable to account for dependence and show that it is asymptotically minimax optimal under fairly general regularity conditions.","Detection boundary,Ising models,phase transitions,sparse signals","Mukherjee, Rajarshi@University of California System@University of California Berkeley::Mukherjee, Sumit@Columbia University::Yuan, Ming@Columbia University"
PRINCIPAL COMPONENT ANALYSIS FOR SECOND-ORDER STATIONARY VECTOR TIME SERIES,ANNALS OF STATISTICS,44,"We extend the principal component analysis (PCA) to second-order stationary vector time series in the sense that we seek for a contemporaneous linear transformation for a p-variate time series such that the transformed series is segmented into several lower-dimensional subseries, and those subseries are uncorrelated with each other both contemporaneously and serially. Therefore, those lower-dimensional series can be analyzed separately as far as the linear dynamic structure is concerned. Technically, it boils down to an eigenanalysis for a positive definite matrix. When p is large, an additional step is required to perform a permutation in terms of either maximum cross-correlations or FDR based on multiple tests. The asymptotic theory is established for both fixed p and diverging p when the sample size n tends to infinity. Numerical experiments with both simulated and real data sets indicate that the proposed method is an effective initial step in analyzing multiple time series data, which leads to substantial dimension reduction in modelling and forecasting high-dimensional linear dynamical structures. Unlike PCA for independent data, there is no guarantee that the required linear transformation exists. When it does not, the proposed method provides an approximate segmentation which leads to the advantages in, for example, forecasting for future values. The method can also be adapted to segment multiple volatility processes.","alpha-mixing,autocorrelation,cross-correlation,dimension reduction,eigenanalysis,high-dimensional time series,weak stationarity","Chang, Jinyuan@Southwestern University of Finance & Economics - China::Guo, Bin@Southwestern University of Finance & Economics - China::Yao, Qiwei@London School Economics & Political Science@University of London"
COMMUNITY DETECTION IN DEGREE-CORRECTED BLOCK MODELS,ANNALS OF STATISTICS,27,"Community detection is a central problem of network data analysis. Given a network, the goal of community detection is to partition the network nodes into a small number of clusters, which could often help reveal interesting structures. The present paper studies community detection in Degree-Corrected Block Models (DCBMs). We first derive asymptotic minimax risks of the problem for a misclassification proportion loss under appropriate conditions. The minimax risks are shown to depend on degree-correction parameters, community sizes and average within and between community connectivities in an intuitive and interpretable way. In addition, we propose a polynomial time algorithm to adaptively perform consistent and even asymptotically optimal community detection in DCBMs.","Clustering,Minimax rates,network analysis,spectral clustering,stochastic block model","Gao, Chao@University of Chicago::Ma, Zongming@University of Pennsylvania::Zhang, Anderson Y.@Yale University::Zhou, Harrison H.@Yale University"
CLT FOR LARGEST EIGENVALUES AND UNIT ROOT TESTING FOR HIGH-DIMENSIONAL NONSTATIONARY TIME SERIES,ANNALS OF STATISTICS,36,"Let {Z(ij)} be independent and identically distributed (i.i.d.) random variables with EZ(ij) = 0, E vertical bar Z(ij)vertical bar(2) = 1 and E vertical bar Z(ij)vertical bar(4) < infinity. Define linear processes Y-tj = Sigma(infinity)(k=0) b(k) Z(t -k,j) with Sigma(infinity)(i=0) vertical bar b(i)vertical bar < infinity. Consider a p-dimensional time series model of the form x(t) = Pi x(t-1) + Sigma(1/2)y(t), 1 <= t <= T with y(t) = (Y-t(1), ..., Y-tp)' and Sigma(1/2) be the square root of a symmetric positive definite matrix. Let B = (1/p)XX* with X = (x(1), ..., x(T))' and X* be the conjugate transpose. This paper establishes both the convergence in probability and the asymptotic joint distribution of the first k largest eigenvalues of B when x(t) is nonstationary. As an application, two new unit root tests for possible nonstationarity of high-dimensional time series are proposed and then studied both theoretically and numerically.","Asymptotic normality,largest eigenvalue,linear process,unit root test","Zhang, Bo@Nanyang Technological University & National Institute of Education (NIE) Singapore@Nanyang Technological University::Pan, Guangming@Nanyang Technological University & National Institute of Education (NIE) Singapore@Nanyang Technological University::Gao, Jiti@Monash University"
SMOOTH BACKFITTING FOR ERRORS-IN-VARIABLES ADDITIVE MODELS,ANNALS OF STATISTICS,28,"In this work, we develop a new smooth backfitting method and theory for estimating additive nonparametric regression models when the covariates are contaminated by measurement errors. For this, we devise a new kernel function that suitably deconvolutes the bias due to measurement errors as well as renders a projection interpretation to the resulting estimator in the space of additive functions. The deconvolution property and the projection interpretation are essential for a successful solution of the problem. We prove that the method based on the new kernel weighting scheme achieves the optimal rate of convergence in one-dimensional deconvolution problems when the smoothness of measurement error distribution is less than a threshold value. We find that the speed of convergence is slower than the univariate rate when the smoothness of measurement error distribution is above the threshold, but it is still much faster than the optimal rate in multivariate deconvolution problems. The theory requires a deliberate analysis of the nonnegligible effects of measurement errors being propagated to other additive components through backfitting operation. We present the finite sample performance of the deconvolution smooth backfitting estimators that confirms our theoretical findings.","Nonparametric additive regression,smooth backfitting,errors-in-variables models,deconvolution,kernel smoothing","Han, Kyunghee@Seoul National University::Park, Byeong U.@Seoul National University"
UNIFYING MARKOV PROPERTIES FOR GRAPHICAL MODELS,ANNALS OF STATISTICS,43,"Several types of graphs with different conditional independence interpretations-also known as Markov properties-have been proposed and used in graphical models. In this paper, we unify these Markov properties by introducing a class of graphs with four types of edges-lines, arrows, arcs and dotted lines-and a single separation criterion. We show that independence structures defined by this class specialize to each of the previously defined cases, when suitable subclasses of graphs are considered. In addition, we define a pairwise Markov property for the subclass of chain mixed graphs, which includes chain graphs with the LWF interpretation, as well as summary graphs (and consequently ancestral graphs). We prove the equivalence of this pairwise Markov property to the global Markov property for compositional graphoid independence models.","AMP Markov property,c-separation,chain graph,compositional graphoid,d-separation,independence model,LWF Markov property,m-separation,mixed graph,pairwise Markov property,regression chain Markov property","Lauritzen, Steffen@University of Copenhagen::Sadeghi, Kayvan@University of Cambridge"
ADAPTATION IN LOG-CONCAVE DENSITY ESTIMATION,ANNALS OF STATISTICS,34,"The log-concave maximum likelihood estimator of a density on the real line based on a sample of size n is known to attain the minimax optimal rate of convergence of O(n(-4/5)) with respect to, for example, squared Hellinger distance. In this paper, we show that it also enjoys attractive adaptation properties, in the sense that it achieves a faster rate of convergence when the logarithm of the true density is k-affine (i.e., made up of k-affine pieces), or close to k-affine, provided in each case that k is not too large. Our results use two different techniques: the first relies on a new Marshall's inequality for log-concave density estimation, and reveals that when the true density is close to log-linear on its support, the log-concave maximum likelihood estimator can achieve the parametric rate of convergence in total variation distance. Our second approach depends on local bracketing entropy methods, and allows us to prove a sharp oracle inequality, which implies in particular a risk bound with respect to various global loss functions, including Kullback-Leibler divergence, of O(k/n log(5/4)(en/k)) when the true density is log-concave and its logarithm is close to k-affine.","Adaptation,bracketing entropy,log-concavity,maximum likelihood estimation,Marshall's inequality","Kim, Arlene K. H.@Sungshin Women's University@University of Cambridge::Guntuboyina, Adityanand@University of California System@University of California Berkeley::Samworth, Richard J.@University of Cambridge"
UNIFORMLY VALID POST-REGULARIZATION CONFIDENCE REGIONS FOR MANY FUNCTIONAL PARAMETERS IN Z-ESTIMATION FRAMEWORK,ANNALS OF STATISTICS,47,"In this paper, we develop procedures to construct simultaneous confidence bands for (p) over tilde potentially infinite-dimensional parameters after model selection for general moment condition models where p is potentially much larger than the sample size of available data, n. This allows us to cover settings with functional response data where each of the p parameters is a function. The procedure is based on the construction of score functions that satisfy Neyman orthogonality condition approximately. The proposed simultaneous confidence bands rely on uniform central limit theorems for high-dimensional vectors (and not on Donsker arguments as we allow for (p) over tilde >> n). To construct the bands, we employ a multiplier bootstrap procedure which is computationally efficient as it only involves resampling the estimated score functions (and does not require resolving the high-dimensional optimization problems). We formally apply the general theory to inference on regression coefficient process in the distribution regression model with a logistic link, where two implementations are analyzed in detail. Simulations and an application to real data are provided to help illustrate the applicability of the results.","Inference after model selection,moment condition models with a continuum of target parameters,Lasso and Post-Lasso with functional response data","Belloni, Alexandre@Duke University::Chernozhukov, Victor@Massachusetts Institute of Technology (MIT)::Chetverikov, Denis@University of California Los Angeles@University of California System::Wei, Ying@Columbia University"
MULTISCALE SCANNING IN INVERSE PROBLEMS,ANNALS OF STATISTICS,77,"In this paper, we propose a multiscale scanning method to determine active components of a quantity f w.r.t. a dictionary U from observations Y in an inverse regression model Y = T f + xi with linear operator T and general random error xi. To this end, we provide uniform confidence statements for the coefficients <phi, f >, phi is an element of U, under the assumption that (T*)(-1)(U) is of wavelet-type. Based on this, we obtain a multiple test that allows to identify the active components of U, that is, < f, phi > not equal 0, phi is an element of U, at controlled, family wise error rate. Our results rely on a Gaussian approximation of the underlying multiscale statistic with a novel scale penalty adapted to the ill-posedness of the problem. The scale penalty furthermore ensures convergence of the statistic's distribution towards a Gumbel limit under reasonable assumptions. The important special cases of tomography and deconvolution are discussed in detail. Further, the regression case, when T = id and the dictionary consists of moving windows of various sizes (scales), is included, generalizing previous results for this setting. We show that our method obeys an oracle optimality, that is, it attains the same asymptotic power as a single-scale testing procedure at the correct scale. Simulations support our theory and we illustrate the potential of the method as an inferential tool for imaging. As a particular application, we discuss super-resolution microscopy and analyze experimental STED data to locate single DNA origami.","Multiscale analysis,scan statistic,ill-posed problem,deconvolution,super-resolution,Gumbel extreme value limit","Proksch, Katharina@University of Gottingen::Werner, Frank@Max Planck Society::Munk, Axel@University of Gottingen"
ASSESSING ROBUSTNESS OF CLASSIFICATION USING AN ANGULAR BREAKDOWN POINT,ANNALS OF STATISTICS,29,"Robustness is a desirable property for many statistical techniques. As an important measure of robustness, the breakdown point has been widely used for regression problems and many other settings. Despite the existing development, we observe that the standard breakdown point criterion is not directly applicable for many classification problems. In this paper, we propose a new breakdown point criterion, namely angular breakdown point, to better quantify the robustness of different classification methods. Using this new breakdown point criterion, we study the robustness of binary large margin classification techniques, although the idea is applicable to general classification methods. Both bounded and unbounded loss functions with linear and kernel learning are considered. These studies provide useful insights on the robustness of different classification methods. Numerical results further confirm our theoretical findings.","Breakdown point,classification,loss function,reproducing kernel Hilbert spaces,robustness","Zhao, Junlong@Beijing Normal University::Yu, Guan@State University of New York (SUNY) System@State University of New York (SUNY) Buffalo::Liu, Yufeng@University of North Carolina School of Medicine@University of North Carolina@University of North Carolina Chapel Hill"
A NEW SCOPE OF PENALIZED EMPIRICAL LIKELIHOOD WITH HIGH-DIMENSIONAL ESTIMATING EQUATIONS,ANNALS OF STATISTICS,41,"Statistical methods with empirical likelihood (EL) are appealing and effective especially in conjunction with estimating equations for flexibly and adaptively incorporating data information. It is known that EL approaches encounter difficulties when dealing with high-dimensional problems. To overcome the challenges, we begin our study with investigating high-dimensional EL from a new scope targeting at high-dimensional sparse model parameters. We show that the new scope provides an opportunity for relaxing the stringent requirement on the dimensionality of the model parameters. Motivated by the new scope, we then propose a new penalized EL by applying two penalty functions respectively regularizing the model parameters and the associated Lagrange multiplier in the optimizations of EL. By penalizing the Lagrange multiplier to encourage its sparsity, a drastic dimension reduction in the number of estimating equations can be achieved. Most attractively, such a reduction in dimensionality of estimating equations can be viewed as a selection among those high-dimensional estimating equations, resulting in a highly parsimonious and effective device for estimating high-dimensional sparse model parameters. Allowing both the dimensionalities of model parameters and estimating equations growing exponentially with the sample size, our theory demonstrates that our new penalized EL estimator is sparse and consistent with asymptotically normally distributed nonzero components. Numerical simulations and a real data analysis show that the proposed penalized EL works promisingly.","Empirical likelihood,estimating equations,high-dimensional statistical methods,moment selection,penalized likelihood","Chang, Jinyuan@Southwestern University of Finance & Economics - China::Tang, Cheng Yong@Pennsylvania Commonwealth System of Higher Education (PCSHE)@Temple University::Wu, Tong Tong@University of Rochester"
APPROXIMATE l(0)-PENALIZED ESTIMATION OF PIECEWISE-CONSTANT SIGNALS ON GRAPHS,ANNALS OF STATISTICS,73,"We study recovery of piecewise-constant signals on graphs by the estimator minimizing an l(0)-edge-penalized objective. Although exact minimization of this objective may be computationally intractable, we show that the same statistical risk guarantees are achieved by the alpha-expansion algorithm which computes an approximate minimizer in polynomial time. We establish that for graphs with small average vertex degree, these guarantees are minimax rate-optimal over classes of edge-sparse signals. For spatially inhomogeneous graphs, we propose minimization of an edge-weighted objective where each edge is weighted by its effective resistance or another measure of its contribution to the graph's connectivity. We establish minimax optimality of the resulting estimators over corresponding edge-weighted sparsity classes. We show theoretically that these risk guarantees are not always achieved by the estimator minimizing the l(1)/total-variation relaxation, and empirically that the l(0)-based estimates are more accurate in high signal-to-noise settings.","Approximation algorithm,graph cut,effective resistance,total-variation denoising","Fan, Zhou@Stanford University::Guan, Leying@Stanford University"
"MULTICLASS CLASSIFICATION, INFORMATION, DIVERGENCE AND SURROGATE RISK",ANNALS OF STATISTICS,33,"We provide a unifying view of statistical information measures, multiway Bayesian hypothesis testing, loss functions for multiclass classification problems and multidistribution f-divergences, elaborating equivalence results between all of these objects, and extending existing results for binary outcome spaces to more general ones. We consider a generalization of f-divergences to multiple distributions, and we provide a constructive equivalence between divergences, statistical information (in the sense of DeGroot) and losses for multiclass classification. A major application of our results is in multiclass classification problems in which we must both infer a discriminant function gamma-for making predictions on a label Y from datum X- and a data representation (or, in the setting of a hypothesis testing problem, an experimental design), represented as a quantizer q from a family of possible quantizers Q. In this setting, we characterize the equivalence between loss functions, meaning that optimizing either of two losses yields an optimal discriminant and quantizer q, complementing and extending earlier results of Nguyen et al. [Ann. Statist. 37 (2009) 876-904] to the multiclass case. Our results provide a more substantial basis than standard classification calibration results for comparing different losses: we describe the convex losses that are consistent for jointly choosing a data representation and minimizing the (weighted) probability of error in multiclass classification problems.","f-divergence,risk,surrogate loss function,hypothesis test","Duchi, John@Stanford University::Khosravi, Khashayar@Stanford University::Ruan, Feng@Stanford University"
"HALFSPACE DEPTHS FOR SCATTER, CONCENTRATION AND SHAPE MATRICES",ANNALS OF STATISTICS,40,"We propose halfspace depth concepts for scatter, concentration and shape matrices. For scatter matrices, our concept is similar to those from Chen, Gao and Ren [Robust covariance and scatter matrix estimation under Huber's contamination model (2018)] and Zhang [J. Multivariate Anal. 82 (2002) 134-165]. Rather than focusing, as in these earlier works, on deepest scatter matrices, we thoroughly investigate the properties of the proposed depth and of the corresponding depth regions. We do so under minimal assumptions and, in particular, we do not restrict to elliptical distributions nor to absolutely continuous distributions. Interestingly, fully understanding scatter halfspace depth requires considering different geometries/topologies on the space of scatter matrices. We also discuss, in the spirit of Zuo and Serfling [Ann. Statist. 28 (2000) 461-482], the structural properties a scatter depth should satisfy, and investigate whether or not these are met by scatter half space depth. Companion concepts of depth for concentration matrices and shape matrices are also proposed and studied. We show the practical relevance of the depth concepts considered in a real-data example from finance.","Curved parameter spaces,elliptical distributions,robustness,scatter matrices,shape matrices,statistical depth","Paindaveine, Davy@Universite Libre de Bruxelles::Van Bever, Germain@Universite Libre de Bruxelles"
MULTILAYER TENSOR FACTORIZATION WITH APPLICATIONS TO RECOMMENDER SYSTEMS,ANNALS OF STATISTICS,51,"Recommender systems have been widely adopted by electronic commerce and entertainment industries for individualized prediction and recommendation, which benefit consumers and improve business intelligence. In this article, we propose an innovative method, namely the recommendation engine of multilayers (REM), for tensor recommender systems. The proposed method utilizes the structure of a tensor response to integrate information from multiple modes, and creates an additional layer of nested latent factors to accommodate between-subjects dependency. One major advantage is that the proposed method is able to address the ""cold-start"" issue in the absence of information from new customers, new products or new contexts. Specifically, it provides more effective recommendations through sub-group information. To achieve scalable computation, we develop a new algorithm for the proposed method, which incorporates a maximum block improvement strategy into the cyclic blockwise-coordinate-descent algorithm. In theory, we investigate algorithmic properties for convergence from an arbitrary initial point and local convergence, along with the asymptotic consistency of estimated parameters. Finally, the proposed method is applied in simulations and IRI marketing data with 116 million observations of product sales. Numerical studies demonstrate that the proposed method outperforms existing competitors in the literature.","Cold-start problem,context-aware recommender system,maximum block improvement,nonconvex optimization,tensor completion","Bi, Xuan@Yale University::Qu, Annie@Yunnan University@University of Illinois Urbana-Champaign@University of Illinois System::Shen, Xiaotong@University of Minnesota Twin Cities@University of Minnesota System"
PRINCIPAL COMPONENT ANALYSIS FOR FUNCTIONAL DATA ON RIEMANNIAN MANIFOLDS AND SPHERES,ANNALS OF STATISTICS,44,"Functional data analysis on nonlinear manifolds has drawn recent interest. Sphere-valued functional data, which are encountered, for example, as movement trajectories on the surface of the earth are an important special case. We consider an intrinsic principal component analysis for smooth Riemannian manifold-valued functional data and study its asymptotic properties. Riemannian functional principal component analysis (RFPCA) is carried out by first mapping the manifold-valued data through Riemannian logarithm maps to tangent spaces around the Frechet mean function, and then performing a classical functional principal component analysis (FPCA) on the linear tangent spaces. Representations of the Riemannian manifold-valued functions and the eigenfunctions on the original manifold are then obtained with exponential maps. The tangent-space approximation yields upper bounds to residual variances if the Riemannian manifold has nonnegative curvature. We derive a central limit theorem for the mean function, as well as root-n uniform convergence rates for other model components. Our applications include a novel framework for the analysis of longitudinal compositional data, achieved by mapping longitudinal compositional data to trajectories on the sphere, illustrated with longitudinal fruit fly behavior patterns. RFPCA is shown to outperform an unrestricted FPCA in terms of trajectory recovery and prediction in applications and simulations.","Compositional data,dimension reduction,functional data analysis,functional principal component analysis,principal geodesic analysis,Riemannian manifold,trajectory,central limit theorem,uniform convergence","Dai, Xiongtao@University of California Davis@University of California System::Mueller, Hans-Georg@University of California Davis@University of California System"
TAIL-GREEDY BOTTOM-UP DATA DECOMPOSITIONS AND FAST MULTIPLE CHANGE-POINT DETECTION,ANNALS OF STATISTICS,70,"This article proposes a ""tail-greedy"", bottom-up transform for one-dimensional data, which results in a nonlinear but conditionally orthonormal, multiscale decomposition of the data with respect to an adaptively chosen unbalanced Haar wavelet basis. The ""tail-greediness"" of the decomposition algorithm, whereby multiple greedy steps are taken in a single pass through the data, both enables fast computation and makes the algorithm applicable in the problem of consistent estimation of the number and locations of multiple change-points in data. The resulting agglomerative change-point detection method avoids the disadvantages of the classical divisive binary segmentation, and offers very good practical performance. It is implemented in the R package breakfast, available from CRAN.","Tail-greediness,bottom-up methods,multiscale methods,segmentation,thresholding,sparsity","Fryzlewicz, Piotr@London School Economics & Political Science@University of London"
ROCKET: ROBUST CONFIDENCE INTERVALS VIA KENDALL'S TAU FOR TRANSELLIPTICAL GRAPHICAL MODELS,ANNALS OF STATISTICS,57,"Understanding complex relationships between random variables is of fundamental importance in high-dimensional statistics, with numerous applications in biological and social sciences. Undirected graphical models are often used to represent dependencies between random variables, where an edge between two random variables is drawn if they are conditionally dependent given all the other measured variables. A large body of literature exists on methods that estimate the structure of an undirected graphical model, however, little is known about the distributional properties of the estimators beyond the Gaussian setting. In this paper, we focus on inference for edge parameters in a high-dimensional transelliptical model, which generalizes Gaussian and nonparanormal graphical models. We propose ROCKET, a novel procedure for estimating parameters in the latent inverse covariance matrix. We establish asymptotic normality of ROCKET in an ultra high dimensional setting under mild assumptions, without relying on oracle model selection results. ROCKET requires the same number of samples that are known to be necessary for obtaining a root n consistent estimator of an element in the precision matrix under a Gaussian model. Hence, it is an optimal estimator under a much larger family of distributions. The result hinges on a tight control of the sparse spectral norm of the nonparametric Kendall's tau estimator of the correlation matrix, which is of independent interest. Empirically, ROCKET outperforms the nonparanormal and Gaussian models in terms of achieving accurate inference on simulated data. We also compare the three methods on real data (daily stock returns), and find that the ROCKET estimator is the only method whose behavior across subsamples agrees with the distribution predicted by the theory.","Graphical model selection,transelliptical graphical models,covariance selection,uniformly valid inference,post-model selection inference,rank-based estimation","Barber, Rina Foygel@University of Chicago::Kolar, Mladen@University of Chicago"
ADAPTIVE INVARIANT DENSITY ESTIMATION FOR ERGODIC DIFFUSIONS OVER ANISOTROPIC CLASSES,ANNALS OF STATISTICS,28,"Consider some multivariate diffusion process X = (X-t)(t >= 0) with unique invariant probability measure and associated invariant density rho, and assume that a continuous record of observations X-T = (X-t)(0 <= t <= T) of X is available. Recent results on functional inequalities for symmetric Markov semi groups are used in the statistical analysis of kernel estimators (rho) over cap (T) = (rho) over cap (T) (X-T) of rho. For the basic problem of estimation with respect to sup-norm risk under anisotropic Holder smoothness constraints, the proposed approach yields an adaptive estimator which converges at a substantially faster rate than in standard multivariate density estimation from i.i.d. observations.","Ergodic diffusion,anisotropic density estimation,adaptation","Strauch, Claudia@University of Mannheim"
ROBUST LOW-RANK MATRIX ESTIMATION,ANNALS OF STATISTICS,23,"Many results have been proved for various nuclear norm penalized estimators of the uniform sampling matrix completion problem. However, most of these estimators are not robust: in most of the cases the quadratic loss function and its modifications are used. We consider robust nuclear norm penalized estimators using two well-known robust loss functions: the absolute value loss and the Huber loss. Under several conditions on the sparsity of the problem (i.e., the rank of the parameter matrix) and on the regularity of the risk function sharp and nonsharp oracle inequalities for these estimators are shown to hold with high probability. As a consequence, the asymptotic behavior of the estimators is derived Similar error bounds are obtained under the assumption of weak sparsity, that is, the case where the matrix is assumed to be only approximately low-rank. In all of our results, we consider a high dimensional setting. In this case, this means that we assume n <= pq. Finally, various simulations confirm our theoretical results.","Matrix completion,robustness,empirical risk minimization,oracle inequality,nuclear norm,sparsity","Elsener, Andreas@ETH Zurich::van de Geer, Sara@ETH Zurich"
SIEVE BOOTSTRAP FOR FUNCTIONAL TIME SERIES,ANNALS OF STATISTICS,37,"A bootstrap procedure for functional time series is proposed which exploits a general vector autoregressive representation of the time series of Fourier coefficients appearing in the Karhunen Loeve expansion of the functional process. A double sieve-type bootstrap method is developed, which avoids the estimation of process operators and generates functional pseudo time series that appropriately mimics the dependence structure of the functional time series at hand. The method uses a finite set of functional principal components to capture the essential driving parts of the infinite dimensional process and a finite order vector autoregressive process to imitate the temporal dependence structure of the corresponding vector time series of Fourier coefficients. By allowing the number of functional principal components as well as the autoregressive order used to increase to infinity (at some appropriate rate) as the sample size increases, consistency of the functional sieve bootstrap can be established. We demonstrate this by proving a basic bootstrap central limit theorem for functional finite Fourier transforms and by establishing bootstrap validity in the context of a fully functional testing problem. A novel procedure to select the number of functional principal components is introduced while simulations illustrate the good finite sample performance of the new bootstrap method proposed.","Bootstrap,Fourier transform,principal components,Karhunen Loeve expansion,spectral density operator","Paparoditis, Efstathios@University of Cyprus"
RESTRICTED STRONG CONVEXITY IMPLIES WEAK SUBMODULARITY,ANNALS OF STATISTICS,53,"We connect high-dimensional subset selection and submodular maximization. Our results extend the work of Das and Kempe [In ICML (2011) 1057-1064] from the setting of linear regression to arbitrary objective functions. For greedy feature selection, this connection allows us to obtain strong multiplicative performance bounds on several methods without statistical modeling assumptions. We also derive recovery guarantees of this form under standard assumptions. Our work shows that greedy algorithms perform within a constant factor from the best possible subset-selection solution for a broad class of general objective functions. Our methods allow a direct control over the number of obtained features as opposed to regularization parameters that only implicitly control sparsity. Our proof technique uses the concept of weak submodularity initially defined by Das and Kempe. We draw a connection between convex analysis and submodular set function theory which may be of independent interest for other statistical learning applications that have combinatorial structure.","Submodular functions,greedy algorithms,restricted strong convexity,subset selection","Elenberg, Ethan R.@University of Texas Austin@University of Texas System::Khanna, Rajiv@University of Texas Austin@University of Texas System::Dimakis, Alexandros G.@University of Texas Austin@University of Texas System::Negahban, Sahand@Yale University"
SLOPE MEETS LASSO: IMPROVED ORACLE BOUNDS AND OPTIMALITY,ANNALS OF STATISTICS,29,"We show that two polynomial time methods, a Lasso estimator with adaptively chosen tuning parameter and a Slope estimator, adaptively achieve the minimax prediction and l(2) estimation rate (s/n)log(p/s) in high-dimensional linear regression on the class of s-sparse vectors in R-P. This is done under the Restricted Eigenvalue (RE) condition for the Lasso and under a slightly more constraining assumption on the design for the Slope. The main results have the form of sharp oracle inequalities accounting for the model misspecification error. The minimax optimal bounds are also obtained for the l(q) estimation errors with 1 <= q <= 2 when the model is well specified. The results are nonasymptotic, and hold both in probability and in expectation. The assumptions that we impose on the design are satisfied with high probability for a large class of random matrices with independent and possibly anisotropically distributed rows. We give a comparative analysis of conditions, under which oracle bounds for the Lasso and Slope estimators can be obtained. In particular, we show that several known conditions, such as the RE condition and the sparse eigenvalue condition are equivalent if the l(2)-norms of regressors are uniformly bounded.","Sparse linear regression,minimax rates,high-dimensional statistics,Slope,Lasso","Bellec, Pierre C.@Universite Paris Saclay (ComUE)@Rutgers State University New Brunswick@ENSAE ParisTech::Lecue, Guillaume@Universite Paris Saclay (ComUE)@Centre National de la Recherche Scientifique (CNRS)@ENSAE ParisTech::Tsybakov, Alexandre B.@Universite Paris Saclay (ComUE)@ENSAE ParisTech"
LOCAL ASYMPTOTIC EQUIVALENCE OF PURE STATES ENSEMBLES AND QUANTUM GAUSSIAN WHITE NOISE,ANNALS OF STATISTICS,58,"Quantum technology is increasingly relying on specialised statistical inference methods for analysing quantum measurement data. This motivates the development of ""quantum statistics"", a field that is shaping up at the overlap of quantum physics and ""classical"" statistics. One of the less investigated topics to date is that of statistical inference for infinite dimensional quantum systems, which can be seen as quantum counterpart of nonparametric statistics. In this paper, we analyse the asymptotic theory of quantum statistical models consisting of ensembles of quantum systems which are identically prepared in a pure state. In the limit of large ensembles, we establish the local asymptotic equivalence (LAE) of this i.i.d. model to a quantum Gaussian white noise model. We use the LAE result in order to establish minimax rates for the estimation of pure states belonging to Hermite Sobolev classes of wave functions. Moreover, for quadratic functional estimation of the same states we note an elbow effect in the rates, whereas for testing a pure state a sharp parametric rate is attained over the nonparametric Hermite Sobolev class.","Le Cam distance,local asymptotic equivalence,quantum Gaussian process,quantum Gaussian sequence,quantum states ensemble,nonparametric estimation,quadratic functionals,nonparametric sharp testing rates","Butucea, Cristina@Universite Paris Saclay (ComUE)@Universite Paris Saclay@ENSAE ParisTech::Guta, Madalin@University of Nottingham::Nussbaum, Michael@Cornell University"
EXTREMAL QUANTILE TREATMENT EFFECTS,ANNALS OF STATISTICS,53,"This paper establishes an asymptotic theory and inference method for quantile treatment effect estimators when the quantile index is close to or equal to zero. Such quantile treatment effects are of interest in many applications, such as the effect of maternal smoking on an infant's adverse birth outcomes. When the quantile index is close to zero, the sparsity of data jeopardizes conventional asymptotic theory and bootstrap inference. When the quantile index is zero, there are no existing inference methods directly applicable in the treatment effect context. This paper addresses both of these issues by proposing new inference methods that are shown to be asymptotically valid as well as having adequate finite sample properties.","Extreme quantile,intermediate quantile","Zhang, Yichong@Singapore Management University"
OPTIMAL MAXIMIN L-1-DISTANCE LATIN HYPERCUBE DESIGNS BASED ON GOOD LATTICE POINT DESIGNS,ANNALS OF STATISTICS,34,"Maximin distance Latin hypercube designs are commonly used for computer experiments, but the construction of such designs is challenging. We construct a series of maximin Latin hypercube designs via Williams transformations of good lattice point designs. Some constructed designs are optimal under the maximin L-1-distance criterion, while others are asymptotically optimal. Moreover, these designs are also shown to have small pairwise correlations between columns.","Computer experiment,correlation,space-filling design,Williams transformation","Wang, Lin@University of California Los Angeles@University of California System::Xiao, Qian@University System of Georgia@University of Georgia::Xu, Hongquan@University of California Los Angeles@University of California System"
RHO-ESTIMATORS REVISITED: GENERAL THEORY AND APPLICATIONS,ANNALS OF STATISTICS,18,"Following Baraud, Birge and Sart [Invent. Math. 207 (2017) 425-517], we pursue our attempt to design a robust universal estimator of the joint distribution of n independent (but not necessarily i.i.d.) observations for an Hellinger-type loss. Given such observations with an unknown joint distribution P and a dominated model Q for P, we build an estimator P based on Q (a rho-estimator) and measure its risk by an Hellinger-type distance. When P does belong to the model, this risk is bounded by some quantity which relies on the local complexity of the model in a vicinity of P. In most situations, this bound corresponds to the minimax risk over the model (up to a possible logarithmic factor). When P does not belong to the model, its risk involves an additional bias term proportional to the distance between P and Q, whatever the true distribution P. From this point of view, this new version of rho-estimators improves upon the previous one described in Baraud, Birge and Sart [Invent. Math. 207 (2017) 425-517] which required that P be absolutely continuous with respect to some known reference measure. Further additional improvements have been brought as compared to the former construction. In particular, it provides a very general treatment of the regression framework with random design as well as a computationally tractable procedure for aggregating estimators. We also give some conditions for the maximum likelihood estimator to be a p-estimator. Finally, we consider the situation where the statistician has at her or his disposal many different models and we build a penalized version of the rho-estimator for model selection and adaptation purposes. In the regression setting, this penalized estimator not only allows one to estimate the regression function but also the distribution of the errors.","rho-estimation,robust estimation,density estimation,regression with random design,statistical models,maximum likelihood estimators,metric dimension,VC-classes","Baraud, Yannick@Universite Cote d'Azur (ComUE)::Birge, Lucien@Sorbonne Universite@Centre National de la Recherche Scientifique (CNRS)"
"THINK GLOBALLY, FIT LOCALLY UNDER THE MANIFOLD SETUP: ASYMPTOTIC ANALYSIS OF LOCALLY LINEAR EMBEDDING",ANNALS OF STATISTICS,36,"Since its introduction in 2000, Locally Linear Embedding (LLE) has been widely applied in data science. We provide an asymptotical analysis of LLE under the manifold setup. We show that for a general manifold, asymptotically we may not obtain the Laplace-Beltrami operator, and the result may depend on nonuniform sampling unless a correct regularization is chosen. We also derive the corresponding kernel function, which indicates that LLE is not a Markov process. A comparison with other commonly applied nonlinear algorithms, particularly a diffusion map, is provided and its relationship with locally linear regression is also discussed.","Locally linear embedding,diffusion maps,dimension reduction,locally linear regression,measurement error","Wu, Hau-Tieng@Duke University::Wu, Nan@University of Toronto"
NONPARAMETRIC COVARIATE-ADJUSTED RESPONSE-ADAPTIVE DESIGN BASED ON A FUNCTIONAL URN MODEL,ANNALS OF STATISTICS,50,"In this paper, we propose a general class of covariate-adjusted response adaptive (CARA) designs based on a new functional urn model. We prove strong consistency concerning the functional urn proportion and the proportion of subjects assigned to the treatment groups, in the whole study and for each covariate profile, allowing the distribution of the responses conditioned on covariates to be estimated nonparametrically. In addition, we establish joint central limit theorems for the above quantities and the sufficient statistics of features of interest, which allow to construct procedures to make inference on the conditional response distributions. These results are then applied to typical situations concerning Gaussian and binary responses.","Clinical trials,covariate-adjusted analysis,inference,large sample theory,personalized medicine,randomization","Aletti, Giacomo@University of Milan::Ghiglietti, Andrea@Catholic University of the Sacred Heart::Rosenberger, William F.@George Mason University"
FINDING A LARGE SUBMATRIX OF A GAUSSIAN RANDOM MATRIX,ANNALS OF STATISTICS,18,"We consider the problem of finding a k xk submatrix of an nxn matrix with i.i.d. standard Gaussian entries, which has a large average entry. It was shown in [Bhamidi, Dey and Nobel (2012)] using nonconstructive methods that the largest average value of a k x k submatrix is 2(1 + o(1)) root log n/k, with high probability (w.h.p.), when k = O(log n/log log n). In the same paper, evidence was provided that a natural greedy algorithm called the Largest Average Submatrix (LAS) for a constant k should produce a matrix with average entry at most (1+ o(1)) root 2 logn/k, namely approximately root 2 smaller than the global optimum, though no formal proof of this fact was provided.

In this paper, we show that the average entry of the matrix produced by the LAS algorithm is indeed (1+ o(1)) root 2 logn/k w.h.p. when k is constant and n grows. Then, by drawing an analogy with the problem of finding cliques in random graphs, we propose a simple greedy algorithm which produces a kxk matrix with asymptotically the same average value (1 + o(1)) root 2 logn/k w.h.p., for k = o(log n). Since the greedy algorithm is the best known algorithm for finding cliques in random graphs, it is tempting to believe that beating the factor root 2 performance gap suffered by both algorithms might be very challenging. Surprisingly, we construct a very simple algorithm which produces a k x k matrix with average value (1 + ok(1) + o(1))(4/3) root 2 logn/k for k = o((log n)(1.5)), that is, with the asymptotic factor 4/3 when k grows.

To get an insight into the algorithmic hardness of this problem, and motivated by methods originating in the theory of spin glasses, we conduct the so-called expected overlap analysis of matrices with average value asymptotically (1+ o(1)) alpha root 2 logn/k for a fixed value alpha is an element of [1, root 2]. The overlap corresponds to the number of common rows and the number of common columns for pairs of matrices achieving this value (see the paper for details). We discover numerically an intriguing phase transition at alpha* (sic) 5 root 2/(3 root 3) approximate to 1.3608...is an element of[4/3, root 2]: when alpha < alpha* the space of overlaps is a continuous subset of [0, 1] 2, whereas alpha = alpha* marks the onset of discontinuity, and as a result the model exhibits the Overlap Gap Property (OGP) when alpha > alpha*, appropriately defined. We conjecture that the OGP observed for alpha > alpha* also marks the onset of the algorithmic hardness-no polynomial time algorithm exists for finding matrices with average value at least (1+ o(1)) alpha root 2 logn/k, when alpha > alpha* and k is a mildly growing function of n.","Random matrix,random graphs,maximum clique,submatrix detection,computational complexity,overlap gap property","Gamarnik, David@Massachusetts Institute of Technology (MIT)::Li, Quan@Massachusetts Institute of Technology (MIT)"
SUPPORT POINTS,ANNALS OF STATISTICS,72,"This paper introduces a new way to compact a continuous probability distribution F into a set of representative points called support points. These points are obtained by minimizing the energy distance, a statistical potential measure initially proposed by Szekely and Rizzo [InterStat 5 (2004) 1-6] for testing goodness-of-fit. The energy distance has two appealing features. First, its distance-based structure allows us to exploit the duality between powers of the Euclidean distance and its Fourier transform for theoretical analysis. Using this duality, we show that support points converge in distribution to F, and enjoy an improved error rate to Monte Carlo for integrating a large class of functions. Second, the minimization of the energy distance can be formulated as a difference-of-convex program, which we manipulate using two algorithms to efficiently generate representative point sets. In simulation studies, support points provide improved integration performance to both Monte Carlo and a specific quasi-Monte Carlo method. Two important applications of support points are then highlighted: (a) as a way to quantify the propagation of uncertainty in expensive simulations and (b) as a method to optimally compact Markov chain Monte Carlo (MCMC) samples in Bayesian computation.","Bayesian computation,energy distance,Monte Carlo,numerical integration,quasi-Monte Carlo,representative points","Mak, Simon@University System of Georgia@Georgia Institute of Technology::Joseph, V. Roshan@University System of Georgia@Georgia Institute of Technology"
DEBIASING THE LASSO: OPTIMAL SAMPLE SIZE FOR GAUSSIAN DESIGNS,ANNALS OF STATISTICS,60,"Performing statistical inference in high-dimensional models is challenging because of the lack of precise information on the distribution of high-dimensional regularized estimators.

Here, we consider linear regression in the high-dimensional regime p >> n and the Lasso estimator: we would like to perform inference on the parameter vector theta*is an element of R-p. Important progress has been achieved in computing confidence intervals and p-values for single coordinates. theta(i)*, i is an element of{1,..., p}. A key role in these new inferential methods is played by a certain debiased estimator (theta) over cap (d). Earlier work establishes that, under suitable assumptions on the design matrix, the coordinates of (theta) over cap (d) are asymptotically Gaussian provided the true parameters vector theta* is s(0)-sparse with s(0) = o(root n/log p).

The condition s(0) = o(root n/log p) is considerably stronger than the one for consistent estimation, namely s(0) = o(n/log p). In this paper, we consider Gaussian designs with known or unknown population covariance. When the covariance is known, we prove that the debiased estimator is asymptotically Gaussian under the nearly optimal condition s(0) = o(n/(log p)(2)).

The same conclusion holds if the population covariance is unknown but can be estimated sufficiently well. For intermediate regimes, we describe the trade-off between sparsity in the coefficients theta*, and sparsity in the inverse covariance of the design. We further discuss several applications of our results beyond high-dimensional inference. In particular, we propose a thresholded Lasso estimator that is minimax optimal up to a factor 1 + o(n)(1) for i.i.d. Gaussian designs.","Lasso,high-dimensional regression,confidence intervals,hypothesis testing,bias and variance,sample size","Javanmard, Adel@University of Southern California::Montanari, Andrea@Stanford University"
MARGINS OF DISCRETE BAYESIAN NETWORKS,ANNALS OF STATISTICS,35,"Bayesian network models with latent variables are widely used in statistics and machine learning. In this paper, we provide a complete algebraic characterization of these models when the observed variables are discrete and no assumption is made about the state-space of the latent variables. We show that it is algebraically equivalent to the so-called nested Markov model, meaning that the two are the same up to inequality constraints on the joint probabilities. In particular, these two models have the same dimension, differing only by inequality constraints for which there is no general description. The nested Markov model is therefore the closest possible description of the latent variable model that avoids consideration of inequalities. A consequence of this is that the constraint finding algorithm of Tian and Pearl [In Proceedings of the 18th Conference on Uncertainty in Artificial Intelligence (2002) 519-527] is complete for finding equality constraints.

Latent variable models suffer from difficulties of unidentifiable parameters and nonregular asymptotics; in contrast the nested Markov model is fully identifiable, represents a curved exponential family of known dimension, and can easily be fitted using an explicit parameterization.","Algebraic statistics,Bayesian network,latent variable model,nested Markov model,Verma constraint","Evans, Robin J.@University of Oxford"
MULTI-THRESHOLD ACCELERATED FAILURE TIME MODEL,ANNALS OF STATISTICS,33,"A two-stage procedure for simultaneously detecting multiple thresholds and achieving model selection in the segmented accelerated failure time (AFT) model is developed in this paper. In the first stage, we formulate the threshold problem as a group model selection problem so that a concave 2-norm group selection method can be applied. In the second stage, the thresholds are finalized via a refining method. We establish the strong consistency of the threshold estimates and regression coefficient estimates under some mild technical conditions. The proposed procedure performs satisfactorily in our simulation studies. Its real world applicability is demonstrated via analyzing a follicular lymphoma data.","Break points,MCP penalty,SCAD penalty,Stute estimator,threshold regression","Li, Jialiang@National University of Singapore::Jin, Baisuo@Chinese Academy of Sciences@University of Science & Technology of China"
MEASURING AND TESTING FOR INTERVAL QUANTILE DEPENDENCE,ANNALS OF STATISTICS,28,"In this article, we introduce the notion of interval quantile independence which generalizes the notions of statistical independence and quantile independence. We suggest an index to measure and test departure from interval quantile independence. The proposed index is invariant to monotone transformations, nonnegative and equals zero if and only if the interval quantile independence holds true. We suggest a moment estimate to implement the test. The resultant estimator is root-n-consistent if the index is positive and n-consistent otherwise, leading to a consistent test of interval quantile independence. The asymptotic distribution of the moment estimator is free of parent distribution, which facilitates to decide the critical values for tests of interval quantile independence. When our proposed index is used to perform feature screening for ultrahigh dimensional data, it has the desirable sure screening property.","Correlation,independence,quantile regression,rank test,sure screening property","Zhu, Liping@Renmin University of China::Zhang, Yaowu@Shanghai University of Finance & Economics::Xu, Kai@Shanghai University of Finance & Economics"
BARYCENTRIC SUBSPACE ANALYSIS ON MANIFOLDS,ANNALS OF STATISTICS,49,"This paper investigates the generalization of Principal Component Analysis (PCA) to Riemannian manifolds. We first propose a new and general type of family of subspaces in manifolds that we call barycentric subspaces. They are implicitly defined as the locus of points which are weighted means of k+1 reference points. As this definition relies on points and not on tangent vectors, it can also be extended to geodesic spaces which are not Riemannian. For instance, in stratified spaces, it naturally allows principal subspaces that span several strata, which is impossible in previous generalizations of PCA. We show that barycentric subspaces locally define a submanifold of dimension k which generalizes geodesic subspaces.

Second, we rephrase PCA in Euclidean spaces as an optimization on flags of linear subspaces (a hierarchy of properly embedded linear subspaces of increasing dimension). We show that the Euclidean PCA minimizes the Accumulated Unexplained Variances by all the subspaces of the flag (AUV). Barycentric subspaces are naturally nested, allowing the construction of hierarchically nested subspaces. Optimizing the AUV criterion to optimally approximate data points with flags of affine spans in Riemannian manifolds lead to a particularly appealing generalization of PCA on manifolds called Barycentric Subspace Analysis (BSA).","Manifold,Frechet mean,barycenter,flag of subspaces,PCA","Pennec, Xavier@Universite Cote d'Azur (ComUE)@Inria"
THE LANDSCAPE OF EMPIRICAL RISK FOR NONCONVEX LOSSES,ANNALS OF STATISTICS,47,"Most high-dimensional estimation methods propose to minimize a cost function (empirical risk) that is a sum of losses associated to each data point (each example). In this paper, we focus on the case of nonconvex losses. Classical empirical process theory implies uniform convergence of the empirical (or sample) risk to the population risk. While under additional assumptions, uniform convergence implies consistency of the resulting M-estimator, it does not ensure that the latter can be computed efficiently.

In order to capture the complexity of computing M-estimators, we study the landscape of the empirical risk, namely its stationary points and their properties. We establish uniform convergence of the gradient and Hessian of the empirical risk to their population counterparts, as soon as the number of samples becomes larger than the number of unknown parameters (modulo logarithmic factors). Consequently, good properties of the population risk can be carried to the empirical risk, and we are able to establish one-to-one correspondence of their stationary points. We demonstrate that in several problems such as nonconvex binary classification, robust regression and Gaussian mixture model, this result implies a complete characterization of the landscape of the empirical risk, and of the convergence properties of descent algorithms.

We extend our analysis to the very high-dimensional setting in which the number of parameters exceeds the number of samples, and provides a characterization of the empirical risk landscape under a nearly information-theoretically minimal condition. Namely, if the number of samples exceeds the sparsity of the parameters vector (modulo logarithmic factors), then a suitable uniform convergence result holds. We apply this result to nonconvex binary classification and robust regression in very high-dimension.","Nonconvex optimization,empirical risk minimization,landscape,uniform convergence","Mei, Song@Stanford University::Bai, Yu@Stanford University::Montanari, Andrea@Stanford University"
WEAK CONVERGENCE OF A PSEUDO MAXIMUM LIKELIHOOD ESTIMATOR FOR THE EXTREMAL INDEX,ANNALS OF STATISTICS,30,"The extremes of a stationary time series typically occur in clusters. A primary measure for this phenomenon is the extremal index, representing the reciprocal of the expected cluster size. Both disjoint and sliding blocks estimator for the extremal index are analyzed in detail. In contrast to many competitors, the estimators only depend on the choice of one parameter sequence. We derive an asymptotic expansion, prove asymptotic normality and show consistency of an estimator for the asymptotic variance. Explicit calculations in certain models and a finite-sample Monte Carlo simulation study reveal that the sliding blocks estimator outperforms other blocks estimators, and that it is competitive to runs- and inter-exceedance estimators in various models. The methods are applied to a variety of financial time series.","Clusters of extremes,extremal index,stationary time series,mixing coefficients,block maxima","Berghaus, Betina@Ruhr University Bochum::Buecher, Axel@Ruhr University Bochum"
SEMIPARAMETRIC EFFICIENCY BOUNDS FOR HIGH-DIMENSIONAL MODELS,ANNALS OF STATISTICS,22,"Asymptotic lower bounds for estimation play a fundamental role in assessing the quality of statistical procedures. In this paper, we propose a framework for obtaining semiparametric efficiency bounds for sparse high-dimensional models, where the dimension of the parameter is larger than the sample size. We adopt a semiparametric point of view: we concentrate on one-dimensional functions of a high-dimensional parameter. We follow two different approaches to reach the lower bounds: asymptotic Cramer-Rao bounds and Le Cam's type of analysis. Both of these approaches allow us to define a class of asymptotically unbiased or ""regular"" estimators for which a lower bound is derived. Consequently, we show that certain estimators obtained by de-sparsifying (or de-biasing) an l(1)-penalized M-estimator are asymptotically unbiased and achieve the lower bound on the variance: thus in this sense they are asymptotically efficient. The paper discusses in detail the linear regression model and the Gaussian graphical model.","Asymptotic efficiency,high-dimensional,sparsity,Lasso,linear regression,graphical models,Cramer-Rao bound,Le Cam's lemma","Jankova, Jana@ETH Zurich::van de Geer, Sara@ETH Zurich"
LIMIT THEOREMS FOR EIGENVECTORS OF THE NORMALIZED LAPLACIAN FOR RANDOM GRAPHS,ANNALS OF STATISTICS,45,"We prove a central limit theorem for the components of the eigenvectors corresponding to the d largest eigenvalues of the normalized Laplacian matrix of a finite dimensional random dot product graph. As a corollary, we show that for stochastic blockmodel graphs, the rows of the spectral embedding of the normalized Laplacian converge to multivariate normals and, furthermore, the mean and the covariance matrix of each row are functions of the associated vertex's block membership. Together with prior results for the eigenvectors of the adjacency matrix, we then compare, via the Chernoff information between multivariate normal distributions, how the choice of embedding method impacts subsequent inference. We demonstrate that neither embedding method dominates with respect to the inference task of recovering the latent block assignments.","Spectral clustering,random dot product graph,stochastic blockmodels,convergence of eigenvectors,Chernoff information","Tang, Minh@Johns Hopkins University::Priebe, Carey E.@Johns Hopkins University"
OPTIMALITY AND SUB-OPTIMALITY OF PCA I: SPIKED RANDOM MATRIX MODELS,ANNALS OF STATISTICS,81,"A central problem of random matrix theory is to understand the eigen-values of spiked random matrix models, introduced by Johnstone, in which a prominent eigenvector (or ""spike"") is planted into a random matrix. These distributions form natural statistical models for principal component analysis (PCA) problems throughout the sciences. Baik, Ben Arous and Peche showed that the spiked Wishart ensemble exhibits a sharp phase transition asymptotically: when the spike strength is above a critical threshold, it is possible to detect the presence of a spike based on the top eigenvalue, and below the threshold the top eigenvalue provides no information. Such results form the basis of our understanding of when PCA can detect a low-rank signal in the presence of noise. However, under structural assumptions on the spike, not all information is necessarily contained in the spectrum. We study the statistical limits of tests for the presence of a spike, including nonspectral tests. Our results leverage Le Cam's notion of contiguity and include:

(i) For the Gaussian Wigner ensemble, we show that PCA achieves the optimal detection threshold for certain natural priors for the spike.

(ii) For any non-Gaussian Wigner ensemble, PCA is sub-optimal for detection. However, an efficient variant of PCA achieves the optimal threshold (for natural priors) by pre-transforming the matrix entries.

(iii) For the Gaussian Wishart ensemble, the PCA threshold is optimal for positive spikes (for natural priors) but this is not always the case for negative spikes.","Random matrix,principal component analysis,hypothesis testing,deformed Wigner,spiked covariance,contiguity,power envelope,phase transition","Perry, Amelia@Massachusetts Institute of Technology (MIT)::Wein, Alexander S.@Massachusetts Institute of Technology (MIT)::Bandeira, Afonso S.@New York University::Moitra, Ankur@Massachusetts Institute of Technology (MIT)"
ON THE EXPONENTIALLY WEIGHTED AGGREGATE WITH THE LAPLACE PRIOR,ANNALS OF STATISTICS,86,"In this paper, we study the statistical behaviour of the Exponentially Weighted Aggregate (EWA) in the problem of high-dimensional regression with fixed design. Under the assumption that the underlying regression vector is sparse, it is reasonable to use the Laplace distribution as a prior. The resulting estimator and, specifically, a particular instance of it referred to as the Bayesian lasso, was already used in the statistical literature because of its computational convenience, even though no thorough mathematical analysis of its statistical properties was carried out. The present work fills this gap by establishing sharp oracle inequalities for the EWA with the Laplace prior. These inequalities show that if the temperature parameter is small, the EWA with the Laplace prior satisfies the same type of oracle inequality as the lasso estimator does, as long as the quality of estimation is measured by the prediction loss. Extensions of the proposed methodology to the problem of prediction with low-rank matrices are considered.","Sparsity,Bayesian lasso,oracle inequality,exponential weights,high-dimensional regression,trace regression,low-rank matrices","Dalalyan, Arnak S.@Universite Paris Saclay (ComUE)@Universite Paris Saclay@ENSAE ParisTech::Grappin, Edwin@Universite Paris Saclay (ComUE)@Universite Paris Saclay@ENSAE ParisTech::Paris, Quentin@National Research University - Higher School of Economics"
GOODNESS-OF-FIT TESTING OF ERROR DISTRIBUTION IN LINEAR MEASUREMENT ERROR MODELS,ANNALS OF STATISTICS,32,"This paper investigates a class of goodness-of-fit tests for fitting an error density in linear regression models with measurement error in covariates. Each test statistic is the integrated square difference between the deconvolution kernel density estimator of the regression model error density and a smoothed version of the null error density, an analog of the so-called Bickel and Rosenblatt test statistic. The asymptotic null distributions of the proposed test statistics are derived for both the ordinary smooth and super smooth cases. The asymptotic power behavior of the proposed tests against a fixed alternative and a class of local nonparametric alternatives for both cases is also described. The finite sample performance of the proposed test is evaluated by a simulation study. The simulation study shows some superiority of the proposed test over some other tests. Finally, a real data is used to illustrate the proposed test.","Deconvolution density estimators,L-2-distance tests","Koul, Hira L.@Michigan State University::Song, Weixing@Kansas State University::Zhu, Xiaoqing@Michigan State University"
CONSISTENCY AND CONVERGENCE RATE OF PHYLOGENETIC INFERENCE VIA REGULARIZATION,ANNALS OF STATISTICS,59,"It is common in phylogenetics to have some, perhaps partial, information about the overall evolutionary tree of a group of organisms and wish to find an evolutionary tree of a specific gene for those organisms. There may not be enough information in the gene sequences alone to accurately reconstruct the correct ""gene tree."" Although the gene tree may deviate from the ""species tree"" due to a variety of genetic processes, in the absence of evidence to the contrary it is parsimonious to assume that they agree. A common statistical approach in these situations is to develop a likelihood penalty to incorporate such additional information. Recent studies using simulation and empirical data suggest that a likelihood penalty quantifying concordance with a species tree can significantly improve the accuracy of gene tree reconstruction compared to using sequence data alone. However, the consistency of such an approach has not yet been established, nor have convergence rates been bounded. Because phylogenetics is a nonstandard inference problem, the standard theory does not apply. In this paper, we propose a penalized maximum likelihood estimator for gene tree reconstruction, where the penalty is the square of the Billera-Holmes-Vogtmann geodesic distance from the gene tree to the species tree. We prove that this method is consistent, and derive its convergence rate for estimating the discrete gene tree structure and continuous edge lengths (representing the amount of evolution that has occurred on that branch) simultaneously. We find that the regularized estimator is ""adaptive fast converging,"" meaning that it can reconstruct all edges of length greater than any given threshold from gene sequences of polynomial length. Our method does not require the species tree to be known exactly; in fact, our asymptotic theory holds for any such guide tree.","Phylogenetics,tree reconstruction,gene tree,species tree,maximum likelihood estimator,regularization","Vu Dinh@Fred Hutchinson Cancer Center::Lam Si Tung Ho@University of California Los Angeles@University of California System::Suchard, Marc A.@University of California Los Angeles@University of California System::Matsen, Frederick A.@Fred Hutchinson Cancer Center"
OPTIMAL SHRINKAGE OF EIGENVALUES IN THE SPIKED COVARIANCE MODEL,ANNALS OF STATISTICS,68,"We show that in a common high-dimensional covariance model, the choice of loss function has a profound effect on optimal estimation.

In an asymptotic framework based on the spiked covariance model and use of orthogonally invariant estimators, we show that optimal estimation of the population covariance matrix boils down to design of an optimal shrinker eta that acts elementwise on the sample eigenvalues. Indeed, to each loss function there corresponds a unique admissible eigenvalue shrinker eta* dominating all other shrinkers. The shape of the optimal shrinker is determined by the choice of loss function and, crucially, by inconsistency of both eigenvalues and eigenvectors of the sample covariance matrix.

Details of these phenomena and closed form formulas for the optimal eigenvalue shrinkers are worked out for a menagerie of 26 loss functions for covariance estimation found in the literature, including the Stein, Entropy, Divergence, Frechet, Bhattacharya/Matusita, Frobenius Norm, Operator Norm, Nuclear Norm and Condition Number losses.","Covariance estimation,optimal shrinkage,Stein loss,entropy loss,divergence loss,Frechet distance,Bhattacharya/Matusita affinity,condition number loss,high-dimensional ssymptotics,spiked covariance","Donoho, David@Stanford University::Gavish, Matan@Hebrew University of Jerusalem::Johnstone, Iain@Stanford University"
LARGE COVARIANCE ESTIMATION THROUGH ELLIPTICAL FACTOR MODELS,ANNALS OF STATISTICS,96,"We propose a general Principal Orthogonal complEment Thresholding (POET) framework for large-scale covariance matrix estimation based on the approximate factor model. A set of high-level sufficient conditions for the procedure to achieve optimal rates of convergence under different matrix norms is established to better understand how POET works. Such a framework allows us to recover existing results for sub-Gaussian data in a more transparent way that only depends on the concentration properties of the sample covariance matrix. As a new theoretical contribution, for the first time, such a framework allows us to exploit conditional sparsity covariance structure for the heavy-tailed data. In particular, for the elliptical distribution, we propose a robust estimator based on the marginal and spatial Kendall's tau to satisfy these conditions. In addition, we study conditional graphical model under the same framework. The technical tools developed in this paper are of general interest to high-dimensional principal component analysis. Thorough numerical results are also provided to back up the developed theory.","Principal component analysis,approximate factor model,sub-Gaussian family,elliptical distribution,conditional graphical model,marginal and spatial Kendall's tau","Fan, Jianqing@Fudan University@Princeton University::Liu, Han@Princeton University::Wang, Weichen@Princeton University"
CURRENT STATUS LINEAR REGRESSION,ANNALS OF STATISTICS,30,"We construct root n-consistent and asymptotically normal estimates for the finite dimensional regression parameter in the current status linear regression model, which do not require any smoothing device and are based on maximum likelihood estimates (MLEs) of the infinite dimensional parameter. We also construct estimates, again only based on these MLEs, which are arbitrarily close to efficient estimates, if the generalized Fisher information is finite. This type of efficiency is also derived under minimal conditions for estimates based on smooth nonmonotone plug-in estimates of the distribution function. Algorithms for computing the estimates and for selecting the bandwidth of the smooth estimates with a bootstrap method are provided. The connection with results in the econometric literature is also pointed out.","Current status,linear regression,MLE,semiparametric model","Groeneboom, Piet@Delft University of Technology::Hendrickx, Kim@Hasselt University"
JUMP FILTERING AND EFFICIENT DRIFT ESTIMATION FOR LEVY-DRIVEN SDES,ANNALS OF STATISTICS,32,"The problem of drift estimation for the solution X of a stochastic differential equation with Levy-type jumps is considered under discrete high-frequency observations with a growing observation window. An efficient and asymptotically normal estimator for the drift parameter is constructed under minimal conditions on the jump behavior and the sampling scheme. In the case of a bounded jump measure density, these conditions reduce to n Delta(3-epsilon)(n)-> 0, where n is the number of observations and Delta(n), is the maximal sampling step. This result relaxes the condition n Delta(2)(n)-> 0 usually required for joint estimation of drift and diffusion coefficient for SDEs with jumps. The main challenge in this estimation problem stems from the appearance of the unobserved continuous part X-c in the likelihood function. In order to construct the drift estimator, we recover this continuous part from discrete observations. More precisely, we estimate, in a nonparametric way, stochastic integrals with respect to X-c. Convergence results of independent interest are proved for these nonparametric estimators.","Levy-driven SDE,efficient drift estimation,maximum likelihood estimation,high frequency data,ergodic properties","Gloter, Arnaud@Universite Paris Saclay (ComUE)@Universite Paris Saclay@Universite d'Evry-Val-d'Essonne::Loukianova, Dasha@Universite Paris Saclay (ComUE)@Universite Paris Saclay@Universite d'Evry-Val-d'Essonne::Mai, Hilmar@Universite Paris Saclay (ComUE)@ENSAE ParisTech"
PARETO QUANTILES OF UNLABELED TREE OBJECTS,ANNALS OF STATISTICS,41,"In this paper, we consider a set of unlabeled tree objects with topological and geometric properties. For each data object, two curve representations are developed to characterize its topological and geometric aspects. We further define the notions of topological and geometric medians as well as quantiles based on both representations. In addition, we take a novel approach to define the Pareto medians and quantiles through a multi-objective optimization problem. In particular, we study two different objective functions which measure the topological variation and geometric variation, respectively. Analytical solutions are provided for topological and geometric medians and quantiles, and in general, for Pareto medians and quantiles, the genetic algorithm is implemented. The proposed methods are applied to analyze a data set of pyramidal neurons.","Data object,genetic algorithm,multi-objective optimization,object oriented data,tree-structured data","Sienkiewicz, Ela@Colorado State University::Wang, Haonan@Colorado State University"
EFFICIENT AND ADAPTIVE LINEAR REGRESSION IN SEMI-SUPERVISED SETTINGS,ANNALS OF STATISTICS,30,"We consider the linear regression problem under semi-supervised settings wherein the available data typically consists of: (i) a small or moderate sized ""labeled"" data, and (ii) a much larger sized ""unlabeled"" data. Such data arises naturally from settings where the outcome, unlike the covariates, is expensive to obtain, a frequent scenario in modern studies involving large databases like electronic medical records (EMR). Supervised estimators like the ordinary least squares (OLS) estimator utilize only the labeled data. It is often of interest to investigate if and when the unlabeled data can be exploited to improve estimation of the regression parameter in the adopted linear model.

In this paper, we propose a class of ""Efficient and Adaptive Semi-Supervised Estimators"" (EASE) to improve estimation efficiency. The EASE are two-step estimators adaptive to model mis-specification, leading to improved (optimal in some cases) efficiency under model mis-specification, and equal (optimal) efficiency under a linear model. This adaptive property, often unaddressed in the existing literature, is crucial for advocating ""safe"" use of the unlabeled data. The construction of EASE primarily involves a flexible ""semi-nonparametric"" imputation, including a smoothing step that works well even when the number of covariates is not small; and a follow up ""refitting"" step along with a cross-validation (CV) strategy both of which have useful practical as well as theoretical implications towards addressing two important issues: under-smoothing and over-fitting. We establish asymptotic results including consistency, asymptotic normality and the adaptive properties of EASE. We also provide influence function expansions and a ""double"" CV strategy for inference. The results are further validated through extensive simulations, followed by application to an EMR study on auto-immunity.","Semi-supervised linear regression,semiparametric inference,model mis-specification,adaptive estimation,semi-nonparametric imputation","Chakrabortty, Abhishek@University of Pennsylvania::Cai, Tianxi@Harvard University"
CONVEXIFIED MODULARITY MAXIMIZATION FOR DEGREE-CORRECTED STOCHASTIC BLOCK MODELS,ANNALS OF STATISTICS,48,"The stochastic block model (SBM), a popular framework for studying community detection in networks, is limited by the assumption that all nodes in the same community are statistically equivalent and have equal expected degrees. The degree-corrected stochastic block model (DCSBM) is a natural extension of SBM that allows for degree heterogeneity within communities. To find the communities under DCSBM, this paper proposes a convexified modularity maximization approach, which is based on a convex programming relaxation of the classical (generalized) modularity maximization formulation, followed by a novel doubly-weighted l(1)-norm k-medoids procedure. We establish nonasymptotic theoretical guarantees for approximate and perfect clustering, both of which build on a new degree-corrected density gap condition. Our approximate clustering results are insensitive to the minimum degree, and hold even in sparse regime with bounded average degrees. In the special case of SBM, our theoretical guarantees match the best-known results of computationally feasible algorithms. Numerically, we provide an efficient implementation of our algorithm, which is applied to both synthetic and real-world networks. Experiment results show that our method enjoys competitive performance compared to the state of the art in the literature.","Community detection,modularity maximization,degree-corrected stochastic block model,convex relaxation,k-medians,social network","Chen, Yudong@Cornell University::Li, Xiaodong@University of California Davis@University of California System::Xu, Jiaming@Purdue University@Purdue University System"
NEAR-OPTIMALITY OF LINEAR RECOVERY IN GAUSSIAN OBSERVATION SCHEME UNDER parallel to.parallel to(2)(2)-LOSS,ANNALS OF STATISTICS,23,"We consider the problem of recovering linear image Bx of a signal x known to belong to a given convex compact set chi from indirect observation omega = Ax + sigma xi of x corrupted by Gaussian noise xi. It is shown that under some assumptions on chi (satisfied, e.g., when chi is the intersection of K concentric ellipsoids/elliptic cylinders), an easy-to-compute linear estimate is near-optimal in terms of its worst case, over x is an element of chi, expected parallel to.parallel to(2)(2)-loss. The main novelty here is that the result imposes no restrictions on A and B. To the best of our knowledge, preceding results on optimality of linear estimates dealt either with one-dimensional Bx (estimation of linear forms) or with the ""diagonal case"" where A, B are diagonal and chi is given by a ""separable"" constraint like chi = {x : Sigma(i)a(i)(2)x(i)(2) <= 1} or chi = {x : max(i) vertical bar a(i)x(i)vertical bar <= 1}.","Linear regression,linear estimation,minimax estimation","Juditsky, Anatoli@Universite Grenoble Alpes (UGA)@Communaute Universite Grenoble Alpes::Nemirovski, Arkadi@University System of Georgia@Georgia Institute of Technology"
AN MCMC APPROACH TO EMPIRICAL BAYES INFERENCE AND BAYESIAN SENSITIVITY ANALYSIS VIA EMPIRICAL PROCESSES,ANNALS OF STATISTICS,40,"Consider a Bayesian situation in which we observe Y similar to p(theta), where theta is an element of Theta and we have a family {vh, h is an element of H} of potential prior distributions on Theta. Let g be a real-valued function of theta, and let I-g(h) be the posterior expectation of g(theta) when the prior is v(h) . We are interested in two problems: (i) selecting a particular value of h, and (ii) estimating the family of posterior expectations {I-g(h), h is an element of H}. Let m(y)(h) be the marginal likelihood of the hyperparameter h: m(y)(h) = integral p(theta)(y)v(h)(d theta). The empirical Bayes estimate of h is, by definition, the value of h that maximizes m(y)(h). It turns out that it is typically possible to use Markov chain Monte Carlo to form point estimates for m(y)(h) and I-g(h) for each individual h in a continuum, and also confidence intervals for m(y)(h) and I-g(h) that are valid pointwise. However, we are interested in forming estimates, with confidence statements, of the entire families of integrals {my(h), h is an element of H} and {I-g(h), h is an element of H}: we need estimates of the first family in order to carry out empirical Bayes inference, and we need estimates of the second family in order to do Bayesian sensitivity analysis. We establish strong consistency and functional central limit theorems for estimates of these families by using tools from empirical process theory. We give two applications, one to latent Dirichlet allocation, which is used in topic modeling, and the other is to a model for Bayesian variable selection in linear regression.","Donsker class,geometric ergodicity,hyperparameter selection,regenerative simulation,latent Dirichlet allocation model","Doss, Hani@University of Florida@State University System of Florida::Park, Yeonhee@UTMD Anderson Cancer Center@University of Texas System"
CURVATURE AND INFERENCE FOR MAXIMUM LIKELIHOOD ESTIMATES,ANNALS OF STATISTICS,21,"Maximum likelihood estimates are sufficient statistics in exponential families, but not in general. The theory of statistical curvature was introduced to measure the effects of MLE insufficiency in one-parameter families. Here, we analyze curvature in the more realistic venue of multiparameter families-more exactly, curved exponential families, a broad class of smoothly defined nonexponential family models. We show that within the set of observations giving the same value for the MLE, there is a ""region of stability"" outside of which the MLE is no longer even a local maximum. Accuracy of the MLE is affected by the location of the observation vector within the region of stability. Our motivating example involves ""g-modeling,"" an empirical Bayes estimation procedure.","Observed information,g-modeling,region of stability,curved exponential families,regularized MLE","Efron, Bradley@Stanford University"
EMPIRICAL BAYES ESTIMATES FOR A TWO-WAY CROSS-CLASSIFIED MODEL,ANNALS OF STATISTICS,35,"We develop an empirical Bayes procedure for estimating the cell means in an unbalanced, two-way additive model with fixed effects. We employ a hierarchical model, which reflects exchangeability of the effects within treatment and within block but not necessarily between them, as suggested before by Lindley and Smith [J. R. Stat. Soc., B 34 (1972) 1-41]. The hyperparameters of this hierarchical model, instead of considered fixed, are to be substituted with data-dependent values in such a way that the point risk of the empirical Bayes estimator is small. Our method chooses the hyperparameters by minimizing an unbiased risk estimate and is shown to be asymptotically optimal for the estimation problem defined above, under suitable conditions. The usual empirical Best Linear Unbiased Predictor (BLUP) is shown to be substantially different from the proposed method in the unbalanced case and, therefore, performs suboptimally. Our estimator is implemented through a computationally tractable algorithm that is scalable to work under large designs. The case of missing cell observations is treated as well.","Shrinkage estimation,empirical Bayes,two-way ANOVA,oracle optimality,Stein's unbiased risk estimate (SURE),empirical BLUP","Brown, Lawrence D.@University of Pennsylvania::Mukherjee, Gourab@University of Southern California::Weinstein, Asaf@Stanford University"
ESTIMATING VARIANCE OF RANDOM EFFECTS TO SOLVE MULTIPLE PROBLEMS SIMULTANEOUSLY,ANNALS OF STATISTICS,35,"The two-level normal hierarchical model (NHM) has played a critical role in statistical theory for the last several decades. In this paper, we propose random effects variance estimator that simultaneously (i) improves on the estimation of the related shrinkage factors, (ii) protects empirical best linear unbiased predictors (EBLUP) [same as empirical Bayes (EB)] of the random effects from the common overshrinkage problem, (iii) avoids complex bias correction in generating strictly positive second-order unbiased mean square error (MSE) (same as integrated Bayes risk) estimator either by the Taylor series or single parametric bootstrap method. The idea of achieving multiple desirable properties in an EBLUP or EB method through a suitably devised random effects variance estimator is the first of its kind and holds promise in providing good inferences for random effects under the EBLUP or EB framework. The proposed methodology is also evaluated using a Monte Carlo simulation study and real data analysis.","Adjusted maximum likelihood method,empirical Bayes,empirical best linear unbiased prediction,linear mixed model,second-order unbiasedness","Hirose, Masayo Yoshimori@Institute of Statistical Mathematics (ISM) - Japan@Research Organization of Information & Systems (ROIS)::Lahiri, Partha@University System of Maryland@University of Maryland College Park"
A BAYESIAN APPROACH TO THE SELECTION OF TWO-LEVEL MULTI-STRATUM FACTORIAL DESIGNS,ANNALS OF STATISTICS,20,"In a multi-stratum factorial experiment, there are multiple error terms (strata) with different variances that arise from complicated structures of the experimental units. For unstructured experimental units, minimum aberration is a popular criterion for choosing regular fractional factorial designs. One difficulty in extending this criterion to multi-stratum factorial designs is that the formulation of a word length pattern based on which minimum aberration is defined requires an order of desirability among the relevant words, but a natural order is often lacking. Furthermore, a criterion based only on word length patterns does not account for the different stratum variances. Mitchell, Morris and Ylvisaker [Statist. Sinica 5 (1995) 559-573] proposed a framework for Bayesian factorial designs. A Gaussian process is used as the prior for the treatment effects, from which a prior distribution of the factorial effects is induced. This approach is applied to study optimal and efficient multi-stratum factorial designs. Good surrogates for the Bayesian criteria that can be related to word length and generalized word length patterns for regular and nonregular designs, respectively, are derived. A tool is developed for eliminating inferior designs and reducing the designs that need to be considered without requiring any knowledge of stratum variances. Numerical examples are used to illustrate the theory in several settings.","A-criterion,block structure,D-criterion,Gaussian process,(M.S)-Criterion,minimum aberration,word length pattern","Chang, Ming-Chung@Academia Sinica - Taiwan::Cheng, Ching-Shui@University of California System@University of California Berkeley"
ACCURACY ASSESSMENT FOR HIGH-DIMENSIONAL LINEAR REGRESSION,ANNALS OF STATISTICS,31,"This paper considers point and interval estimation of the l(q) loss of an estimator in high-dimensional linear regression with random design. We establish the minimax rate for estimating the l(q) loss and the minimax expected length of confidence intervals for the l(q) loss of rate-optimal estimators of the regression vector, including commonly used estimators such as Lasso, scaled Lasso, square-root Lasso and Dantzig Selector. Adaptivity of confidence intervals for the l(q) loss is also studied. Both the setting of the known identity design covariance matrix and known noise level and the setting of unknown design covariance matrix and unknown noise level are studied. The results reveal interesting and significant differences between estimating the l(2) loss and l(q) loss with 1 <= q < 2 as well as between the two settings.

New technical tools are developed to establish rate sharp lower bounds for the minimax estimation error and the expected length of minimax and adaptive confidence intervals for the l(q) loss. A significant difference between loss estimation and the traditional parameter estimation is that for loss estimation the constraint is on the performance of the estimator of the regression vector, but the lower bounds are on the difficulty of estimating its l(q) loss. The technical tools developed in this paper can also be of independent interest.","Accuracy assessment,adaptivity,confidence interval,high-dimensional linear regression,loss estimation,minimax lower bound,minimaxity,sparsity","Cai, T. Tony@University of Pennsylvania::Guo, Zijian@Rutgers State University New Brunswick"
BALL DIVERGENCE: NONPARAMETRIC TWO SAMPLE TEST,ANNALS OF STATISTICS,25,"In this paper, we first introduce Ball Divergence, a novel measure of the difference between two probability measures in separable Banach spaces, and show that the Ball Divergence of two probability measures is zero if and only if these two probability measures are identical without any moment assumption. Using Ball Divergence, we present a metric rank test procedure to detect the equality of distribution measures underlying independent samples. It is therefore robust to outliers or heavy-tail data. We show that this multivariate two sample test statistic is consistent with the Ball Divergence, and it converges to a mixture of x(2) distributions under the null hypothesis and a normal distribution under the alternative hypothesis. Importantly, we prove its consistency against a general alternative hypothesis. Moreover, this result does not depend on the ratio of the two imbalanced sample sizes, ensuring that can be applied to imbalanced data. Numerical studies confirm that our test is superior to several existing tests in terms of Type I error and power. We conclude our paper with two applications of our method: one is for virtual screening in drug development process and the other is for genome wide expression analysis in hormone replacement therapy.","Ball Divergence,Banach space,metric rank,permutation procedure","Pan, Wenliang@Sun Yat Sen University::Tian, Yuan@Sun Yat Sen University::Wang, Xueqin@Sun Yat Sen University::Zhang, Heping@Yale University@Sun Yat Sen University"
DISTRIBUTED TESTING AND ESTIMATION UNDER SPARSE HIGH DIMENSIONAL MODELS,ANNALS OF STATISTICS,31,"This paper studies hypothesis testing and parameter estimation in the context of the divide-and-conquer algorithm. In a unified likelihood-based framework, we propose new test statistics and point estimators obtained by aggregating various statistics from k subsamples of size n/k, where n is the sample size. In both low dimensional and sparse high dimensional settings, we address the important question of how large k can be, as n grows large, such that the loss of efficiency due to the divide-and-conquer algorithm is negligible. In other words, the resulting estimators have the same inferential efficiencies and estimation rates as an oracle with access to the full sample. Thorough numerical results are provided to back up the theory.","Divide and conquer,debiasing,massive data,thresholding","Battey, Heather@Imperial College London@Princeton University::Fan, Jianqing@Fudan University@Princeton University::Liu, Han@Princeton University::Lu, Junwei@Princeton University::Zhu, Ziwei@Princeton University"
HIGH-DIMENSIONAL A-LEARNING FOR OPTIMAL DYNAMIC TREATMENT REGIMES,ANNALS OF STATISTICS,30,"Precision medicine is a medical paradigm that focuses on finding the most effective treatment decision based on individual patient information. For many complex diseases, such as cancer, treatment decisions need to be tailored over time according to patients' responses to previous treatments. Such an adaptive strategy is referred as a dynamic treatment regime. A major challenge in deriving an optimal dynamic treatment regime arises when an extraordinary large number of prognostic factors, such as patient's genetic information, demographic characteristics, medical history and clinical measurements over time are available, but not all of them are necessary for making treatment decision. This makes variable selection an emerging need in precision medicine.

In this paper, we propose a penalized multi-stage A-learning for deriving the optimal dynamic treatment regime when the number of covariates is of the nonpolynomial (NP) order of the sample size. To preserve the double robustness property of the A-learning method, we adopt the Dantzig selector, which directly penalizes the A-leaning estimating equations. Oracle inequalities of the proposed estimators for the parameters in the optimal dynamic treatment regime and error bounds on the difference between the value functions of the estimated optimal dynamic treatment regime and the true optimal dynamic treatment regime are established. Empirical performance of the proposed approach is evaluated by simulations and illustrated with an application to data from the STAR* D study.","A-learning,Dantzig selector,NP-dimensionality,model misspecification,optimal dynamic treatment regime,oracle inequality","Shi, Chengchun@North Carolina State University@University of North Carolina::Fan, Ailin@North Carolina State University@University of North Carolina::Song, Rui@North Carolina State University@University of North Carolina::Lu, Wenbin@North Carolina State University@University of North Carolina"
TEST FOR HIGH-DIMENSIONAL REGRESSION COEFFICIENTS USING REFITTED CROSS-VALIDATION VARIANCE ESTIMATION,ANNALS OF STATISTICS,25,"Testing a hypothesis for high-dimensional regression coefficients is of fundamental importance in the statistical theory and applications. In this paper, we develop a new test for the overall significance of coefficients in high-dimensional linear regression models based on an estimated U-statistics of order two. With the aid of the martingale central limit theorem, we prove that the asymptotic distributions of the proposed test are normal under two different distribution assumptions. Refitted cross-validation (RCV) variance estimation is utilized to avoid the overestimation of the variance and enhance the empirical power. We examine the finite-sample performances of the proposed test via Monte Carlo simulations, which show that the new test based on the RCV estimator achieves higher powers, especially for the sparse cases. We also demonstrate an application by an empirical analysis of a microarray data set on Yorkshire gilts.","High-dimensional regression,hypothesis testing,martingale central limit theorem,refitted cross-validation variance estimation,U-statistics","Cui, Hengjian@Capital Normal University::Guo, Wenwen@Capital Normal University::Zhong, Wei@Xiamen University"
ARE DISCOVERIES SPURIOUS? DISTRIBUTIONS OF MAXIMUM SPURIOUS CORRELATIONS AND THEIR APPLICATIONS,ANNALS OF STATISTICS,34,"Over the last two decades, many exciting variable selection methods have been developed for finding a small group of covariates that are associated with the response from a large pool. Can the discoveries from these data mining approaches be spurious due to high dimensionality and limited sample size? Can our fundamental assumptions about the exogeneity of the covariates needed for such variable selection be validated with the data? To answer these questions, we need to derive the distributions of the maximum spurious correlations given a certain number of predictors, namely, the distribution of the correlation of a response variable Y with the best s linear combinations of p covariates X, even when X and Y are independent. When the covariance matrix of X possesses the restricted eigenvalue property, we derive such distributions for both a finite s and a diverging s, using Gaussian approximation and empirical process techniques. However, such a distribution depends on the unknown covariance matrix of X. Hence, we use the multiplier bootstrap procedure to approximate the unknown distributions and establish the consistency of such a simple bootstrap approach. The results are further extended to the situation where the residuals are from regularized fits. Our approach is then used to construct the upper confidence limit for the maximum spurious correlation and to test the exogeneity of the covariates. The former provides a baseline for guarding against false discoveries and the latter tests whether our fundamental assumptions for high-dimensional model selection are statistically valid. Our techniques and results are illustrated with both numerical examples and real data analysis.","High dimension,spurious correlation,bootstrap,false discovery","Fan, Jianqing@Fudan University@Princeton University::Shao, Qi-Man@Chinese University of Hong Kong@Princeton University::Zhou, Wen-Xin@University of California San Diego@University of California System"
ADAPTIVE ESTIMATION OF PLANAR CONVEX SETS,ANNALS OF STATISTICS,35,"In this paper, we consider adaptive estimation of an unknown planar compact, convex set from noisy measurements of its support function. Both the problem of estimating the support function at a point and that of estimating the whole convex set are studied. For pointwise estimation, we consider the problem in a general nonasymptotic framework, which evaluates the performance of a procedure at each individual set, instead of the worst case performance over a large parameter space as in conventional minimax theory. A data-driven adaptive estimator is proposed and is shown to be optimally adaptive to every compact, convex set. For estimating the whole convex set, we propose estimators that are shown to adaptively achieve the optimal rate of convergence. In both of these problems, our analysis makes no smoothness assumptions on the boundary of the unknown convex set.","Adaptive estimation,circle convexity,convex set,Hausdorff distance,minimax rate of convergence,support function","Cai, T. Tony@University of Pennsylvania::Guntuboyina, Adityanand@University of California System@University of California Berkeley::Wei, Yuting@University of California System@University of California Berkeley"
CONSISTENCY OF AIC AND BIC IN ESTIMATING THE NUMBER OF SIGNIFICANT COMPONENTS IN HIGH-DIMENSIONAL PRINCIPAL COMPONENT ANALYSIS,ANNALS OF STATISTICS,30,"In this paper, we study the problem of estimating the number of significant components in principal component analysis (PCA), which corresponds to the number of dominant eigenvalues of the covariance matrix of p variables. Our purpose is to examine the consistency of the estimation criteria AIC and BIC based on the model selection criteria by Akaike [In 2nd International Symposium on Information Theory (1973) 267-281, Akademia Kiado] and Schwarz [Estimating the dimension of a model 6 (1978) 461464] under a high-dimensional asymptotic framework. Using random matrix theory techniques, we derive sufficient conditions for the criterion to be strongly consistent for the case when the dominant population eigenvalues are bounded, and when the dominant eigenvalues tend to infinity. Moreover, the asymptotic results are obtained without normality assumption on the population distribution. Simulation studies are also conducted, and results show that the sufficient conditions in our theorems are essential.","AIC,BIC,consistency,dimensionality,high-dimensional framework,number of significant components,principal component analysis,random matrix theory,signal processing,spiked model","Bai, Zhidong@Northeast Normal University - China::Choi, Kwok Pui@National University of Singapore::Fujikoshi, Yasunori@Hiroshima University"
ON THE SYSTEMATIC AND IDIOSYNCRATIC VOLATILITY WITH LARGE PANEL HIGH-FREQUENCY DATA,ANNALS OF STATISTICS,37,"In this paper, we separate the integrated (spot) volatility of an individual Ito process into integrated (spot) systematic and idiosyncratic volatilities, and estimate them by aggregation of local factor analysis (localization) with large-dimensional high-frequency data. We show that, when both the sampling frequency n and the dimensionality p go to infinity and p >= C root n for some constant C, our estimators of High dimensional Ito process; common driving process; specific driving process, integrated High dimensional Ito process, common driving process, specific driving process, systematic and idiosyncratic volatilities are root n (n(1/4) for spot estimates) consistent, the best rate achieved in estimating the integrated (spot) volatility which is readily identified even with univariate high-frequency data. However, when Cn(1/4) <= p < C root n, aggregation of n(1/4)-consistent local estimates of systematic and idiosyncratic volatilities results in p-consistent (not root n-consistent) estimates of integrated systematic and idiosyncratic volatilities. Even more interesting, when p < Cn(1/4), the integrated estimate has the same convergence rate as the spot estimate, both being p-consistent. This reveals a distinctive feature from aggregating local estimates in the low-dimensional highfrequency data setting. We also present estimators of the integrated (spot) idiosyncratic volatility matrices as well as their inverse matrices under some sparsity assumption. We finally present a factor-based estimator of the inverse of the spot volatility matrix. Numerical studies including the Monte Carlo experiments and real data analysis justify the performance of our estimators.","High dimensional Ito process,common driving process,specific driving process","Kong, Xin-Bing@Nanjing Audit University"
A SMOOTH BLOCK BOOTSTRAP FOR QUANTILE REGRESSION WITH TIME SERIES,ANNALS OF STATISTICS,38,"Quantile regression allows for broad (conditional) characterizations of a response distribution beyond conditional means and is of increasing interest in economic and financial applications. Because quantile regression estimators have complex limiting distributions, several bootstrap methods for the independent data setting have been proposed, many of which involve smoothing steps to improve bootstrap approximations. Currently, no similar advances in smoothed bootstraps exist for quantile regression with dependent data. To this end, we establish a smooth tapered block bootstrap procedure for approximating the distribution of quantile regression estimators for time series. This bootstrap involves two rounds of smoothing in resampling: individual observations are resampled via kernel smoothing techniques and resampled data blocks are smoothed by tapering. The smooth bootstrap results in performance improvements over previous unsmoothed versions of the block bootstrap as well as normal approximations based on Powell's kernel variance estimator, which are common in application. Our theoretical results correct errors in proofs for earlier and simpler versions of the (unsmoothed) moving blocks bootstrap for quantile regression and broaden the validity of block bootstraps for this problem under weak conditions. We illustrate the smooth bootstrap through numerical studies and examples.","Kernel smoothing,jackknife after bootstrap,moving blocks,tapering,value at risk","Gregory, Karl B.@University of South Carolina System@University of South Carolina Columbia::Lahiri, Soumendra N.@North Carolina State University@University of North Carolina::Nordman, Daniel J.@Iowa State University"
ASYMPTOTIC DISTRIBUTION-FREE TESTS FOR SEMIPARAMETRIC REGRESSIONS WITH DEPENDENT DATA,ANNALS OF STATISTICS,38,"This article proposes a new general methodology for constructing nonparametric and semiparametric Asymptotically Distribution-Free (ADF) tests for semiparametric hypotheses in regression models for possibly dependent data coming from a strictly stationary process. Classical tests based on the difference between the estimated distributions of the restricted and unrestricted regression errors are not ADF. In this article, we introduce a novel transformation of this difference that leads to ADF tests with well-known critical values. The general methodology is illustrated with applications to testing for parametric models against nonparametric or semiparametric alternatives, and semiparametric constrained mean-variance models. Several Monte Carlo studies and an empirical application show that the finite sample performance of the proposed tests is satisfactory in moderate sample sizes.","Beta-mixing,error distribution,goodness-of-fit tests,local polynomial estimation,nonparametric regression","Escanciano, Juan Carlos@Indiana University Bloomington@Indiana University System::Carlos Pardo-Fernandez, Juan@University of Vigo::Van Keilegom, Ingrid@KU Leuven"
GRADIENT-BASED STRUCTURAL CHANGE DETECTION FOR NONSTATIONARY TIME SERIES M-ESTIMATION,ANNALS OF STATISTICS,33,"We consider structural change testing for a wide class of time series M-estimation with nonstationary predictors and errors. Flexible predictor-error relationships, including exogenous, state-heteroscedastic and autoregressive regressions and their mixtures, are allowed. New uniform Bahadur representations are established with nearly optimal approximation rates. A CUSUMtype test statistic based on the gradient vectors of the regression is considered. In this paper, a simple bootstrap method is proposed and is proved to be consistent for M-estimation structural change detection under both abrupt and smooth nonstationarity and temporal dependence. Our bootstrap procedure is shown to have certain asymptotically optimal properties in terms of accuracy and power. A public health time series dataset is used to illustrate our methodology, and asymmetry of structural changes in high and low quantiles is found.","M-estimation,piecewise local stationarity,bootstrap,structural change","Wu, Weichi@University of London@University College London::Zhou, Zhou@University of Toronto"
MODERATE DEVIATIONS AND NONPARAMETRIC INFERENCE FOR MONOTONE FUNCTIONS,ANNALS OF STATISTICS,38,"This paper considers self-normalized limits and moderate deviations of nonparametric maximum likelihood estimators for monotone functions. We obtain their self-normalized Cramer-type moderate deviations and limit distribution theorems for the nonparametric maximum likelihood estimator in the current status model and the Grenander-type estimator. As applications of the results, we present a new procedure to construct asymptotical confidence intervals and asymptotical rejection regions of hypothesis testing for monotone functions. The theoretical results can guarantee that the new test has the probability of type II error tending to 0 exponentially. Simulation studies also show that the new nonparametric test works well for the most commonly used parametric survival functions such as exponential and Weibull survival distributions.","Grenander estimator,interval censored data,large deviations,moderate deviations,nonparametric MLE,self-normalized limit,strong approximation,Talagrand inequality","Gao, Fuqing@Wuhan University::Xiong, Jie@University of Macau::Zhao, Xingqiu@Hong Kong Polytechnic University"
UNIFORM ASYMPTOTIC INFERENCE AND THE BOOTSTRAP AFTER MODEL SELECTION,ANNALS OF STATISTICS,22,"Recently, Tibshirani et al. [J. Amer. Statist. Assoc. 111 (2016) 600-620] proposed a method for making inferences about parameters defined by model selection, in a typical regression setting with normally distributed errors. Here, we study the large sample properties of this method, without assuming normality. We prove that the test statistic of Tibshirani et al. (2016) is asymptotically valid, as the number of samples n grows and the dimension d of the regression problem stays fixed. Our asymptotic result holds uniformly over a wide class of nonnormal error distributions. We also propose an efficient bootstrap version of this test that is provably (asymptotically) conservative, and in practice, often delivers shorter intervals than those from the original normality-based approach. Finally, we prove that the test statistic of Tibshirani et al. (2016) does not enjoy uniform validity in a high-dimensional setting, when the dimension d is allowed grow.","Post-selection inference,selective inference,asymptotics,bootstrap,forward,stepwise regression,lasso","Tibshirani, Ryan J.@Carnegie Mellon University::Rinaldo, Alessandro@Carnegie Mellon University::Tibshirani, Rob@Stanford University::Wasserman, Larry@Carnegie Mellon University"
DETECTION THRESHOLDS FOR THE beta-MODEL ON SPARSE GRAPHS,ANNALS OF STATISTICS,40,"In this paper, we study sharp thresholds for detecting sparse signals in beta-models for potentially sparse random graphs. The results demonstrate interesting interplay between graph sparsity, signal sparsity and signal strength. In regimes of moderately dense signals, irrespective of graph sparsity, the detection thresholds mirror corresponding results in independent Gaussian sequence problems. For sparser signals, extreme graph sparsity implies that all tests are asymptotically powerless, irrespective of the signal strength. On the other hand, sharp detection thresholds are obtained, up to matching constants, on denser graphs. The phase transitions mentioned above are sharp. As a crucial ingredient, we study a version of the higher criticism test which is provably sharp up to optimal constants in the regime of sparse signals. The theoretical results are further verified by numerical simulations.","Detection boundary,sparse random graphs,beta model,higher criticism,sparse signals","Mukherjee, Rajarshi@University of California System@University of California Berkeley::Mukherjee, Sumit@Columbia University::Sen, Subhabrata@Microsoft"
ADAPTIVE SUP-NORM ESTIMATION OF THE WIGNER FUNCTION IN NOISY QUANTUM HOMODYNE TOMOGRAPHY,ANNALS OF STATISTICS,57,"In quantum optics, the quantum state of a light beam is represented through the Wigner function, a density on R-2, which may take negative values but must respect intrinsic positivity constraints imposed by quantum physics. In the framework of noisy quantum homodyne tomography with efficiency parameter 1/2 < eta <= 1, we study the theoretical performance of a kernel estimator of the Wigner function. We prove that it is minimax efficient, up to a logarithmic factor in the sample size, for the L-infinity-risk over a class of infinitely differentiable functions. We also compute the lower bound for the L-2-risk. We construct an adaptive estimator, that is, which does not depend on the smoothness parameters, and prove that it attains the minimax rates for the corresponding smoothness of the class of functions up to a logarithmic factor in the sample size. Finite sample behaviour of our adaptive procedure is explored through numerical experiments.","Nonparametric minimax estimation,adaptive estimation,inverse problem,L-2 and L-infinity risks,quantum homodyne tomography,Wigner function,Radon transform,quantum state","Lounici, Karim@Universite Cote d'Azur (ComUE)@Georgia Institute of Technology@University System of Georgia@CNRS - National Institute for Mathematical Sciences (INSMI)@Centre National de la Recherche Scientifique (CNRS)@University of Nice Sophia Antipolis::Meziani, Katia@PSL Research University Paris (ComUE)@Universite Paris-Dauphine@ENSAE ParisTech@CNRS - National Institute for Mathematical Sciences (INSMI)@Centre National de la Recherche Scientifique (CNRS)@Universite Paris Saclay (ComUE)::Peyre, Gabriel@Ecole Normale Superieure (ENS)@PSL Research University Paris (ComUE)@Centre National de la Recherche Scientifique (CNRS)"
I-LAMM FOR SPARSE LEARNING: SIMULTANEOUS CONTROL OF ALGORITHMIC COMPLEXITY AND STATISTICAL ERROR,ANNALS OF STATISTICS,35,"We propose a computational framework named iterative local adaptive majorize-minimization (I-LAMM) to simultaneously control algorithmic complexity and statistical error when fitting high-dimensional models. I-LAMM is a two-stage algorithmic implementation of the local linear approximation to a family of folded concave penalized quasi-likelihood. The first stage solves a convex program with a crude precision tolerance to obtain a coarse initial estimator, which is further refined in the second stage by iteratively solving a sequence of convex programs with smaller precision tolerances. Theoretically, we establish a phase transition: the first stage has a sublinear iteration complexity, while the second stage achieves an improved linear rate of convergence. Though this framework is completely algorithmic, it provides solutions with optimal statistical performances and controlled algorithmic complexity for a large family of nonconvex optimization problems. The iteration effects on statistical errors are clearly demonstrated via a contraction property. Our theory relies on a localized version of the sparse/restricted eigenvalue condition, which allows us to analyze a large family of loss and penalty functions and provide optimality guarantees under very weak assumptions (e.g., I-LAMM requires much weaker minimal signal strength than other procedures). Thorough numerical results are provided to support the obtained theory.","Algorithmic statistics,iteration complexity,local adaptive MM,nonconvex statistical optimization,optimal rate of convergence","Fan, Jianqing@Fudan University@Princeton University::Liu, Han@Princeton University::Sun, Qiang@University of Toronto@Princeton University::Zhang, Tong@Fudan University"
STRONG ORTHOGONAL ARRAYS OF STRENGTH TWO PLUS,ANNALS OF STATISTICS,17,"Strong orthogonal arrays were recently introduced and studied in He and Tang [Biometrika 100 (2013) 254-260] as a class of space-filling designs for computer experiments. To enjoy the benefits of better space-filling properties, when compared to ordinary orthogonal arrays, strong orthogonal arrays need to have strength three or higher, which may require run sizes that are too large for experimenters to afford. To address this problem, we introduce a new class of arrays, called strong orthogonal arrays of strength two plus. These arrays, while being more economical than strong orthogonal arrays of strength three, still enjoy the better two-dimensional space-filling property of the latter. Among the many results we have obtained on the characterizations and constructions of strong orthogonal arrays of strength two plus, worth special mention is their intimate connection with second-order saturated designs.","Complementary design,computer experiment,Latin hypercube,second-order saturated design,space-filling design","He, Yuanzhen@Beijing Normal University::Cheng, Ching-Shui@Academia Sinica - Taiwan::Tang, Boxin@Simon Fraser University"
STATISTICAL INFERENCE FOR SPATIAL STATISTICS DEFINED IN THE FOURIER DOMAIN,ANNALS OF STATISTICS,41,"A class of Fourier based statistics for irregular spaced spatial data is introduced. Examples include the Whittle likelihood, a parametric estimator of the covariance function based on the L-2-contrast function and a simple nonparametric estimator of the spatial autocovariance which is a nonnegative function. The Fourier based statistic is a quadratic form of a discrete Fourier-type transform of the spatial data. Evaluation of the statistic is computationally tractable, requiring O(nb) operations, where b are the number of Fourier frequencies used in the definition of the statistic and n is the sample size. The asymptotic sampling properties of the statistic are derived using both increasing domain and fixed-domain spatial asymptotics. These results are used to construct a statistic which is asymptotically pivotal.","Fixed and increasing domain asymptotics,irregular spaced locations,quadratic forms,spatial spectral density function,stationary spatial random fields","Rao, Suhasini Subba@Texas A&M University College Station@Texas A&M University System"
ON THE INFERENCE ABOUT THE SPECTRAL DISTRIBUTION OF HIGH-DIMENSIONAL COVARIANCE MATRIX BASED ON HIGH-FREQUENCY NOISY OBSERVATIONS,ANNALS OF STATISTICS,37,"In practice, observations are often contaminated by noise, making the resulting sample covariance matrix a signal-plus-noise sample covariance matrix. Aiming to make inferences about the spectral distribution of the population covariance matrix under such a situation, we establish an asymptotic relationship that describes how the limiting spectral distribution of (signal) sample covariance matrices depends on that of signal-plus-noisetype sample covariance matrices. As an application, we consider inferences about the spectral distribution of integrated covolatility (ICV) matrices of high-dimensional diffusion processes based on high-frequency data with microstructure noise. The (slightly modified) pre-averaging estimator is a signal-plus-noise sample covariance matrix, and the aforementioned result, together with a (generalized) connection between the spectral distribution of signal sample covariance matrices and that of the population covariance matrix, enables us to propose a two-step procedure to consistently estimate the spectral distribution of ICV for a class of diffusion processes. An alternative approach is further proposed, which possesses several desirable properties: it is more robust, it eliminates the effects of microstructure noise, and the asymptotic relationship that enables consistent estimation of the spectral distribution of ICV is the standard Mar. cenko-Pastur equation. The performance of the two approaches is examined via simulation studies under both synchronous and asynchronous observation settings.","High-dimension,high-frequency,integrated covariance matrices,Marcenko-Pastur equation,microstructure noise","Xia, Ningning@Shanghai University of Finance & Economics::Zheng, Xinghua@Hong Kong University of Science & Technology"
ONLINE RULES FOR CONTROL OF FALSE DISCOVERY RATE AND FALSE DISCOVERY EXCEEDANCE,ANNALS OF STATISTICS,41,"Multiple hypothesis testing is a core problem in statistical inference and arises in almost every scientific field. Given a set of null hypotheses H(n) = (H-1,..., H-n), Benjamini and Hochberg [J.R. Stat. Soc. Ser. B. Stat. Methodol. 57 (1995) 289-300] introduced the false discovery rate (FDR), which is the expected proportion of false positives among rejected null hypotheses, and proposed a testing procedure that controls FDR below a preassigned significance level. Nowadays FDR is the criterion of choice for large-scale multiple hypothesis testing.

In this paper we consider the problem of controlling FDR in an online manner. Concretely, we consider an ordered-possibly infinite-sequence of null hypotheses H = (H-1, H-2, H-3,...) where, at each step i, the statistician must decide whether to reject hypothesis Hi having access only to the previous decisions. This model was introduced by Foster and Stine [J.R. Stat. Soc. Ser. B. Stat. Methodol. 70 (2008) 429-444].

We study a class of generalized alpha investing procedures, first introduced by Aharoni and Rosset [J.R. Stat. Soc. Ser. B. Stat. Methodol. 76 (2014) 771-794]. We prove that any rule in this class controls online FDR, provided p-values corresponding to true nulls are independent from the other p-values. Earlier work only established mFDR control. Next, we obtain conditions under which generalized alpha investing controls FDR in the presence of general p-values dependencies. We also develop a modified set of procedures that allow to control the false discovery exceedance (the tail of the proportion of false discoveries). Finally, we evaluate the performance of online procedures on both synthetic and real data, comparing them with offline approaches, such as adaptive Benjamini-Hochberg.","Hypothesis testing,false discovery rate (FDR),false discovery exceedance (FDX),online decision making","Javanmard, Adel@University of Southern California::Montanari, Andrea@Stanford University"
FREQUENCY DOMAIN MINIMUM DISTANCE INFERENCE FOR POSSIBLY NONINVERTIBLE AND NONCAUSAL ARMA MODELS,ANNALS OF STATISTICS,26,"This article introduces frequency domain minimum distance procedures for performing inference in general, possibly non causal and/or noninvertible, autoregressive moving average (ARMA) models. We use information from higher order moments to achieve identification on the location of the roots of the AR and MA polynomials for non-Gaussian time series. We propose a minimum distance estimator that optimally combines the information contained in second, third, and fourth moments. Contrary to existing estimators, the proposed one is consistent under general assumptions, and may improve on the efficiency of estimators based on only second order moments. Our procedures are also applicable for processes for which either the third or the fourth order spectral density is the zero function.","Higher-order moments,higher-order spectra,nonminimum phase,Whittle estimate","Velasco, Carlos@Universidad Carlos III de Madrid::Lobato, Ignacio N.@Instituto Tecnologico Autonomo de Mexico"
ON CONSISTENCY AND SPARSITY FOR SLICED INVERSE REGRESSION IN HIGH DIMENSIONS,ANNALS OF STATISTICS,28,"We provide here a framework to analyze the phase transition phenomenon of slice inverse regression (SIR), a supervised dimension reduction technique introduced by Li [J. Amer. Statist. Assoc. 86 (1991) 316-342]. Under mild conditions, the asymptotic ratio rho = lim p/n is the phase transition parameter and the SIR estimator is consistent if and only if rho = 0. When dimension p is greater than n, we propose a diagonal thresholding screening SIR (DT-SIR) algorithm. This method provides us with an estimate of the eigenspace of var(E [x vertical bar y]), the covariance matrix of the conditional expectation. The desired dimension reduction space is then obtained by multiplying the inverse of the covariance matrix on the eigenspace. Under certain sparsity assumptions on both the covariance matrix of predictors and the loadings of the directions, we prove the consistency of DT-SIR in estimating the dimension reduction space in high-dimensional data analysis. Extensive numerical experiments demonstrate superior performances of the proposed method in comparison to its competitors.","Dimension reduction,random matrix theory,sliced inverse regression","Lin, Qian@Tsinghua University@Harvard University::Zhao, Zhigen@Pennsylvania Commonwealth System of Higher Education (PCSHE)@Temple University::Liu, Jun S.@Tsinghua University@Harvard University"
REGULARIZATION AND THE SMALL-BALL METHOD I: SPARSE RECOVERY,ANNALS OF STATISTICS,40,"We obtain bounds on estimation error rates for regularization procedures of the form

(f) over cap is an element of argmin(f is an element of F)(1/N Sigma(N)(i=1) (Yi - f (X-i))(2) + lambda Psi(f))

when Psi is a norm and F is convex.

Our approach gives a common framework that may be used in the analysis of learning problems and regularization problems alike. In particular, it sheds some light on the role various notions of sparsity have in regularization and on their connection with the size of subdifferentials of Psi in a neighborhood of the true minimizer.

As ""proof of concept"" we extend the known estimates for the LASSO, SLOPE and trace norm regularization.","Empirical processes,high-dimensional statistics","Lecue, Guillaume@Universite Paris Saclay (ComUE)@Universite Paris Saclay@Centre National de la Recherche Scientifique (CNRS)@ENSAE ParisTech::Mendelson, Shahar@Technion Israel Institute of Technology@Australian National University"
GAUSSIAN AND BOOTSTRAP APPROXIMATIONS FOR HIGH-DIMENSIONAL U-STATISTICS AND THEIR APPLICATIONS,ANNALS OF STATISTICS,70,"This paper studies the Gaussian and bootstrap approximations for the probabilities of a nondegenerate U-statistic belonging to the hyperrectangles in R-d when the dimension d is large. A two-step Gaussian approximation procedure that does not impose structural assumptions on the data distribution is proposed. Subject to mild moment conditions on the kernel, we establish the explicit rate of convergence uniformly in the class of all hyperrectangles in Rd that decays polynomially in sample size for a high-dimensional scaling limit, where the dimension can be much larger than the sample size. We also provide computable approximation methods for the quantiles of the maxima of centered U-statistics. Specifically, we provide a unified perspective for the empirical bootstrap, the randomly reweighted bootstrap and the Gaussian multiplier bootstrap with the jackknife estimator of covariance matrix as randomly reweighted quadratic forms and we establish their validity. We show that all three methods are inferentially first-order equivalent for high-dimensional U-statistics in the sense that they achieve the same uniform rate of convergence over all d-dimensional hyperrectangles. In particular, they are asymptotically valid when the dimension d can be as large as O(e(nc)) for some constant c is an element of (0, 1/7).

The bootstrap methods are applied to statistical applications for high-dimensional non-Gaussian data including: (i) principled and data-dependent tuning parameter selection for regularized estimation of the covariance matrix and its related functionals; (ii) simultaneous inference for the covariance and rank correlation matrices. In particular, for the thresholded covariance matrix estimator with the bootstrap selected tuning parameter, we show that for a class of sub-Gaussian data, error bounds of the bootstrapped thresholded covariance matrix estimator can be much tighter than those of the minimax estimator with a universal threshold. In addition, we also show that the Gaussian-like convergence rates can be achieved for heavy-tailed data, which are less conservative than those obtained by the Bonferroni technique that ignores the dependency in the underlying data distribution.","U-statistics,high-dimensional inference,Gaussian approximation,bootstrap","Chen, Xiaohui@University of Illinois Urbana-Champaign@University of Illinois System"
SELECTIVE INFERENCE WITH A RANDOMIZED RESPONSE,ANNALS OF STATISTICS,34,"Inspired by sample splitting and the reusable holdout introduced in the field of differential privacy, we consider selective inference with a randomized response. We discuss two major advantages of using a randomized response for model selection. First, the selectively valid tests are more powerful after randomized selection. Second, it allows consistent estimation and weak convergence of selective inference procedures. Under independent sampling, we prove a selective (or privatized) central limit theorem that transfers procedures valid under asymptotic normality without selection to their corresponding selective counterparts. This allows selective inference in nonparametric settings. Finally, we propose a framework of inference after combining multiple randomized selection procedures. We focus on the classical asymptotic setting, leaving the interesting high-dimensional asymptotic questions for future work.","Selective inference,nonparametric,differential privacy","Tian, Xiaoying@Stanford University::Taylor, Jonathan@Stanford University"
MULTISCALE BLIND SOURCE SEPARATION,ANNALS OF STATISTICS,74,"We provide a new methodology for statistical recovery of single linear mixtures of piecewise constant signals (sources) with unknown mixing weights and change points in a multiscale fashion. We show exact recovery within an epsilon-neighborhood of the mixture when the sources take only values in a known finite alphabet. Based on this we provide the SLAM (Separates Linear Alphabet Mixtures) estimators for the mixing weights and sources. For Gaussian error, we obtain uniform confidence sets and optimal rates (up to log-factors) for all quantities. SLAM is efficiently computed as a nonconvex optimization problem by a dynamic program tailored to the finite alphabet assumption. Its performance is investigated in a simulation study. Finally, it is applied to assign copy-number aberrations from genetic sequencing data to different clones and to estimate their proportions.","Multiscale inference,honest confidence sets,change point regression,finite alphabet linear mixture,exact recovery,genetic sequencing","Behr, Merle@University of Gottingen::Holmes, Chris@University of Oxford::Munk, Axel@University of Gottingen@Max Planck Society"
SHARP ORACLE INEQUALITIES FOR LEAST SQUARES ESTIMATORS IN SHAPE RESTRICTED REGRESSION,ANNALS OF STATISTICS,25,"The performance of Least Squares (LS) estimators is studied in shape-constrained regression models under Gaussian and sub-Gaussian noise. General bounds on the performance of LS estimators over closed convex sets are provided. These results have the form of sharp oracle inequalities that account for the model misspecification error. In the presence of misspecification, these bounds imply that the LS estimator estimates the projection of the true parameter at the same rate as in the well-specified case.

In isotonic and unimodal regression, the LS estimator achieves the non-parametric rate n(-2/3) as well as a parametric rate of order k/n up to logarithmic factors, where k is the number of constant pieces of the true parameter. In univariate convex regression, the LS estimator satisfies an adaptive risk bound of order q/n up to logarithmic factors, where q is the number of affine pieces of the true regression function. This adaptive risk bound holds for any collection of design points. While Guntuboyina and Sen [Probab. Theory Related Fields 163 (2015) 379-411] established that the nonparametric rate of convex regression is of order n(-4/5) for equispaced design points, we show that the nonparametric rate of convex regression can be as slow as n(-2/3) for some worst-case design points. This phenomenon can be explained as follows: Although convexity brings more structure than unimodality, for some worstcase design points this extra structure is uninformative and the nonparametric rates of unimodal regression and convex regression are both n(-2/3). Higher order cones, such as the cone of beta-monotone sequences, are also studied.","Shape restricted regression,convexity,minimax rates,Gaussian width,concentration","Bellec, Pierre C.@ENSAE ParisTech@Rutgers State University New Brunswick@Centre National de la Recherche Scientifique (CNRS)@Universite Paris Saclay (ComUE)@CNRS - Institute for Humanities & Social Sciences (INSHS)"
ORACLE INEQUALITIES FOR SPARSE ADDITIVE QUANTILE REGRESSION IN REPRODUCING KERNEL HILBERT SPACE,ANNALS OF STATISTICS,54,"This paper considers the estimation of the sparse additive quantile regression (SAQR) in high-dimensional settings. Given the nonsmooth nature of the quantile loss function and the nonparametric complexities of the component function estimation, it is challenging to analyze the theoretical properties of ultrahigh-dimensional SAQR. We propose a regularized learning approach with a two-fold Lasso-type regularization in a reproducing kernel Hilbert space (RKHS) for SAQR. We establish nonasymptotic oracle inequalities for the excess risk of the proposed estimator without any coherent conditions. If additional assumptions including an extension of the restricted eigenvalue condition are satisfied, the proposed method enjoys sharp oracle rates without the light tail requirement. In particular, the proposed estimator achieves the minimax lower bounds established for sparse additive mean regression. As a by-product, we also establish the concentration inequality for estimating the population mean when the general Lipschitz loss is involved. The practical effectiveness of the new method is demonstrated by competitive numerical results.","Quantile regression,additive models,sparsity,regularization methods,reproducing kernel Hilbert space","Lv, Shaogao@Nanjing Audit University::Lin, Huazhen@Southwestern University of Finance & Economics - China::Lian, Heng@City University of Hong Kong::Huang, Jian@University of Iowa"
ON BAYESIAN INDEX POLICIES FOR SEQUENTIAL RESOURCE ALLOCATION,ANNALS OF STATISTICS,38,"This paper is about index policies for minimizing (frequentist) regret in a stochastic multi-armed bandit model, inspired by a Bayesian view on the problem. Our main contribution is to prove that the Bayes-UCB algorithm, which relies on quantiles of posterior distributions, is asymptotically optimal when the reward distributions belong to a one-dimensional exponential family, for a large class of prior distributions. We also show that the Bayesian literature gives new insight on what kind of exploration rates could be used in frequentist, UCB-type algorithms. Indeed, approximations of the Bayesian optimal solution or the Finite-Horizon Gittins indices provide a justification for the kl-UCB+ and kl-UCB-H+ algorithms, whose asymptotic optimality is also established.","Multi-armed bandit problems,Bayesian methods,upper-confidence bounds,Gittins indices","Kaufmann, Emilie@Centrale Lille@Inria@Centre National de la Recherche Scientifique (CNRS)@Fondation I-SITE ULNE@Universite de Lille@Universite Lille-Nord-de-France (ComUE)"
TESTING INDEPENDENCE WITH HIGH-DIMENSIONAL CORRELATED SAMPLES,ANNALS OF STATISTICS,45,"Testing independence among a number of (ultra) high-dimensional random samples is a fundamental and challenging problem. By arranging n identically distributed p-dimensional random vectors into a p x n data matrix, we investigate the problem of testing independence among columns under the matrix-variate normal modeling of data. We propose a computationally simple and tuning-free test statistic, characterize its limiting null distribution, analyze the statistical power and prove its minimax optimality. As an important by-product of the test statistic, a ratio-consistent estimator for the quadratic functional of a covariance matrix from correlated samples is developed. We further study the effect of correlation among samples to an important high-dimensional inference problem-large-scale multiple testing of Pearson's correlation coefficients. Indeed, blindly using classical inference results based on the assumed independence of samples will lead to many false discoveries, which suggests the need for conducting independence testing before applying existing methods. To address the challenge arising from correlation among samples, we propose a ""sandwich estimator"" of Pearson's correlation coefficient by de-correlating the samples. Based on this approach, the resulting multiple testing procedure asymptotically controls the overall false discovery rate at the nominal level while maintaining good statistical power. Both simulated and real data experiments are carried out to demonstrate the advantages of the proposed methods.","Independence test,multiple testing of correlations,false discovery rate,matrix-variate normal,quadratic functional estimation,high-dimensional sample correlation matrix","Chen, Xi@New York University::Liu, Weidong@Shanghai Jiao Tong University"
DETECTING RARE AND FAINT SIGNALS VIA THRESHOLDING MAXIMUM LIKELIHOOD ESTIMATORS,ANNALS OF STATISTICS,32,"Motivated by the analysis of RNA sequencing (RNA-seq) data for genes differentially expressed across multiple conditions, we consider detecting rare and faint signals in high-dimensional response variables. We address the signal detection problem under a general framework, which includes generalized linear models for count-valued responses as special cases. We propose a test statistic that carries out a multi-level thresholding on maximum likelihood estimators (MLEs) of the signals, based on a new Cramer-type moderate deviation result for multidimensional MLEs. Based on the multi-level thresholding test, a multiple testing procedure is proposed for signal identification. Numerical simulations and a case study on maize RNA-seq data are conducted to demonstrate the effectiveness of the proposed approaches on signal detection and identification.","Detection boundary,false discovery proportion,generalized linear model,moderate deviation,multiple testing procedure,RNA-seq data","Qiu, Yumou@University of Nebraska System@University of Nebraska Lincoln::Chen, Song Xi@Peking University::Nettleton, Dan@Iowa State University"
BAYESIAN ESTIMATION OF SPARSE SIGNALS WITH A CONTINUOUS SPIKE-AND-SLAB PRIOR,ANNALS OF STATISTICS,39,"We introduce a new framework for estimation of sparse normal means, bridging the gap between popular frequentist strategies (LASSO) and popular Bayesian strategies (spike-and-slab). The main thrust of this paper is to introduce the family of Spike-and-Slab LASSO (SS-LASSO) priors, which form a continuum between the Laplace prior and the point-mass spike-and-slab prior. We establish several appealing frequentist properties of SS-LASSO priors, contrasting them with these two limiting cases. First, we adopt the penalized likelihood perspective on Bayesian modal estimation and introduce the framework of Bayesian penalty mixing with spike-and-slab priors. We show that the SS-LASSO global posterior mode is (near) minimax rate-optimal under squared error loss, similarly as the LASSO. Going further, we introduce an adaptive two-step estimator which can achieve provably sharper performance than the LASSO. Second, we show that the whole posterior keeps pace with the global mode and concentrates at the (near) minimax rate, a property that is known not to hold for the single Laplace prior. The minimax-rate optimality is obtained with a suitable class of independent product priors (for known levels of sparsity) as well as with dependent mixing priors (adapting to the unknown levels of sparsity). Up to now, the rate-optimal posterior concentration has been established only for spike-and-slab priors with a point mass at zero. Thus, the SS-LASSO priors, despite being continuous, possess similar optimality properties as the ""theoretically ideal"" point-mass mixtures. These results provide valuable theoretical justification for our proposed class of priors, underpinning their intuitive appeal and practical potential.","Asymptotic minimaxity,LASSO,posterior concentration,spike-and-slab","Rockova, Veronika@University of Chicago"
HIGH DIMENSIONAL CENSORED QUANTILE REGRESSION,ANNALS OF STATISTICS,38,"Censored quantile regression (CQR) has emerged as a useful regression tool for survival analysis. Some commonly used CQR methods can be characterized by stochastic integral-based estimating equations in a sequential manner across quantile levels. In this paper, we analyze CQR in a high dimensional setting where the regression functions over a continuum of quantile levels are of interest. We propose a two-step penalization procedure, which accommodates stochastic integral based estimating equations and address the challenges due to the recursive nature of the procedure. We establish the uniform convergence rates for the proposed estimators, and investigate the properties on weak convergence and variable selection. We conduct numerical studies to confirm our theoretical findings and illustrate the practical utility of our proposals.","High dimensional survival data,varying covariate effects,censored quantile regression","Zheng, Qi@University of Louisville::Peng, Limin@Emory University::He, Xuming@University of Michigan System@University of Michigan"
CHERNOFF INDEX FOR COX TEST OF SEPARATE PARAMETRIC FAMILIES,ANNALS OF STATISTICS,32,"The asymptotic efficiency of a generalized likelihood ratio test proposed by Cox is studied under the large deviations framework for error probabilities developed by Chernoff. In particular, two separate parametric families of hypotheses are considered [In Proc. 4th Berkeley Sympos. Math. Statist. and Prob. (1961) 105-123; J. Roy. Statist. Soc. Ser. B 24 (1962) 406-424]. The significance level is set such that the maximal type I and type II error probabilities for the generalized likelihood ratio test decay exponentially fast with the same rate. We derive the analytic form of such a rate that is also known as the Chernoff index [Ann. Math. Stat. 23 (1952) 493-507], a relative efficiency measure when there is no preference between the null and the alternative hypotheses. We further extend the analysis to approximate error probabilities when the two families are not completely separated. Discussions are provided concerning the implications of the present result on model selection.","Asymptotic relative efficiency,generalized likelihood ratio,generalized linear models,large deviation,model selection,nonnested hypotheses,variable selection","Li, Xiaoou@University of Minnesota Twin Cities@University of Minnesota System::Liu, Jingchen@Columbia University::Ying, Zhiliang@Columbia University"
OPTIMAL BOUNDS FOR AGGREGATION OF AFFINE ESTIMATORS,ANNALS OF STATISTICS,41,"We study the problem of aggregation of estimators when the estimators are not independent of the data used for aggregation and no sample splitting is allowed. If the estimators are deterministic vectors, it is well known that the minimax rate of aggregation is of order log(M), where M is the number of estimators to aggregate. It is proved that for affine estimators, the minimax rate of aggregation is unchanged: it is possible to handle the linear dependence between the affine estimators and the data used for aggregation at no extra cost. The minimax rate is not impacted either by the variance of the affine estimators, or any other measure of their statistical complexity. The minimax rate is attained with a penalized procedure over the convex hull of the estimators, for a penalty that is inspired from the Q-aggregation procedure. The results follow from the interplay between the penalty, strong convexity and concentration.","Affine estimator,aggregation,sequence model,sharp oracle inequality,concentration inequality,Hanson-Wright","Bellec, Pierre C.@Universite Paris Saclay (ComUE)@Rutgers State University New Brunswick@ENSAE ParisTech"
RATE-OPTIMAL PERTURBATION BOUNDS FOR SINGULAR SUBSPACES WITH APPLICATIONS TO HIGH-DIMENSIONAL STATISTICS,ANNALS OF STATISTICS,60,"Perturbation bounds for singular spaces, in particularWedin's sin Theta theorem, are a fundamental tool in many fields including high-dimensional statistics, machine learning and applied mathematics. In this paper, we establish separate perturbation bounds, measured in both spectral and Frobenius sin Theta distances, for the left and right singular subspaces. Lower bounds, which show that the individual perturbation bounds are rate-optimal, are also given.

The new perturbation bounds are applicable to a wide range of problems. In this paper, we consider in detail applications to low-rank matrix denoising and singular space estimation, high-dimensional clustering and canonical correlation analysis (CCA). In particular, separate matching upper and lower bounds are obtained for estimating the left and right singular spaces. To the best of our knowledge, this is the first result that gives different optimal rates for the left and right singular spaces under the same perturbation.","Canonical correlation analysis,clustering,high-dimensional statistics,low-rank matrix denoising,perturbation bound,singular value decomposition,sin Theta distances,spectral method","Cai, T. Tony@University of Pennsylvania::Zhang, Anru@University of Wisconsin System@University of Wisconsin Madison"
EXACT FORMULAS FOR THE NORMALIZING CONSTANTS OF WISHART DISTRIBUTIONS FOR GRAPHICAL MODELS,ANNALS OF STATISTICS,39,"Gaussian graphical models have received considerable attention during the past four decades from the statistical and machine learning communities. In Bayesian treatments of this model, the G-Wishart distribution serves as the conjugate prior for inverse covariance matrices satisfying graphical constraints. While it is straightforward to posit the unnormalized densities, the normalizing constants of these distributions have been known only for graphs that are chordal, or decomposable. Up until now, it was unknown whether the normalizing constant for a general graph could be represented explicitly, and a considerable body of computational literature emerged that attempted to avoid this apparent intractability. We close this question by providing an explicit representation of the G-Wishart normalizing constant for general graphs.","Bartlett decomposition,bipartite graph,Cholesky decomposition,chordal graph,directed acyclic graph,G-Wishart distribution,Gaussian graphical model,generalized hypergeometric function of matrix argument,moral graph,normalizing constant,Wishart distribution","Uhler, Caroline@Massachusetts Institute of Technology (MIT)::Lenkoski, Alex@Unknow::Richards, Donald@Penn State University@Pennsylvania Commonwealth System of Higher Education (PCSHE)"
CONSISTENT PARAMETER ESTIMATION FOR LASSO AND APPROXIMATE MESSAGE PASSING,ANNALS OF STATISTICS,41,"This paper studies the optimal tuning of the regularization parameter in LASSO or the threshold parameters in approximate message passing (AMP). Considering a model in which the design matrix and noise are zero-mean i.i.d. Gaussian, we propose a data-driven approach for estimating the regularization parameter of LASSO and the threshold parameters in AMP. Our estimates are consistent, that is, they converge to their asymptotically optimal values in probability as n, the number of observations, and p, the ambient dimension of the sparse vector, grow to infinity, while n/p converges to a fixed number delta. As a byproduct of our analysis, we will shed light on the asymptotic properties of the solution paths of LASSO and AMP.","LASSO,estimation,sparsity,approximate message passing","Mousavi, Ali@Rice University::Maleki, Arian@Columbia University::Baraniuk, Richard G.@Rice University"
ON SEMIDEFINITE RELAXATIONS FOR THE BLOCK MODEL,ANNALS OF STATISTICS,58,"The stochastic block model (SBM) is a popular tool for community detection in networks, but fitting it by maximum likelihood (MLE) involves a computationally infeasible optimization problem. We propose a new semidefinite programming (SDP) solution to the problem of fitting the SBM, derived as a relaxation of the MLE. We put ours and previously proposed SDPs in a unified framework, as relaxations of the MLE over various subclasses of the SBM, which also reveals a connection to the well-known problem of sparse PCA. Our main relaxation, which we call SDP-1, is tighter than other recently proposed SDP relaxations, and thus previously established theoretical guarantees carry over. However, we show that SDP-1 exactly recovers true communities over a wider class of SBMs than those covered by current results. In particular, the assumption of strong assortativity of the SBM, implicit in consistency conditions for previously proposed SDPs, can be relaxed to weak assortativity for our approach, thus significantly broadening the class of SBMs covered by the consistency results. We also show that strong assortativity is indeed a necessary condition for exact recovery for previously proposed SDP approaches and not an artifact of the proofs. Our analysis of SDPs is based on primal-dual witness constructions, which provides some insight into the nature of the solutions of various SDPs. In particular, we show how to combine features from SDP-1 and already available SDPs to achieve the most flexibility in terms of both assortativity and block-size constraints, as our relaxation has the tendency to produce communities of similar sizes. This tendency makes it the ideal tool for fitting network histograms, a method gaining popularity in the graphon estimation literature, as we illustrate on an example of a social networks of dolphins. We also provide empirical evidence that SDPs outperform spectral methods for fitting SBMs with a large number of blocks.","Community detection,network,semidefinite programming,stochastic block model","Amini, Arash A.@University of California Los Angeles@University of California System::Levina, Elizaveta@University of Michigan System@University of Michigan"
Expectation propagation in the large data limit,JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES B-STATISTICAL METHODOLOGY,30,"Expectation propagation (EP) is a widely successful algorithm for variational inference. EP is an iterative algorithm used to approximate complicated distributions, typically to find a Gaussian approximation of posterior distributions. In many applications of this type, EP performs extremely well. Surprisingly, despite its widespread use, there are very few theoretical guarantees on Gaussian EP, and it is quite poorly understood. To analyse EP, we first introduce a variant of EP: averaged EP, which operates on a smaller parameter space. We then consider averaged EP and EP in the limit of infinite data, where the overall contribution of each likelihood term is small and where posteriors are almost Gaussian. In this limit, we prove that the iterations of both averaged EP and EP are simple: they behave like iterations of Newton's algorithm for finding the mode of a function. We use this limit behaviour to prove that EP is asymptotically exact, and to obtain other insights into the dynamic behaviour of EP, e.g. that it may diverge under poor initialization exactly like Newton's method. EP is a simple algorithm to state, but a difficult one to study. Our results should facilitate further research into the theoretical properties of this important method.","Expectation propagation,Variational inference","Dehaene, Guillaume@Ecole Polytechnique Federale de Lausanne@University of Geneva::Barthelme, Simon@Unknow"
Inference for empirical Wasserstein distances on finite spaces,JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES B-STATISTICAL METHODOLOGY,87,"The Wasserstein distance is an attractive tool for data analysis but statistical inference is hindered by the lack of distributional limits. To overcome this obstacle, for probability measures supported on finitely many points, we derive the asymptotic distribution of empirical Wasserstein distances as the optimal value of a linear programme with random objective function. This facilitates statistical inference (e.g. confidence intervals for sample-based Wasserstein distances) in large generality. Our proof is based on directional Hadamard differentiability. Failure of the classical bootstrap and alternatives are discussed. The utility of the distributional results is illustrated on two data sets.","Bootstrap,Central limit theorem,Directional Hadamard derivative,Hypothesis testing,Optimal transport,Wasserstein distance","Sommerfeld, Max@University of Gottingen::Munk, Axel@University of Gottingen@Max Planck Society"
A geometric approach to confidence regions and bands for functional parameters,JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES B-STATISTICAL METHODOLOGY,23,"Functional data analysis is now a well-established discipline of statistics, with its core concepts and perspectives in place. Despite this, there are still fundamental statistical questions which have received relatively little attention. One of these is the systematic construction of confidence regions for functional parameters. This work is concerned with developing, understanding and visualizing such regions. We provide a general strategy for constructing confidence regions in a real separable Hilbert space by using hyperellipsoids and hyper-rectangles. We then propose specific implementations which work especially well in practice. They provide powerful hypothesis tests and useful visualization tools without relying on simulation. We also demonstrate the negative result that nearly all regions, including our own, have zero coverage when working with empirical covariances. To overcome this challenge we propose a new paradigm for evaluating confidence regions by showing that the distance between an estimated region and the desired region (with proper coverage) tends to 0 faster than the regions shrink to a point. We call this phenomena ghosting and refer to the empirical regions as ghost regions. We illustrate the proposed methods in a simulation study and an application to fractional anisotropy tract profile data.","Confidence bands,Confidence regions,Functional data analysis,Hypothesis testing,Principal component analysis","Choi, Hyunphil@Penn State University@Pennsylvania Commonwealth System of Higher Education (PCSHE)::Reimherr, Matthew@Penn State University@Pennsylvania Commonwealth System of Higher Education (PCSHE)"
An imputation-regularized optimization algorithm for high dimensional missing data problems and beyond,JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES B-STATISTICAL METHODOLOGY,64,"Missing data are frequently encountered in high dimensional problems, but they are usually difficult to deal with by using standard algorithms, such as the expectation-maximization algorithm and its variants. To tackle this difficulty, some problem-specific algorithms have been developed in the literature, but there still lacks a general algorithm. This work is to fill the gap: we propose a general algorithm for high dimensional missing data problems. The algorithm works by iterating between an imputation step and a regularized optimization step. At the imputation step, the missing data are imputed conditionally on the observed data and the current estimates of parameters and, at the regularized optimization step, a consistent estimate is found via the regularization approach for the minimizer of a Kullback-Leibler divergence defined on the pseudocomplete data. For high dimensional problems, the consistent estimate can be found under sparsity constraints. The consistency of the averaged estimate for the true parameter can be established under quite general conditions. The algorithm is illustrated by using high dimensional Gaussian graphical models, high dimensional variable selection and a random-coefficient model.","Expectation-maximization algorithm,Gaussian graphical model,Gibbs sampler,Imputation consistency,Random-coefficient model,Variable selection","Liang, Faming@Purdue University@Purdue University System::Jia, Bochao@University of Florida@State University System of Florida::Xue, Jingnan@Unknow::Li, Qizhai@Chinese Academy of Sciences::Luo, Ye@University of Florida@State University System of Florida"
False discovery rate control for high dimensional networks of quantile associations conditioning on covariates,JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES B-STATISTICAL METHODOLOGY,36,"Motivated by gene coexpression pattern analysis, we propose a novel sample quantile contingency (SQUAC) statistic to infer quantile associations conditioning on covariates. It features enhanced flexibility in handling variables with both arbitrary distributions and complex association patterns conditioning on covariates. We first derive its asymptotic null distribution, and then develop a multiple-testing procedure based on the SQUAC statistic to test simultaneously the independence between one pair of variables conditioning on covariates for all p(p-1)/2 pairs. Here, p is the length of the outcomes and could exceed the sample size. The testing procedure does not require resampling or perturbation and thus is computationally efficient. We prove by theory and numerical experiments that this testing method asymptotically controls the false discovery rate. It outperforms all alternative methods when the complex association patterns exist. Applied to a gastric cancer data set, this testing method successfully inferred the gene coexpression networks of early and late stage patients. It identified more changes in the networks which are associated with cancer survivals. We extend our method to the case that both the length of the outcomes and the length of covariates exceed the sample size, and show that the asymptotic theory still holds.","False discovery rate,Gene coexpression networks,High dimensional networks,Quantile regression","Xie, Jichun@Duke University::Li, Ruosha@University of Texas Health Science Center Houston@University of Texas System"
Multiple matrix Gaussian graphs estimation,JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES B-STATISTICAL METHODOLOGY,53,"Matrix-valued data, where the sampling unit is a matrix consisting of rows and columns of measurements, are emerging in numerous scientific and business applications. Matrix Gaussian graphical models are a useful tool to characterize the conditional dependence structure of rows and columns. We employ non-convex penalization to tackle the estimation of multiple graphs from matrix-valued data under a matrix normal distribution. We propose a highly efficient non-convex optimization algorithm that can scale up for graphs with hundreds of nodes. We establish the asymptotic properties of the estimator, which requires less stringent conditions and has a sharper probability error bound than existing results. We demonstrate the efficacy of our proposed method through both simulations and real functional magnetic resonance imaging analyses.","Conditional independence,Gaussian graphical model,Matrix normal distribution,Non-convex penalization,Resting state functional magnetic resonance imaging,Sparsistency","Zhu, Yunzhang@Ohio State University::Li, Lexin@University of California Berkeley@University of California System"
On mitigating the analytical limitations of finely stratified experiments,JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES B-STATISTICAL METHODOLOGY,50,"Although attractive from a theoretical perspective, finely stratified experiments such as paired designs suffer from certain analytical limitations that are not present in block-randomized experiments with multiple treated and control individuals in each block. In short, when using a weighted difference in means to estimate the sample average treatment effect, the traditional variance estimator in a paired experiment is conservative unless the pairwise average treatment effects are constant across pairs; however, in more coarsely stratified experiments, the corresponding variance estimator is unbiased if treatment effects are constant within blocks, even if they vary across blocks. Using insights from classical least squares theory, we present an improved variance estimator that is appropriate in finely stratified experiments. The variance estimator remains conservative in expectation but is asymptotically no more conservative than the classical estimator and can be considerably less conservative. The magnitude of the improvement depends on the extent to which effect heterogeneity can be explained by observed covariates. Aided by this estimator, a new test for the null hypothesis of a constant treatment effect is proposed. These findings extend to some, but not all, superpopulation models, depending on whether the covariates are viewed as fixed across samples.","Agnostic regression,Average treatment effect,Neymanian inference,Stratified experiment","Fogarty, Colin B.@Massachusetts Institute of Technology (MIT)"
The correlated pseudomarginal method,JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES B-STATISTICAL METHODOLOGY,47,"The pseudomarginal algorithm is a Metropolis-Hastings-type scheme which samples asymptotically from a target probability density when we can only estimate unbiasedly an unnormalized version of it. In a Bayesian context, it is a state of the art posterior simulation technique when the likelihood function is intractable but can be estimated unbiasedly by using Monte Carlo samples. However, for the performance of this scheme not to degrade as the number T of data points increases, it is typically necessary for the number N of Monte Carlo samples to be proportional to T to control the relative variance of the likelihood ratio estimator appearing in the acceptance probability of this algorithm. The correlated pseudomarginal method is a modification of the pseudomarginal method using a likelihood ratio estimator computed by using two correlated likelihood estimators. For random-effects models, we show under regularity conditions that the parameters of this scheme can be selected such that the relative variance of this likelihood ratio estimator is controlled when N increases sublinearly with T and we provide guidelines on how to optimize the algorithm on the basis of a non-standard weak convergence analysis. The efficiency of computations for Bayesian inference relative to the pseudomarginal method empirically increases with T and exceeds two orders of magnitude in some examples.","Asymptotic posterior normality,Correlated random numbers,Intractable likelihood,Metropolis-Hastings algorithm,Particle filter,Random-effects model,Weak convergence","Deligiannidis, George@University of Oxford::Doucet, Arnaud@University of Oxford::Pitt, Michael K.@Kings College London@University of London"
Random-walk models of network formation and sequential Monte Carlo methods for graphs,JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES B-STATISTICAL METHODOLOGY,56,"We introduce a class of generative network models that insert edges by connecting the starting and terminal vertices of a random walk on the network graph. Within the taxonomy of statistical network models, this class is distinguished by permitting the location of a new edge to depend explicitly on the structure of the graph, but being nonetheless statistically and computationally tractable. In the limit of infinite walk length, the model converges to an extension of the preferential attachment modelin this sense, it can be motivated alternatively by asking what preferential attachment is an approximation to. Theoretical properties, including the limiting degree sequence, are studied analytically. If the entire history of the graph is observed, parameters can be estimated by maximum likelihood. If only the final graph is available, its history can be imputed by using Markov chain Monte Carlo methods. We develop a class of sequential Monte Carlo algorithms that are more generally applicable to sequential network models and may be of interest in their own right. The model parameters can be recovered from a single graph generated by the model. Applications to data clarify the role of the random-walk length as a length scale of interactions within the graph.","Generative models,Preferential attachment,Random walk,Sequential Monte Carlo methods,Sequential network models","Bloem-Reddy, Benjamin@University of Oxford::Orbanz, Peter@Columbia University"
Detecting heteroscedasticity in non-parametric regression using weighted empirical processes,JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES B-STATISTICAL METHODOLOGY,27,"Heteroscedastic errors can lead to inaccurate statistical conclusions if they are not properly handled. We introduce a test for heteroscedasticity for the non-parametric regression model with multiple covariates. It is based on a suitable residual-based empirical distribution function. The residuals are constructed by using local polynomial smoothing. Our test statistic involves a detection function' that can verify heteroscedasticity by exploiting just the independence-dependence structure between the detection function and model errors, i.e. we do not require a specific model of the variance function. The procedure is asymptotically distribution free: inferences made from it do not depend on unknown parameters. It is consistent at the parametric (root n) rate of convergence. Our results are extended to the case of missing responses and illustrated with simulations.","Heteroscedastic non-parametric regression,Local polynomial smoother,Missingness at random,Transfer principle,Weighted empirical process","Chown, Justin@Ruhr University Bochum::Mueller, Ursula U.@Texas A&M University College Station@Texas A&M University System"
Hybrid quantile regression estimation for time series models with conditional heteroscedasticity,JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES B-STATISTICAL METHODOLOGY,39,"Estimating conditional quantiles of financial time series is essential for risk management and many other financial applications. For time series models with conditional heteroscedasticity, although it is the generalized auto-regressive conditional heteroscedastic (GARCH) model that has the greatest popularity, quantile regression for this model usually gives rise to non-smooth non-convex optimization which may hinder its practical feasibility. The paper proposes an easy-to-implement hybrid quantile regression estimation procedure for the GARCH model, where we overcome the intractability due to the square-root form of the conditional quantile function by a simple transformation. The method takes advantage of the efficiency of the GARCH model in modelling the volatility globally as well as the flexibility of quantile regression in fitting quantiles at a specific level. The asymptotic distribution of the estimator is derived and is approximated by a novel mixed bootstrapping procedure. A portmanteau test is further constructed to check the adequacy of fitted conditional quantiles. The finite sample performance of the method is examined by simulation studies, and its advantages over existing methods are illustrated by an empirical application to value-at-risk forecasting.","Bootstrap method,Conditional quantile,Generalized auto-regressive conditional heteroscedasticity,Non-linear time series,Quantile regression","Zheng, Yao@University of Hong Kong::Zhu, Qianqian@Shanghai University of Finance & Economics::Li, Guodong@University of Hong Kong::Xiao, Zhijie@Boston College"
Full likelihood inference for abundance from continuous time capture-recapture data,JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES B-STATISTICAL METHODOLOGY,51,"Capture-recapture experiments are widely used cost-effective sampling techniques for estimating population sizes or abundances in biology, ecology, demography, epidemiology and reliability studies. For continuous time capture-recapture data, existing estimation methods are based on conditional likelihoods and an inverse weighting estimating equation. The corresponding Wald-type confidence intervals for the abundance may have severe undercoverage, and their lower limits can be below the number of individuals captured. We propose a full likelihood inference approach by combining a parametric or partial likelihood with the empirical likelihood. Under both parametric and semiparametric intensity models, we demonstrate that the maximum likelihood estimator attains the semiparametric efficiency lower bound and that the full likelihood ratio statistic for the abundance is asymptotically (2) with 1 degree of freedom. Simulations indicate that compared with conditional-likelihood-based methods, the maximum full likelihood estimator has a smaller mean-square error, and the likelihood ratio confidence intervals often have remarkable gains in coverage probability. We illustrate the advantages of the proposed approach by analysing illegal immigrant data for the Netherlands and Prinia flaviventris data from Hong Kong.","Abundance,Andersen-Gill model,Capture-recapture experiment,Conditional likelihood,Empirical likelihood","Liu, Yang@East China Normal University::Liu, Yukun@East China Normal University::Li, Pengfei@University of Waterloo::Qin, Jing@NIH National Institute of Allergy & Infectious Diseases (NIAID)@National Institutes of Health (NIH) - USA"
Sparse generalized eigenvalue problem: optimal statistical rates via truncated Rayleigh flow,JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES B-STATISTICAL METHODOLOGY,52,"The sparse generalized eigenvalue problem (GEP) plays a pivotal role in a large family of high dimensional statistical models, including sparse Fisher's discriminant analysis, canonical correlation analysis and sufficient dimension reduction. The sparse GEP involves solving a non-convex optimization problem. Most existing methods and theory in the context of specific statistical models that are special cases of the sparse GEP require restrictive structural assumptions on the input matrices. We propose a two-stage computational framework to solve the sparse GEP. At the first stage, we solve a convex relaxation of the sparse GEP. Taking the solution as an initial value, we then exploit a non-convex optimization perspective and propose the truncated Rayleigh flow method (which we call rifle') to estimate the leading generalized eigenvector. We show that rifle converges linearly to a solution with the optimal statistical rate of convergence. Theoretically, our method significantly improves on the existing literature by eliminating structural assumptions on the input matrices. To achieve this, our analysis involves two key ingredients: a new analysis of the gradient-based method on non-convex objective functions, and a fine-grained characterization of the evolution of sparsity patterns along the solution path. Thorough numerical studies are provided to validate the theoretical results.","Convex relaxation,Non-convex optimization,Sparse canonical correlation analysis,Sparse Fisher's discriminant analysis,Sparse sufficient dimension reduction","Tan, Kean Ming@University of Minnesota Twin Cities@University of Minnesota System::Wang, Zhaoran@Northwestern University::Liu, Han@Northwestern University::Zhang, Tong@Unknow"
Bayesian regression tree ensembles that adapt to smoothness and sparsity,JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES B-STATISTICAL METHODOLOGY,37,"Ensembles of decision trees are a useful tool for obtaining flexible estimates of regression functions. Examples of these methods include gradient-boosted decision trees, random forests and Bayesian classification and regression trees. Two potential shortcomings of tree ensembles are their lack of smoothness and their vulnerability to the curse of dimensionality. We show that these issues can be overcome by instead considering sparsity inducing soft decision trees in which the decisions are treated as probabilistic. We implement this in the context of the Bayesian additive regression trees framework and illustrate its promising performance through testing on benchmark data sets. We provide strong theoretical support for our methodology by showing that the posterior distribution concentrates at the minimax rate (up to a logarithmic factor) for sparse functions and functions with additive structures in the high dimensional regime where the dimensionality of the covariate space is allowed to grow nearly exponentially in the sample size. Our method also adapts to the unknown smoothness and sparsity levels, and can be implemented by making minimal modifications to existing Bayesian additive regression tree algorithms.","Bayesian additive regression trees,Bayesian non-parametrics,High dimensional regimes,Model averaging,Posterior consistency","Linero, Antonio R.@Florida State University@State University System of Florida::Yang, Yun@University of Illinois Urbana-Champaign@University of Illinois System"
AdaPT: an interactive procedure for multiple testing with side information,JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES B-STATISTICAL METHODOLOGY,52,"We consider the problem of multiple-hypothesis testing with generic side information: for each hypothesis H-i we observe both a p-value p(i) and some predictor x(i) encoding contextual information about the hypothesis. For large-scale problems, adaptively focusing power on the more promising hypotheses (those more likely to yield discoveries) can lead to much more powerful multiple-testing procedures. We propose a general iterative framework for this problem, the adaptive p-value thresholding procedure which we call AdaPT, which adaptively estimates a Bayes optimal p-value rejection threshold and controls the false discovery rate in finite samples. At each iteration of the procedure, the analyst proposes a rejection threshold and observes partially censored p-values, estimates the false discovery proportion below the threshold and proposes another threshold, until the estimated false discovery proportion is below alpha. Our procedure is adaptive in an unusually strong sense, permitting the analyst to use any statistical or machine learning method she chooses to estimate the optimal threshold, and to switch between different models at each iteration as information accrues. We demonstrate the favourable performance of AdaPT by comparing it with state of the art methods in five real applications and two simulation studies.","Adaptive inference,False discovery rate,Martingales,Multiple testing,p-value weighting,Selective inference","Lei, Lihua@University of California Berkeley@University of California System::Fithian, William@University of California Berkeley@University of California System"
Asymptotic properties and information criteria for misspecified generalized linear mixed models,JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES B-STATISTICAL METHODOLOGY,38,"The problem of misspecification poses challenges in model selection. The paper studies the asymptotic properties of estimators for generalized linear mixed models with misspecification under the framework of conditional Kullback-Leibler divergence. A conditional generalized information criterion is introduced, and a model selection procedure is proposed by minimizing the criterion. We prove that the model selection procedure proposed is asymptotically loss efficient when all the candidate models are misspecified. The model selection consistency of the model selection procedure is also established when the true data-generating procedure lies within the set of candidate models. Simulation experiments confirm the effectiveness of the method proposed. The use of the criterion for model selection is illustrated through an analysis of the European Currency Opinion Survey data.","Asymptotic loss efficiency,Conditional inference,Misspecified generalized linear mixed model,Model selection,Penalized likelihood","Yu, Dalei@Yunnan University of Finance & Economics::Zhang, Xinyu@Chinese Academy of Sciences@Academy of Mathematics & System Sciences, CAS::Yau, Kelvin K. W.@City University of Hong Kong"
Maximin projection learning for optimal treatment decision with heterogeneous individualized treatment effects,JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES B-STATISTICAL METHODOLOGY,21,"A salient feature of data from clinical trials and medical studies is inhomogeneity. Patients not only differ in baseline characteristics, but also in the way that they respond to treatment. Optimal individualized treatment regimes are developed to select effective treatments based on patient's heterogeneity. However, the optimal treatment regime might also vary for patients across different subgroups. We mainly consider patients' heterogeneity caused by groupwise individualized treatment effects assuming the same marginal treatment effects for all groups. We propose a new maximin projection learning method for estimating a single treatment decision rule that works reliably for a group of future patients from a possibly new subpopulation. Based on estimated optimal treatment regimes for all subgroups, the proposed maximin treatment regime is obtained by solving a quadratically constrained linear programming problem, which can be efficiently computed by interior point methods. Consistency and asymptotic normality of the estimator are established. Numerical examples show the reliability of the methodology proposed.","Heterogeneity,Maximin projection learning,Optimal treatment regime,Quadratically constrained linear programming","Shi, Chengchun@North Carolina State University@University of North Carolina::Song, Rui@North Carolina State University@University of North Carolina::Lu, Wenbin@North Carolina State University@University of North Carolina::Fu, Bo@Fudan University"
Semiparametrically efficient estimation in quantile regression of secondary analysis,JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES B-STATISTICAL METHODOLOGY,19,"Analysing secondary outcomes is a common practice for case-control studies. Traditional secondary analysis employs either completely parametric models or conditional mean regression models to link the secondary outcome to covariates. In many situations, quantile regression models complement mean-based analyses and provide alternative new insights on the associations of interest. For example, biomedical outcomes are often highly asymmetric, and median regression is more useful in describing the central' behaviour than mean regressions. There are also cases where the research interest is to study the high or low quantiles of a population, as they are more likely to be at risk. We approach the secondary quantile regression problem from a semiparametric perspective, allowing the covariate distribution to be completely unspecified. We derive a class of consistent semiparametric estimators and identify the efficient member. The asymptotic properties of the resulting estimators are established. Simulation results and a real data analysis are provided to demonstrate the superior performance of our approach with a comparison with the only existing approach so far in the literature.","Biased samples,Case-control study,Heteroscedastic errors,Quantile regression,Secondary analysis,Semiparametric estimation","Liang, Liang@Texas A&M University College Station@Texas A&M University System::Ma, Yanyuan@Penn State University@Pennsylvania Commonwealth System of Higher Education (PCSHE)::Wei, Ying@Columbia University::Carroll, Raymond J.@Texas A&M University College Station@Texas A&M University System@University of Technology Sydney"
On structure testing for component covariance matrices of a high dimensional mixture,JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES B-STATISTICAL METHODOLOGY,36,"By studying the family of p-dimensional scale mixtures, the paper shows for the first time a non-trivial example where the eigenvalue distribution of the corresponding sample covariance matrix does not converge to the celebrated Marenko-Pastur law. A different and new limit is found and characterized. The reasons for failure of the Marenko-Pastur limit in this situation are found to be a strong dependence between the p-co-ordinates of the mixture. Next, we address the problem of testing whether the mixture has a spherical covariance matrix. To analyse the traditional John's-type test we establish a novel and general central limit theorem for linear statistics of eigenvalues of the sample covariance matrix. It is shown that John's test and its recent high dimensional extensions both fail for high dimensional mixtures, precisely because of the different spectral limit above. As a remedy, a new test procedure is constructed afterwards for the sphericity hypothesis. This test is then applied to identify the covariance structure in model-based clustering. It is shown that the test has much higher power than the widely used integrated classification likelihood and Bayesian information criteria in detecting non-spherical component covariance matrices of a high dimensional mixture.","Large covariance matrix,Marenko-Pastur law,Sphericity test","Li, Weiming@Shanghai University of Finance & Economics::Yao, Jianfeng@University of Hong Kong"
From multiple Gaussian sequences to functional data and beyond: a Stein estimation approach,JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES B-STATISTICAL METHODOLOGY,40,"We expand the notion of Gaussian sequence models to n experiments and propose a Stein estimation strategy which relies on pooling information across experiments. An oracle inequality is established to assess conditional risks given the underlying effects, based on which we can quantify the size of relative error and obtain a tuning-free recovery strategy that is easy to compute, produces model parsimony and extends to unknown variance. We show that the simultaneous recovery is adaptive to an oracle strategy, which also enjoys a robustness guarantee in a minimax sense. A connection to functional data is established, via Le Cam theory, for fixed and random designs under general regularity settings. We further extend the model projection to general bases with mild conditions on correlation structure and conclude with potential application to other statistical problems. Simulated and real data examples are provided to lend empirical support to the methodology proposed and to illustrate the potential for substantial computational savings.","Conditional oracle inequality,Functional data,Le Cam equivalence,Non-parametric regression,Simultaneous recovery,Wavelets","Koudstaal, Mark@University of Toronto::Yao, Fang@University of Toronto@Peking University"
Unified empirical likelihood ratio tests for functional concurrent linear models and the phase transition from sparse to dense functional data,JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES B-STATISTICAL METHODOLOGY,48,"We consider the problem of testing functional constraints in a class of functional concurrent linear models where both the predictors and the response are functional data measured at discrete time points. We propose test procedures based on the empirical likelihood with bias-corrected estimating equations to conduct both pointwise and simultaneous inferences. The asymptotic distributions of the test statistics are derived under the null and local alternative hypotheses, where sparse and dense functional data are considered in a unified framework. We find a phase transition in the asymptotic null distributions and the orders of detectable alternatives from sparse to dense functional data. Specifically, the tests proposed can detect alternatives of n-order when the number of repeated measurements per curve is of an order larger than n0 with n being the number of curves. The transition points 0 for pointwise and simultaneous tests are different and both are smaller than the transition point in the estimation problem. Simulation studies and real data analyses are conducted to demonstrate the methods proposed.","Empirical likelihood,Functional analysis of variance,Non-parametric hypothesis testing,Unified inference","Wang, Honglang@Indiana University-Purdue University Indianapolis@Indiana University System::Zhong, Ping-Shou@Michigan State University::Cui, Yuehua@Michigan State University::Li, Yehua@Iowa State University"
A block model for node popularity in networks with community structure,JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES B-STATISTICAL METHODOLOGY,42,"The community structure that is observed in empirical networks has been of particular interest in the statistics literature, with a strong emphasis on the study of block models. We study an important network feature called node popularity, which is closely associated with community structure. Neither the classical stochastic block model nor its degree-corrected extension can satisfactorily capture the dynamics of node popularity as observed in empirical networks. We propose a popularity-adjusted block model for flexible and realistic modelling of node popularity. We establish consistency of likelihood modularity for community detection as well as estimation of node popularities and model parameters, and demonstrate the advantages of the new modularity over the degree-corrected block model modularity in simulations. By analysing the political blogs network, the British Members of Parliament network and the Digital bibliography and library project' bibliographical network, we illustrate that improved empirical insights can be gained through this methodology.","Community detection,Degree-corrected block model,Likelihood modularity,Node popularity,Popularity-adjusted block model,Stochastic block model","Sengupta, Srijan@Virginia Polytechnic Institute & State University::Chen, Yuguo@University of Illinois Urbana-Champaign@University of Illinois System"
Matrix variate regressions and envelope models,JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES B-STATISTICAL METHODOLOGY,45,"Modern technology often generates data with complex structures in which both response and explanatory variables are matrix valued. Existing methods in the literature can tackle matrix-valued predictors but are rather limited for matrix-valued responses. We study matrix variate regressions for such data, where the response Y on each experimental unit is a random matrix and the predictor X can be either a scalar, a vector or a matrix, treated as non-stochastic in terms of the conditional distribution Y|X. We propose models for matrix variate regressions and then develop envelope extensions of these models. Under the envelope framework, redundant variation can be eliminated in estimation and the number of parameters can be notably reduced when the matrix variate dimension is large, possibly resulting in significant gains in efficiency. The methods proposed are applicable to high dimensional settings.","Matrix-valued response,Matrix variate regression,Reducing subspace,Sufficient dimension reduction","Ding, Shanshan@University of Delaware::Cook, R. Dennis@University of Minnesota Twin Cities@University of Minnesota System"
Semiparametric dynamic max-copula model for multivariate time series,JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES B-STATISTICAL METHODOLOGY,33,"The paper presents a novel non-linear framework for the construction of flexible multivariate dependence structure (i.e. copulas) from existing copulas based on a straightforward pairwise max-'rule. The newly constructed max-copula has a closed form and has strong interpretability. Compared with the classical linear symmetric' mixture copula, the max-copula can be viewed as a non-linear asymmetric' framework. It is capable of modelling asymmetric dependence and joint tail behaviour while also offering good performance in non-extremal behaviour modelling. Max-copulas that are based on single-factor and block factor models are developed to offer parsimonious modelling for structured dependence, especially in high dimensional applications. Combined with semiparametric time series models, the max-copula can be used to develop flexible and accurate models for multivariate time series. A new semiparametric composite maximum likelihood method is proposed for parameter estimation, where the consistency and asymptotic normality of estimators are established. The flexibility of the max-copula and the accuracy of the proposed estimation procedure are illustrated through extensive numerical experiments. Real data applications in value-at-risk estimation and portfolio optimization for financial risk management demonstrate the max-copula's promising ability to capture accurately joint movements of high dimensional multivariate stock returns under both normal and crisis regimes of the financial market.","Asymmetric dependence,Composite maximum likelihood,Copula construction,Market crisis,Mixture modelling,Tail dependence","Zhao, Zifeng@University of Wisconsin System@University of Wisconsin Madison::Zhang, Zhengjun@University of Wisconsin System@University of Wisconsin Madison"
Kernel-based tests for joint independence,JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES B-STATISTICAL METHODOLOGY,42,"We investigate the problem of testing whether d possibly multivariate random variables, which may or may not be continuous, are jointly (or mutually) independent. Our method builds on ideas of the two-variable Hilbert-Schmidt independence criterion but allows for an arbitrary number of variables. We embed the joint distribution and the product of the marginals in a reproducing kernel Hilbert space and define the d-variable Hilbert-Schmidt independence criterion dHSIC as the squared distance between the embeddings. In the population case, the value of dHSIC is 0 if and only if the d variables are jointly independent, as long as the kernel is characteristic. On the basis of an empirical estimate of dHSIC, we investigate three non-parametric hypothesis tests: a permutation test, a bootstrap analogue and a procedure based on a gamma approximation. We apply non-parametric independence testing to a problem in causal discovery and illustrate the new methods on simulated and real data sets.","Causal inference,Independence test,Kernel methods,V-statistics","Pfister, Niklas@Unknow::Buhlmann, Peter@Unknow::Schoelkopf, Bernhard@Max Planck Society::Peters, Jonas@University of Copenhagen@Max Planck Society"
Statistical inference based on randomly generated auxiliary variables,JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES B-STATISTICAL METHODOLOGY,20,"In most real life studies, auxiliary variables are available and are employed to explain and understand missing data patterns and to evaluate and control causal relationships with variables of interest. Usually their availability is assumed to be a fact, even if the variables are measured without the objectives of the study in mind. As a result, inference with missing data and causal inference require some assumptions that cannot easily be validated or checked. In this paper, a framework is constructed in which auxiliary variables are treated as a selection, possibly random, from the universe of variables on a population. This framework provides conditions to make statistical inference beyond the traces of bias or effects found by the auxiliary variables themselves. The utility of the framework is demonstrated for the analysis and reduction of non-response in surveys. However, the framework may be more generally used to understand the strength of association between variables. Important roles are played by the diversity and diffusion of the population of interest, features that are defined in the paper and the estimation of which is discussed.","Causal inference,Independent variable,Missing data,Non-response,Surveys","Schouten, Barry@Utrecht University"
High dimensional change point estimation via sparse projection,JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES B-STATISTICAL METHODOLOGY,51,"Change points are a very common feature of big data' that arrive in the form of a data stream. We study high dimensional time series in which, at certain time points, the mean structure changes in a sparse subset of the co-ordinates. The challenge is to borrow strength across the co-ordinates to detect smaller changes than could be observed in any individual component series. We propose a two-stage procedure called inspect for estimation of the change points: first, we argue that a good projection direction can be obtained as the leading left singular vector of the matrix that solves a convex optimization problem derived from the cumulative sum transformation of the time series. We then apply an existing univariate change point estimation algorithm to the projected series. Our theory provides strong guarantees on both the number of estimated change points and the rates of convergence of their locations, and our numerical studies validate its highly competitive empirical performance for a wide range of data-generating mechanisms. Software implementing the methodology is available in the R package InspectChangepoint.","Change point estimation,Convex optimization,Dimension reduction,Piecewise stationary,Segmentation,Sparsity","Wang, Tengyao@University of Cambridge::Samworth, Richard J.@University of Cambridge"
Optimal a priori balance in the design of controlled experiments,JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES B-STATISTICAL METHODOLOGY,53,"We develop a unified theory of designs for controlled experiments that balance baseline covariates a priori (before treatment and before randomization) using the framework of minimax variance and a new method called kernel allocation. We show that any notion of a priori balance must go hand in hand with a notion of structure, since with no structure on the dependence of outcomes on baseline covariates complete randomization (no special covariate balance) is always minimax optimal. Restricting the structure of dependence, either parametrically or non-parametrically, gives rise to certain covariate imbalance metrics and optimal designs. This recovers many popular imbalance metrics and designs previously developed ad hoc, including randomized block designs, pairwise-matched allocation and rerandomization. We develop a new design method called kernel allocation based on the optimal design when structure is expressed by using kernels, which can be parametric or non-parametric. Relying on modern optimization methods, kernel allocation, which ensures nearly perfect covariate balance without biasing estimates under model misspecification, offers sizable advantages in precision and power as demonstrated in a range of real and synthetic examples. We provide strong theoretical guarantees on variance, consistency and rates of convergence and develop special algorithms for design and hypothesis testing.","Causal inference,Controlled experimentation,Covariate balance,Functional analysis,Mixed integer programming,Semidefinite programming","Kallus, Nathan@Cornell University"
Goodness-of-fit tests for high dimensional linear models,JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES B-STATISTICAL METHODOLOGY,45,"We propose a framework for constructing goodness-of-fit tests in both low and high dimensional linear models. We advocate applying regression methods to the scaled residuals following either an ordinary least squares or lasso fit to the data, and using some proxy for prediction error as the final test statistic. We call this family residual prediction tests. We show that simulation can be used to obtain the critical values for such tests in the low dimensional setting and demonstrate using both theoretical results and extensive numerical studies that some form of the parametric bootstrap can do the same when the high dimensional linear model is under consideration. We show that residual prediction tests can be used to test for significance of groups or individual variables as special cases, and here they compare favourably with state of the art methods, but we also argue that they can be designed to test for as diverse model misspecifications as heteroscedasticity and non-linearity.","Bootstrap,Diagnostics,Goodness of fit,High dimensional models,Lasso","Shah, Rajen D.@University of Cambridge::Buhlmann, Peter@Unknow"
False discovery proportion estimation by permutations: confidence for significance analysis of microarrays,JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES B-STATISTICAL METHODOLOGY,25,"Significance analysis of microarrays (SAM) is a highly popular permutation-based multiple-testing method that estimates the false discovery proportion (FDP): the fraction of false positive results among all rejected hypotheses. Perhaps surprisingly, until now this method had no known properties. This paper extends SAM by providing 1- upper confidence bounds for the FDP, so that exact confidence statements can be made. As a special case, an estimate of the FDP is obtained that underestimates the FDP with probability at most 0.5. Moreover, using a closed testing procedure, this paper decreases the upper bounds and estimates in such a way that the confidence level is maintained. We base our methods on a general result on exact testing with random permutations.","Confidence,False discovery proportion,Multiple testing,Permutation","Hemerik, Jesse@Leiden University::Goeman, Jelle J.@Leiden University"
Exact Bayesian inference in spatiotemporal Cox processes driven by multivariate Gaussian processes,JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES B-STATISTICAL METHODOLOGY,33,"We present a novel inference methodology to perform Bayesian inference for spatiotemporal Cox processes where the intensity function depends on a multivariate Gaussian process. Dynamic Gaussian processes are introduced to enable evolution of the intensity function over discrete time. The novelty of the method lies on the fact that no discretization error is involved despite the non-tractability of the likelihood function and infinite dimensionality of the problem. The method is based on a Markov chain Monte Carlo algorithm that samples from the joint posterior distribution of the parameters and latent variables of the model. A particular choice of the dominating measure to obtain the likelihood function is shown to be crucial to devise a valid Markov chain Monte Carlo algorithm. The models are defined in a general and flexible way but they are amenable to direct sampling from the relevant distributions because of careful characterization of its components. The models also enable the inclusion of regression covariates and/or temporal components to explain the variability of the intensity function. These components may be subject to relevant interaction with space and/or time. Real and simulated examples illustrate the methodology, followed by concluding remarks.","Augmented model,Dynamic Gaussian process,Intractable likelihood,Markov chain Monte Carlo sampling,Point pattern","Goncalves, Flavio B.@Universidade Federal de Minas Gerais::Gamerman, Dani@Universidade Federal do Rio de Janeiro"
Another look at distance-weighted discrimination,JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES B-STATISTICAL METHODOLOGY,57,"Distance-weighted discrimination (DWD) is a modern margin-based classifier with an interesting geometric motivation. It was proposed as a competitor to the support vector machine (SVM). Despite many recent references on DWD, DWD is far less popular than the SVM, mainly because of computational and theoretical reasons. We greatly advance the current DWD methodology and its learning theory. We propose a novel thrifty algorithm for solving standard DWD and generalized DWD, and our algorithm can be several hundred times faster than the existing state of the art algorithm based on second-order cone programming. In addition, we exploit the new algorithm to design an efficient scheme to tune generalized DWD. Furthermore, we formulate a natural kernel DWD approach in a reproducing kernel Hilbert space and then establish the Bayes risk consistency of the kernel DWD by using a universal kernel such as the Gaussian kernel. This result solves an open theoretical problem in the DWD literature. A comparison study on 16 benchmark data sets shows that data-driven generalized DWD consistently delivers higher classification accuracy with less computation time than the SVM.","Bayes risk consistency,Classification,Distance-weighted discrimination,Kernel learning,Majorization-minimization principle,Second-order cone programming","Wang, Boxiang@University of Minnesota Twin Cities@University of Minnesota System::Zou, Hui@University of Minnesota Twin Cities@University of Minnesota System"
Approximate residual balancing: debiased inference of average treatment effects in high dimensions,JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES B-STATISTICAL METHODOLOGY,67,"There are many settings where researchers are interested in estimating average treatment effects and are willing to rely on the unconfoundedness assumption, which requires that the treatment assignment be as good as random conditional on pretreatment variables. The unconfoundedness assumption is often more plausible if a large number of pretreatment variables are included in the analysis, but this can worsen the performance of standard approaches to treatment effect estimation. We develop a method for debiasing penalized regression adjustments to allow sparse regression methods like the lasso to be used for root n-consistent inference of average treatment effects in high dimensional linear models. Given linearity, we do not need to assume that the treatment propensities are estimable, or that the average treatment effect is a sparse contrast of the outcome model parameters. Rather, in addition to standard assumptions used to make lasso regression on the outcome model consistent under 1-norm error, we require only overlap, i.e. that the propensity score be uniformly bounded away from 0 and 1. Procedurally, our method combines balancing weights with a regularized regression adjustment.","Causal inference,Potential outcomes,Propensity score,Sparse estimation","Athey, Susan@Stanford University::Imbens, Guido W.@Stanford University::Wager, Stefan@Stanford University"
Estimated Wold representation and spectral-density-driven bootstrap for time series,JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES B-STATISTICAL METHODOLOGY,34,"The second-order dependence structure of purely non-deterministic stationary processes is described by the coefficients of the famous Wold representation. These coefficients can be obtained by factorizing the spectral density of the process. This relationship together with some spectral density estimator is used to obtain consistent estimators of these coefficients. A spectral-density-driven bootstrap for time series is then developed which uses the entire sequence of estimated moving average coefficients together with appropriately generated pseudoinnovations to obtain a bootstrap pseudo-time-series. It is shown that if the underlying process is linear and if the pseudoinnovations are generated by means of an independent and identically distributed wild bootstrap which mimics, to the extent necessary, the moment structure of the true innovations, this bootstrap proposal asymptotically works for a wide range of statistics. The relationships of the proposed bootstrap procedure to some other bootstrap procedures, including the auto-regressive sieve bootstrap, are discussed. It is shown that the latter is a special case of the spectral-density-driven bootstrap, if a parametric auto-regressive spectral density estimator is used. Simulations investigate the performance of the new bootstrap procedure in finite sample situations. Furthermore, a real life data example is presented.","Bootstrap,Linear processes,Moving average representation,Spectral density estimation,Spectral density factorization","Krampe, Jonas@Braunschweig University of Technology::Kreiss, Jens-Peter@Braunschweig University of Technology::Paparoditis, Efstathios@University of Cyprus"
Bayesian inference for Gaussian graphical models beyond decomposable graphs,JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES B-STATISTICAL METHODOLOGY,26,"Bayesian inference for graphical models has received much attention in the literature in recent years. It is well known that, when the graph G is decomposable, Bayesian inference is significantly more tractable than in the general non-decomposable setting. Penalized likelihood inference in contrast has made tremendous gains in the past few years in terms of scalability and tractability. Bayesian inference, however, has not had the same level of success, though a scalable Bayesian approach has its strengths, especially in terms of quantifying uncertainty. To address this gap, we propose a scalable and flexible novel Bayesian approach for estimation and model selection in Gaussian undirected graphical models. We first develop a class of generalized G-Wishart distributions with multiple shape parameters for an arbitrary underlying graph. This class contains the G-Wishart distribution as a special case. We then introduce the class of generalized Bartlett graphs and derive an efficient Gibbs sampling algorithm to obtain posterior draws from generalized G-Wishart distributions corresponding to a generalized Bartlett graph. The class of generalized Bartlett graphs contains the class of decomposable graphs as a special case but is substantially larger than the class of decomposable graphs. We proceed to derive theoretical properties of the proposed Gibbs sampler. We then demonstrate that the proposed Gibbs sampler is scalable to significantly higher dimensional problems compared with using an accept-reject or a Metropolis-Hasting algorithm. Finally, we show the efficacy of the proposed approach on simulated and real data. In particular, we demonstrate that our generalized Bartlett methodology can be used for efficient model selection by reducing the graph search space by using penalized likelihood and pseudolikelihood methods.","Gaussian graphical models,Generalized Bartlett graph,Generalized G-Wishart distribution,Gibbs sampler,Scalable Bayesian inference","Khare, Kshitij@University of Florida@State University System of Florida::Rajaratnam, Bala@University of California Davis@University of California System::Saha, Abhishek@University of Florida@State University System of Florida"
Auxiliary gradient-based sampling algorithms,JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES B-STATISTICAL METHODOLOGY,26,"We introduce a new family of Markov chain Monte Carlo samplers that combine auxiliary variables, Gibbs sampling and Taylor expansions of the target density. Our approach permits the marginalization over the auxiliary variables, yielding marginal samplers, or the augmentation of the auxiliary variables, yielding auxiliary samplers. The well-known Metropolis-adjusted Langevin algorithm MALA and preconditioned Crank-Nicolson-Langevin algorithm pCNL are shown to be special cases. We prove that marginal samplers are superior in terms of asymptotic variance and demonstrate cases where they are slower in computing time compared with auxiliary samplers. In the context of latent Gaussian models we propose new auxiliary and marginal samplers whose implementation requires a single tuning parameter, which can be found automatically during the transient phase. Extensive experimentation shows that the increase in efficiency (measured as the effective sample size per unit of computing time) relative to (optimized implementations of) pCNL, elliptical slice sampling and MALA ranges from tenfold in binary classification problems to 25 fold in log-Gaussian Cox processes to 100 fold in Gaussian process regression, and it is on a par with Riemann manifold Hamiltonian Monte Carlo sampling in an example where that algorithm has the same complexity as the aforementioned algorithms. We explain this remarkable improvement in terms of the way that alternative samplers try to approximate the eigenvalues of the target. We introduce a novel Markov chain Monte Carlo sampling scheme for hyperparameter learning that builds on the auxiliary samplers. The MATLAB code for reproducing the experiments in the paper is publicly available and an on-line supplement to this paper contains additional experiments and implementation details.","Latent Gaussian models,Machine learning,Markov chain Monte Carlo sampling,Peskun ordering,Scalability","Titsias, Michalis K.@Athens University of Economics & Business::Papaspiliopoulos, Omiros@Pompeu Fabra University"
Modelling non-stationary multivariate time series of counts via common factors,JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES B-STATISTICAL METHODOLOGY,21,"We develop a new parameter-driven model for multivariate time series of counts. The time series is not necessarily stationary. We model the mean process as the product of modulating factors and unobserved stationary processes. The former characterizes the long-run movement in the data, whereas the latter is responsible for rapid fluctuations and other unknown or unavailable covariates. The unobserved stationary processes evolve independently of the past observed counts and might interact with each other. We express the multivariate unobserved stationary processes as a linear combination of possibly low dimensional factors that govern the contemporaneous and serial correlation within and across the observed counts. Regression coefficients in the modulating factors are estimated via pseudo-maximum-likelihood estimation, and identification of common factor(s) is carried out through eigenanalysis on a positive definite matrix that pertains to the autocovariance of the observed counts at non-zero lags. Theoretical validity of the two-step estimation procedure is documented. In particular, we establish consistency and asymptotic normality of the pseudo-maximum-likelihood estimator in the first step and the convergence rate of the second-step estimator. We also present an exhaustive simulation study to examine the finite sample performance of the estimators, and numerical results corroborate our theoretical findings. Finally, we illustrate the use of the proposed model through an application to the numbers of National Science Foundation fundings awarded to seven research universities from January 2001 to December 2012.","Count time series,Eigendecomposition,Factor model,Generalized auto-regressive conditional heteroscedasticity,Generalized linear models,Pseudolikelihood","Wang, Fangfang@University of Wisconsin System@University of Wisconsin Madison::Wang, Haonan@Colorado State University"
Confidence intervals for causal effects with invalid instruments by using two-stage hard thresholding with voting,JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES B-STATISTICAL METHODOLOGY,54,"A major challenge in instrumental variable (IV) analysis is to find instruments that are valid, or have no direct effect on the outcome and are ignorable. Typically one is unsure whether all of the putative IVs are in fact valid. We propose a general inference procedure in the presence of invalid IVs, called two-stage hard thresholding with voting. The procedure uses two hard thresholding steps to select strong instruments and to generate candidate sets of valid IVs. Voting takes the candidate sets and uses majority and plurality rules to determine the true set of valid IVs. In low dimensions with invalid instruments, our proposal correctly selects valid IVs, consistently estimates the causal effect, produces valid confidence intervals for the causal effect and has oracle optimal width, even if the so-called 50% rule or the majority rule is violated. In high dimensions, we establish nearly identical results without oracle optimality. In simulations, our proposal outperforms traditional and recent methods in the invalid IV literature. We also apply our method to reanalyse the causal effect of education on earnings.","Exclusion restriction,High dimensional covariates,Invalid instruments,Majority voting,Plurality voting,Treatment effect","Guo, Zijian@Rutgers State University New Brunswick::Kang, Hyunseung@University of Wisconsin System@University of Wisconsin Madison::Cai, T. Tony@University of Pennsylvania::Small, Dylan S.@University of Pennsylvania"
Spatially varying auto-regressive models for prediction of new human immunodeficiency virus diagnoses,JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C-APPLIED STATISTICS,42,"In demand of predicting new human immunodeficiency virus (HIV) diagnosis rates based on publicly available HIV data that are abundant in space but have few points in time, we propose a class of spatially varying auto-regressive models compounded with conditional auto-regressive spatial correlation structures. We then propose to use the copula approach and a flexible conditional auto-regressive formulation to model the dependence between adjacent counties. These models allow for spatial and temporal correlation as well as space-time interactions and are naturally suitable for predicting HIV cases and other spatiotemporal disease data that feature a similar data structure. We apply the proposed models to HIV data over Florida, California and New England states and compare them with a range of linear mixed models that have been recently popular for modelling spatiotemporal disease data. The results show that for such data our proposed models outperform the others in terms of prediction.","Bayesian hierarchical models,Conditional auto-regressive models,Copula,Spatiotemporal data","Shand, Lyndsay@University of Illinois Urbana-Champaign@University of Illinois System::Li, Bo@University of Illinois Urbana-Champaign@University of Illinois System::Park, Trevor@University of Illinois Urbana-Champaign@University of Illinois System::Albarracin, Dolores@University of Illinois Urbana-Champaign@University of Illinois System"
"Bounded, efficient and multiply robust estimation of average treatment effects using instrumental variables",JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES B-STATISTICAL METHODOLOGY,65,"Instrumental variables are widely used for estimating causal effects in the presence of unmeasured confounding. Under the standard instrumental variable model, however, the average treatment effect is only partially identifiable. To address this, we propose novel assumptions that enable identification of the average treatment effect. Our identification assumptions are clearly separated from model assumptions that are needed for estimation, so researchers are not required to commit to a specific observed data model in establishing identification. We then construct multiple estimators that are consistent under three different observed data models, and multiply robust estimators that are consistent in the union of these observed data models. We pay special attention to the case of binary outcomes, for which we obtain bounded estimators of the average treatment effect that are guaranteed to lie between -1 and 1. Our approaches are illustrated with simulations and a data analysis evaluating the causal effect of education on earnings.","Binary outcome,Causal inference,Identification,Semiparametric inference,Unmeasured confounding","Wang, Linbo@Harvard T.H. Chan School of Public Health::Tchetgen, Eric Tchetgen@Harvard T.H. Chan School of Public Health"
Testing mutual independence in high dimension via distance covariance,JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES B-STATISTICAL METHODOLOGY,45,"We introduce an L2-type test for testing mutual independence and banded dependence structure for high dimensional data. The test is constructed on the basis of the pairwise distance covariance and it accounts for the non-linear and non-monotone dependences among the data, which cannot be fully captured by the existing tests based on either Pearson correlation or rank correlation. Our test can be conveniently implemented in practice as the limiting null distribution of the test statistic is shown to be standard normal. It exhibits excellent finite sample performance in our simulation studies even when the sample size is small albeit the dimension is high and is shown to identify non-linear dependence in empirical data analysis successfully. On the theory side, asymptotic normality of our test statistic is shown under quite mild moment assumptions and with little restriction on the growth rate of the dimension as a function of sample size. As a demonstration of good power properties for our distance-covariance-based test, we further show that an infeasible version of our test statistic has the rate optimality in the class of Gaussian distributions with equal correlation.","Banded dependence,Degenerate U-statistics,Distance correlation,High dimensionality,Hoeffding decomposition","Yao, Shun@University of Illinois Urbana-Champaign@University of Illinois System::Zhang, Xianyang@Texas A&M University College Station@Texas A&M University System::Shao, Xiaofeng@University of Illinois Urbana-Champaign@University of Illinois System"
"Random networks, graphical models and exchangeability",JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES B-STATISTICAL METHODOLOGY,61,"We study conditional independence relationships for random networks and their interplay with exchangeability. We show that, for finitely exchangeable network models, the empirical subgraph densities are maximum likelihood estimates of their theoretical counterparts. We then characterize all possible Markov structures for finitely exchangeable random graphs, thereby identifying a new class of Markov network models corresponding to bidirected Kneser graphs. In particular, we demonstrate that the fundamental property of dissociatedness corresponds to a Markov property for exchangeable networks described by bidirected line graphs. Finally we study those exchangeable models that are also summarized in the sense that the probability of a network depends only on the degree distribution, and we identify a class of models that is dual to the Markov graphs of Frank and Strauss. Particular emphasis is placed on studying consistency properties of network models under the process of forming subnetworks and we show that the only consistent systems of Markov properties correspond to the empty graph, the bidirected line graph of the complete graph and the complete graph.","Bidirected Markov property,Exchangeable arrays,Exponential random-graph model,de Finetti's theorem,Graph limit,Graphon,Kneser graph,Marginal beta model,Mobius parameterization,Petersen graph","Lauritzen, Steffen@University of Copenhagen::Rinaldo, Alessandro@Carnegie Mellon University::Sadeghi, Kayvan@University of Cambridge"
Detecting and dating structural breaks in functional data without dimension reduction,JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES B-STATISTICAL METHODOLOGY,36,"Methodology is proposed to uncover structural breaks in functional data that is fully functional' in the sense that it does not rely on dimension reduction techniques. A thorough asymptotic theory is developed for a fully functional break detection procedure as well as for a break date estimator, assuming a fixed break size and a shrinking break size. The latter result is utilized to derive confidence intervals for the unknown break date. The main results highlight that the fully functional procedures perform best under conditions when analogous estimators based on functional principal component analysis are at their worst, namely when the feature of interest is orthogonal to the leading principal components of the data. The theoretical findings are confirmed by means of a Monte Carlo simulation study in finite samples. An application to annual temperature curves illustrates the practical relevance of the procedures proposed.","Change point analysis,Functional data,Functional principal components,Functional time series,Structural breaks,Temperature data","Aue, Alexander@University of California Davis@University of California System::Rice, Gregory@University of Waterloo::Sonmez, Ozan@University of California Davis@University of California System"
"Panning for gold: ""model-X' knockoffs for high dimensional controlled variable selection",JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES B-STATISTICAL METHODOLOGY,38,"Many contemporary large-scale applications involve building interpretable models linking a large set of potential covariates to a response in a non-linear fashion, such as when the response is binary. Although this modelling problem has been extensively studied, it remains unclear how to control the fraction of false discoveries effectively even in high dimensional logistic regression, not to mention general high dimensional non-linear models. To address such a practical problem, we propose a new framework of model-X' knockoffs, which reads from a different perspective the knockoff procedure that was originally designed for controlling the false discovery rate in linear models. Whereas the knockoffs procedure is constrained to homoscedastic linear models with np, the key innovation here is that model-X knockoffs provide valid inference from finite samples in settings in which the conditional distribution of the response is arbitrary and completely unknown. Furthermore, this holds no matter the number of covariates. Correct inference in such a broad setting is achieved by constructing knockoff variables probabilistically instead of geometrically. To do this, our approach requires that the covariates are random (independent and identically distributed rows) with a distribution that is known, although we provide preliminary experimental evidence that our procedure is robust to unknown or estimated distributions. To our knowledge, no other procedure solves the controlled variable selection problem in such generality but, in the restricted settings where competitors exist, we demonstrate the superior power of knockoffs through simulations. Finally, we apply our procedure to data from a case-control study of Crohn's disease in the UK, making twice as many discoveries as the original analysis of the same data.","False discovery rate,Generalized linear models,Genomewide association study,Knockoff filter,Logistic regression,Markov blanket,Testing for conditional independence in non-linear models","Candes, Emmanuel@Stanford University::Fan, Yingying@University of Southern California::Janson, Lucas@Stanford University::Lv, Jinchi@University of Southern California"
Semi-supervised approaches to efficient evaluation of model prediction performance,JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES B-STATISTICAL METHODOLOGY,46,"In many modern machine learning applications, the outcome is expensive or time consuming to collect whereas the predictor information is easy to obtain. Semi-supervised (SS) learning aims at utilizing large amounts of unlabelled' data along with small amounts of labelled' data to improve the efficiency of a classical supervised approach. Though numerous SS learning classification and prediction procedures have been proposed in recent years, no methods currently exist to evaluate the prediction performance of a working regression model. In the context of developing phenotyping algorithms derived from electronic medical records, we present an efficient two-step estimation procedure for evaluating a binary classifier based on various prediction performance measures in the SS setting. In step I, the labelled data are used to obtain a non-parametrically calibrated estimate of the conditional risk function. In step II, SS estimates of the prediction accuracy parameters are constructed based on the estimated conditional risk function and the unlabelled data. We demonstrate that, under mild regularity conditions, the estimators proposed are consistent and asymptotically normal. Importantly, the asymptotic variance of the SS estimators is always smaller than that of the supervised counterparts under correct model specification. We also correct for potential overfitting bias in the SS estimators in finite samples with cross-validation and we develop a perturbation resampling procedure to approximate their distributions. Our proposals are evaluated through extensive simulation studies and illustrated with two real electronic medical record studies aiming to develop phenotyping algorithms for rheumatoid arthritis and multiple sclerosis.","Model evaluation,Perturbation resampling,Receiver operating characteristic curve,Risk prediction,Semi-supervised learning","Gronsbell, Jessica L.@Harvard University::Cai, Tianxi@Harvard University"
Testing for marginal linear effects in quantile regression,JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES B-STATISTICAL METHODOLOGY,37,"The paper develops a new marginal testing procedure to detect significant predictors that are associated with the conditional quantiles of a scalar response. The idea is to fit the marginal quantile regression on each predictor one at a time, and then to base the test on the t-statistics that are associated with the most predictive predictors. A resampling method is devised to calibrate this test statistic, which has non-regular limiting behaviour due to the selection of the most predictive variables. Asymptotic validity of the procedure is established in a general quantile regression setting in which the marginal quantile regression models can be misspecified. Even though a fixed dimension is assumed to derive the asymptotic results, the test proposed is applicable and computationally feasible for large dimensional predictors. The method is more flexible than existing marginal screening test methods based on mean regression and has the added advantage of being robust against outliers in the response. The approach is illustrated by using an application to a human immunodeficiency virus drug resistance data set.","Bootstrap calibration,Inference,Marginal regression,Non-standard asymptotics,Quantile regression","Wang, Huixia Judy@George Washington University::McKeague, Ian W.@Columbia University::Qian, Min@Columbia University"
Estimation of tail risk based on extreme expectiles,JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES B-STATISTICAL METHODOLOGY,47,"We use tail expectiles to estimate alternative measures to the value at risk and marginal expected shortfall, which are two instruments of risk protection of utmost importance in actuarial science and statistical finance. The concept of expectiles is a least squares analogue of quantiles. Both are M-quantiles as the minimizers of an asymmetric convex loss function, but expectiles are the only M-quantiles that are coherent risk measures. Moreover, expectiles define the only coherent risk measure that is also elicitable. The estimation of expectiles has not, however, received any attention yet from the perspective of extreme values. Two estimation methods are proposed here, either making use of quantiles or relying directly on least asymmetrically weighted squares. A main tool is first to estimate large values of expectile-based value at risk and marginal expected shortfall within the range of the data, and then to extrapolate the estimates obtained to the very far tails. We establish the limit distributions of both of the resulting intermediate and extreme estimators. We show via a detailed simulation study the good performance of the procedures and present concrete applications to medical insurance data and three large US investment banks.","Asymmetric squared loss,Coherency,Expectiles,Extrapolation,Extreme values,Heavy tails,Marginal expected shortfall,Value at risk","Daouia, Abdelaati@Universite Toulouse 1 Capitole@Universite de Toulouse::Girard, Stephane@Inria@Institut National Polytechnique de Grenoble@Communaute Universite Grenoble Alpes@Centre National de la Recherche Scientifique (CNRS)@Universite Grenoble Alpes (UGA)::Stupfler, Gilles@Aix-Marseille Universite@University of Nottingham"
Boosting in the Presence of Outliers: Adaptive Classification With Nonconvex Loss Functions,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,56,"This article examines the role and the efficiency of nonconvex loss functions for binary classification problems. In particular, we investigate how to design adaptive and effective boosting algorithms that are robust to the presence of outliers in the data or to the presence of errors in the observed data labels. We demonstrate that nonconvex losses play an important role for prediction accuracy because of the diminishing gradient propertiesthe ability of the losses to efficiently adapt to the outlying data. We propose a new boosting framework called ArchBoost that uses diminishing gradient property directly and leads to boosting algorithms that are provably robust. Along with the ArchBoost framework, a family of nonconvex losses is proposed, which leads to the new robust boosting algorithms, named adaptive robust boosting (ARB). Furthermore, we develop a new breakdown point analysis and a new influence function analysis that demonstrate gains in robustness. Moreover, based only on local curvatures, we establish statistical and optimization properties of the proposed ArchBoost algorithms with highly nonconvex losses. Extensive numerical and real data examples illustrate theoretical properties and reveal advantages over the existing boosting methods when data are perturbed by an adversary or otherwise.","Boosting,Breakdown point,Classification,Influence function,Nonconvexity,Robustness","Li, Alexander Hanbo@University of California San Diego@University of California System::Bradic, Jelena@University of California San Diego@University of California System"
Bayesian Nonparametric Calibration and Combination of Predictive Distributions,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,60,"We introduce a Bayesian approach to predictive density calibration and combination that accounts for parameter uncertainty and model set incompleteness through the use of random calibration functionals and random combination weights. Building on the work of Ranjan and Gneiting, we use infinite beta mixtures for the calibration. The proposed Bayesian nonparametric approach takes advantage of the flexibility of Dirichlet process mixtures to achieve any continuous deformation of linearly combined predictive distributions. The inference procedure is based on combination Gibbs and slice sampling. We provide some conditions under which the proposed probabilistic calibration converges in terms of weak posterior consistency to the true underlying density for both cases of iid and Markovian observations. This calibration property improves upon the earlier calibration approaches. We study the methodology in simulation examples with fat tails and multimodal densities and apply it to density forecasts of daily S&P returns and daily maximum wind speed at the Frankfurt airport.","Bayesian nonparametrics,Density forecast,Forecast calibration,Forecast combination,Posterior consistency","Bassetti, Federico@University of Pavia::Casarin, Roberto@Universita Ca Foscari Venezia::Ravazzolo, Francesco@Free University of Bozen-Bolzano"
Design-Based Maps for Finite Populations of Spatial Units,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,24,"The estimation of the values of a survey variable in finite populations of spatial units is considered for making maps when samples of spatial units are selected by probabilistic sampling schemes. The single values are estimated by means of an inverse distance weighting predictor. The design-based asymptotic properties of the resulting maps, referred to as the design-based maps, are considered when the study area remains fixed and the sizes of the spatial units tend to zero. Conditions ensuring design-based asymptotic unbiasedness and consistency are derived. They essentially require the existence of a pointwise or uniformly continuous density function of the survey variable onto the study area, some regularities in the size and shape of the units, and the use of spatially balanced designs to select units. The continuity assumption can be relaxed into a Riemann integrability assumption when estimation is performed at a sufficiently small spatial grain and the estimates are subsequently aggregated at a greater grain. A computationally simple mean squared error estimator is proposed. A simulation study is performed to assess the theoretical results. An application to estimate the map of wine cultivations in Tuscany (Central Italy) is considered.","Design consistency,Monte Carlo study,Spatially explicit estimation,Spatial interpolation,Spatial sampling","Fattorini, L.@University of Siena::Marcheselli, M.@University of Siena::Pratelli, L.@Unknow"
Group-Linear Empirical Bayes Estimates for a Heteroscedastic Normal Mean,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,20,"The problem of estimating the mean of a normal vector with known but unequal variances introduces substantial difficulties that impair the adequacy of traditional empirical Bayes estimators. By taking a different approach that treats the known variances as part of the random observations, we restore symmetry and thus the effectiveness of such methods. We suggest a group-linear empirical Bayes estimator, which collects observations with similar variances and applies a spherically symmetric estimator to each group separately. The proposed estimator is motivated by a new oracle rule which is stronger than the best linear rule, and thus provides a more ambitious benchmark than that considered in the previous literature. Our estimator asymptotically achieves the new oracle risk (under appropriate conditions) and at the same time is minimax. The group-linear estimator is particularly advantageous in situations where the true means and observed variances are empirically dependent. To demonstrate the merits of the proposed methods in real applications, we analyze the baseball data used by Brown (2008), where the group-linear methods achieved the prediction error of the best nonparametric estimates that have been applied to the dataset, and significantly lower error than other parametric and semiparametric empirical Bayes estimators.","Asymptotic optimality,Compound decision,Empirical Bayes,Heteroscedasticity,Shrinkage estimator","Weinstein, Asaf@Stanford University::Ma, Zhuang@University of Pennsylvania::Brown, Lawrence D.@University of Pennsylvania::Zhang, Cun-Hui@Rutgers State University New Brunswick"
Equivalence of Regression Curves,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,34,"This article investigates the problem whether the difference between two parametric models m(1), m(2) describing the relation between a response variable and several covariates in two different groups is practically irrelevant, such that inference can be performed on the basis of the pooled sample. Statistical methodology is developed to test the hypotheses H-0: d(m(1), m(2)) >= epsilon versus H-1: d(m(1), m(2)) <= epsilon to demonstrate equivalence between the two regression curves m(1), m(2) for a prespecified threshold E, where d denotes a distance measuring the distance between m(1) and m(2). Our approach is based on the asymptotic properties of a suitable estimator d (m(1)) over cap, (m(2)) over cap of this distance. To improve the approximation of the nominal level for small sample sizes, a bootstrap test is developed, which addresses the specific form of the interval hypotheses. In particular, data have to be generated under the null hypothesis, which implicitly defines a manifold for the parameter vector. The results are illustrated by means of a simulation study and a data example. It is demonstrated that the new methods substantially improve currently available approaches with respect to power and approximation of the nominal level.","Constrained parameter estimation,Equivalence of curves,Keywords and phrases: Dose-response studies,Nonlinear regression,Parametric bootstrap","Dette, Holger@Ruhr University Bochum::Moellenhoff, Kathrin@Ruhr University Bochum::Volgushev, Stanislav@University Toronto Mississauga@University of Toronto::Bretz, Frank@Shanghai University of Finance & Economics@Novartis"
On Reject and Refine Options in Multicategory Classification,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,58,"In many real applications of statistical learning, a decision made from misclassification can be too costly to afford; in this case, a reject option, which defers the decision until further investigation is conducted, is often preferred. In recent years, there has been much development for binary classification with a reject option. Yet, little progress has been made for the multicategory case. In this article, we propose margin-based multicategory classification methods with a reject option. In addition, and more importantly, we introduce a new and unique refine option for the multicategory problem, where the class of an observation is predicted to be from a set of class labels, whose cardinality is not necessarily one. The main advantage of both options lies in their capacity of identifying error-prone observations. Moreover, the refine option can provide more constructive information for classification by effectively ruling out implausible classes. Efficient implementations have been developed for the proposed methods. On the theoretical side, we offer a novel statistical learning theory and show a fast convergence rate of the excess-risk of our methods with emphasis on diverging dimensionality and number of classes. The results can be further improved under a low noise assumption and be generalized to the excess 0-d-1 risk. Finite-sample upper bounds for the reject and reject/refine rates are also provided. A set of comprehensive simulation and real data studies has shown the usefulness of the new learning tools compared to regular multicategory classifiers.","Coordinate descent,Discriminant analysis,Diverging number of classes,High-dimensional data,Multi-class classification,Statistical learning theory","Zhang, Chong@University of Waterloo::Wang, Wenbo@State University of New York (SUNY) Binghamton@State University of New York (SUNY) System::Qiao, Xingye@State University of New York (SUNY) Binghamton@State University of New York (SUNY) System"
Dimensionality Reduction and Variable Selection in Multivariate Varying-Coefficient Models With a Large Number of Covariates,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,37,"Motivated by the study of gene and environment interactions, we consider a multivariate response varying-coefficient model with a large number of covariates. The need of nonparametrically estimating a large number of coefficient functions given relatively limited data poses a big challenge for fitting such a model. To overcome the challenge, we develop a method that incorporates three ideas: (i) reduce the number of unknown functions to be estimated by using (noncentered) principal components; (ii) approximate the unknown functions by polynomial splines; (iii) apply sparsity-inducing penalization to select relevant covariates. The three ideas are integrated into a penalized least-square framework. Our asymptotic theory shows that the proposed method can consistently identify relevant covariates and can estimate the corresponding coefficient functions with the same convergence rate as when only the relevant variables are included in the model. We also develop a novel computational algorithm to solve the penalized least-square problem by combining proximal algorithms and optimization over Stiefel manifolds. Our method is illustrated using data from Framingham Heart Study.","Multivariate regression,Oracle property,Polynomial splines","He, Kejun@Renmin University of China::Lian, Heng@City University of Hong Kong::Ma, Shujie@University of California Riverside@University of California System::Huang, Jianhua Z.@Texas A&M University College Station@Texas A&M University System"
On the Effect of Bias Estimation on Coverage Accuracy in Nonparametric Inference,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,30,"Nonparametric methods play a central role in modern empirical work. While they provide inference procedures that are more robust to parametric misspecification bias, they may be quite sensitive to tuning parameter choices. We study the effects of bias correction on confidence interval coverage in the context of kernel density and local polynomial regression estimation, and prove that bias correction can be preferred to undersmoothing for minimizing coverage error and increasing robustness to tuning parameter choice. This is achieved using a novel, yet simple, Studentization, which leads to a new way of constructing kernel-based bias-corrected confidence intervals. In addition, for practical cases, we derive coverage error optimal bandwidths and discuss easy-to-implement bandwidth selectors. For interior points, we show that the mean-squared error (MSE)-optimal bandwidth for the original point estimator (before bias correction) delivers the fastest coverage error decay rate after bias correction when second-order (equivalent) kernels are employed, but is otherwise suboptimal because it is too ""large."" Finally, for odd-degree local polynomial regression, we show that, as with point estimation, coverage error adapts to boundary points automatically when appropriate Studentization is used; however, the MSE-optimal bandwidth for the original point estimator is suboptimal. All the results are established using valid Edgeworth expansions and illustrated with simulated data. Our findings have important consequences for empirical work as they indicate that bias-corrected confidence intervals, coupled with appropriate standard errors, have smaller coverage error and are less sensitive to tuning parameter choices in practically relevant cases where additional smoothness is available.","Coverage error,Edgeworth expansion,Kernel methods,Local polynomial regression","Calonico, Sebastian@University of Miami::Cattaneo, Matias D.@University of Michigan System@University of Michigan::Farrell, Max H.@University of Chicago"
Parametric-Rate Inference for One-Sided Differentiable Parameters,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,13,"Suppose one has a collection of parameters indexed by a (possibly infinite dimensional) set. Given data generated from some distribution, the objective is to estimate the maximal parameter in this collection evaluated at the distribution that generated the data. This estimation problem is typically nonregular when the maximizing parameter is nonunique, and as a result standard asymptotic techniques generally fail in this case. We present a technique for developing parametric-rate confidence intervals for the quantity of interest in these nonregular settings. We show that our estimator is asymptotically efficient when the maximizing parameter is unique so that regular estimation is possible. We apply our technique to a recent example from the literature in which one wishes to report the maximal absolute correlation between a prespecified outcome and one of p predictors. The simplicity of our technique enables an analysis of the previously open case where p grows with sample size. Specifically, we only require that logp grows slower than root n, where n is the sample size. We show that, unlike earlier approaches, our method scales to massive datasets: the point estimate and confidence intervals can be constructed in O(np) time.","Nonregular inference,Stabilized one-step estimator,Variable screening","Luedtke, Alexander R.@University of California Berkeley@University of California System::van der Laan, Mark J.@University of California Berkeley@University of California System"
Distribution-Free Detection of Structured Anomalies: Permutation and Rank-Based Scans,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,38,"The scan statistic is by far the most popular method for anomaly detection, being popular in syndromic surveillance, signal and image processing, and target detection based on sensor networks, among other applications. The use of the scan statistics in such settings yields a hypothesis testing procedure, where the null hypothesis corresponds to the absence of anomalous behavior. If the null distribution is known, then calibration of a scan-based test is relatively easy, as it can be done by Monte Carlo simulation. When the null distribution is unknown, it is less straightforward. We investigate two procedures. The first one is a calibration by permutation and the other is a rank-based scan test, which is distribution-free and less sensitive to outliers. Furthermore, the rank scan test requires only a one-time calibration for a given data size making it computationally much more appealing. In both cases, we quantify the performance loss with respect to an oracle scan test that knows the null distribution. We show that using one of these calibration procedures results in only a very small loss of power in the context of a natural exponential family. This includes the classical normal location model, popular in signal processing, and the Poisson model, popular in syndromic surveillance. We perform numerical experiments on simulated data further supporting our theory and also on a real dataset from genomics. Supplementary materials for this article are available online.","Permutation tests,Rank tests,Scan statistic","Arias-Castro, Ery@University of California San Diego@University of California System::Castro, Rui M.@Eindhoven University of Technology::Tanczos, Ervin@Unknow::Wang, Meng@Stanford University"
Efficient Functional ANOVA Through Wavelet-Domain Markov Groves,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,36,"We introduce a wavelet-domain method for functional analysis of variance (fANOVA). It is based on a Bayesian hierarchical model that employs a graphical hyperprior in the form of a Markov grove (MG)-that is, a collection of Markov trees-for linking the presence/absence of factor effects at all location-scale combinations, there-by incorporating the natural clustering of factor effects in the wavelet-domain across locations and scales. Inference under the model enjoys both analytical simplicity and computational efficiency. Specifically, the posterior of the full hierarchical model is available in closed form through a pyramid algorithm operationally similar to Mallat's pyramid algorithm for discrete wavelet transform (DWT), achieving for exact Bayesian inference the same computational efficiency-linear in both the number of observations and the number of locations-as for carrying out the DWT. In particular, posterior probabilities of the presence of factor contributions to functional variation are directly available from the pyramid algorithm, while posterior samples for the factor effects can be drawn directly from the exact posterior through standard (not Markov chain) Monte Carlo. We investigate the performance of our method through extensive simulation and show that it substantially outperforms existing wavelet-domain fANOVA methods in a variety of common settings. We illustrate the method through analyzing the orthosis data. Supplementary materials for this article are available online.","Bayesian inference,Functional data analysis,Graphical models,Hierarchical models,Multi-scale inference,Scalable inference","Ma, Li@Duke University::Soriano, Jacopo@Google Incorporated"
Invariant Inference and Efficient Computation in the Static Factor Model,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,35,"Factor models are used in a wide range of areas. Two issues with Bayesian versions of these models are a lack of invariance to ordering of and scaling of the variables and computational inefficiency. This article develops invariant and efficient Bayesian methods for estimating static factor models. This approach leads to inference that does not depend upon the ordering or scaling of the variables, and we provide arguments to explain this invariance. Beginning from identified parameters which are subject to orthogonality restrictions, we use parameter expansions to obtain a specification with computationally convenient conditional posteriors. We show significant gains in computational efficiency. Identifying restrictions that are commonly employed result in interpretable factors or loadings and, using our approach, these can be imposed ex-post. This allows us to investigate several alternative identifying (noninvariant) schemes without the need to respecify and resample the model. We illustrate the methods with two macroeconomic datasets.","Bayesian,Markov chain Monte Carlo,Reduced rank regression","Chan, Joshua@University of Technology Sydney::Leon-Gonzalez, Roberto@Unknow::Strachan, Rodney W.@University of Queensland"
Optimal Subsampling for Large Sample Logistic Regression,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,28,"For massive data, the family of subsampling algorithms is popular to downsize the data volume and reduce computational burden. Existing studies focus on approximating the ordinary least-square estimate in linear regression, where statistical leverage scores are often used to define subsampling probabilities. In this article, we propose fast subsampling algorithms to efficiently approximate the maximum likelihood estimate in logistic regression. We first establish consistency and asymptotic normality of the estimator from a general subsampling algorithm, and then derive optimal subsampling probabilities that minimize the asymptotic mean squared error of the resultant estimator. An alternative minimization criterion is also proposed to further reduce the computational cost. The optimal subsampling probabilities depend on the full data estimate, so we develop a two-step algorithm to approximate the optimal subsampling procedure. This algorithm is computationally efficient and has a significant reduction in computing time compared to the full data approach. Consistency and asymptotic normality of the estimator from a two-step algorithm are also established. Synthetic and real datasets are used to evaluate the practical performance of the proposed method. Supplementary materials for this article are available online.","A-optimality,Logistic regression,Massive data,Optimal subsampling,Rare event","Wang, HaiYing@University of New Hampshire@University of Connecticut@University System Of New Hampshire::Zhu, Rong@Chinese Academy of Sciences@Academy of Mathematics & System Sciences, CAS::Ma, Ping@University System of Georgia@University of Georgia"
The Bouncy Particle Sampler: A Nonreversible Rejection-Free Markov Chain Monte Carlo Method,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,31,"Many Markov chain Monte Carlo techniques currently available rely on discrete-time reversible Markov processes whose transition kernels are variations of the Metropolis-Hastings algorithm. We explore and generalize an alternative scheme recently introduced in the physics literature (Peters and de With 2012) where the target distribution is explored using a continuous-time nonreversible piecewise-deterministic Markov process. In the Metropolis-Hastings algorithm, a trial move to a region of lower target density, equivalently of higher ""energy,"" than the current state can be rejected with positive probability. In this alternative approach, a particle moves along straight lines around the space and, when facing a high energy barrier, it is not rejected but its path is modified by bouncing against this barrier. By reformulating this algorithm using inhomogeneous Poisson processes, we exploit standard sampling techniques to simulate exactly this Markov process in a wide range of scenarios of interest. Additionally, when the target distribution is given by a product of factors dependent only on subsets of the state variables, such as the posterior distribution associated with a probabilistic graphical model, this method can be modified to take advantage of this structure by allowing computationally cheaper ""local"" bounces, which only involve the state variables associated with a factor, while the other state variables keep on evolving. In this context, by leveraging techniques from chemical kinetics, we propose several computationally efficient implementations. Experimentally, this new class of Markov chain Monte Carlo schemes compares favorably to state-of-the-art methods on various Bayesian inference tasks, including for high-dimensional models and large datasets. Supplementary materials for this article are available online.","Inhomogeneous Poisson process,Markov chain Monte Carlo,Piecewise deterministic Markov process,Probabilistic graphical models,Rejection-free simulation","Bouchard-Cote, Alexandre@University of British Columbia::Vollmer, Sebastian J.@University of Warwick::Doucet, Arnaud@University of Oxford"
Using Standard Tools From Finite Population Sampling to Improve Causal Inference for Complex Experiments,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,50,"This article considers causal inference for treatment contrasts from a randomized experiment using potential outcomes in a finite population setting. Adopting a Neymanian repeated sampling approach that integrates such causal inference with finite population survey sampling, an inferential framework is developed for general mechanisms of assigning experimental units to multiple treatments. This framework extends classical methods by allowing the possibility of randomization restrictions and unequal replications. Novel conditions that are ""milder"" than strict additivity of treatment effects, yet permit unbiased estimation of the finite population sampling variance of any treatment contrast estimator, are derived. The consequences of departures from such conditions are also studied under the criterion of minimax bias, and a new justification for using the Neymanian conservative sampling variance estimator in experiments is provided. The proposed approach can readily be extended to the case of treatments with a general factorial structure.","Assignment probabilities,Linear unbiased estimator,Potential outcomes,Split-plot design,Stratified assignment,Treatment contrasts","Mukerjee, Rahul@Indian Institute of Management Calcutta::Dasgupta, Tirthankar@Harvard University::Rubin, Donald B.@Harvard University"
Inference in Linear Regression Models with Many Covariates and Heteroscedasticity,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,42,"The linear regression model is widely used in empirical work in economics, statistics, and many other disciplines. Researchers often include many covariates in their linear model specification in an attempt to control for confounders. We give inference methods that allow for many covariates and heteroscedasticity. Our results are obtained using high-dimensional approximations, where the number of included covariates is allowed to grow as fast as the sample size. We find that all of the usual versions of Eicker-White heteroscedasticity consistent standard error estimators for linear models are inconsistent under this asymptotics. We then propose a new heteroscedasticity consistent standard error formula that is fully automatic and robust to both (conditional) heteroscedasticity of unknown form and the inclusion of possibly many covariates. We apply our findings to three settings: parametric linear models with many covariates, linear panel models with many fixed effects, and semiparametric semi-linear models with many technical regressors. Simulation evidence consistent with our theoretical results is provided, and the proposed methods are also illustrated with an empirical application. Supplementary materials for this article are available online.","Heteroscedasticity,High-dimensional models,Linear regression,Many regressors,Standard errors","Cattaneo, Matias D.@University of Michigan System@University of Michigan::Jansson, Michael@University of California Berkeley@University of California System@Aarhus University::Newey, Whitney K.@Massachusetts Institute of Technology (MIT)"
Bayesian Inference in the Presence of Intractable Normalizing Functions,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,61,"Models with intractable normalizing functions arise frequently in statistics. Common examples of such models include exponential random graph models for social networks and Markov point processes for ecology and disease modeling. Inference for these models is complicated because the normalizing functions of their probability distributions include the parameters of interest. In Bayesian analysis, they result in so-called doubly intractable posterior distributions which pose significant computational challenges. Several Monte Carlo methods have emerged in recent years to address Bayesian inference for such models. We provide a framework for understanding the algorithms, and elucidate connections among them. Through multiple simulated and real data examples, we compare and contrast the computational and statistical efficiency of these algorithms and discuss their theoretical bases. Our study provides practical recommendations for practitioners along with directions for future research for Markov chain Monte Carlo (MCMC) methodologists. Supplementary materials for this article are available online.","Doubly intractable distributions,Exponential random graph models,Importance sampling,Markov chain Monte Carlo,Markov point processes","Park, Jaewoo@Penn State University@Pennsylvania Commonwealth System of Higher Education (PCSHE)::Haran, Murali@Penn State University@Pennsylvania Commonwealth System of Higher Education (PCSHE)"
Modeling Heterogeneity in Healthcare Utilization Using Massive Medical Claims Data,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,44,"We introduce a modeling approach for characterizing heterogeneity in healthcare utilization using massive medical claims data. We first translate the medical claims observed for a large study population and across five years into individual-level discrete events of care called utilization sequences. We model the utilization sequences using an exponential proportional hazards mixture model to capture heterogeneous behaviors in patients' healthcare utilization. The objective is to cluster patients according to their longitudinal utilization behaviors and to determine the main drivers of variation in healthcare utilization while controlling for the demographic, geographic, and health characteristics of the patients. Due to the computational infeasibility of fitting a parametric proportional hazards model for high-dimensional, large-sample size data we use an iterative one-step procedure to estimate the model parameters and impute the cluster membership. The approach is used to draw inferences on utilization behaviors of children in the Medicaid system with persistent asthma across six states. We conclude with policy implications for targeted interventions to improve adherence to recommended care practices for pediatric asthma. Supplementary materials for this article are available online.","Healthcare utilization,Latent variable model,Medicaid system,Pediatric asthma,Proportional hazards model,Survival analysis","Hilton, Ross P.@University System of Georgia@Georgia Institute of Technology::Zheng, Yuchen@University System of Georgia@Georgia Institute of Technology::Serban, Nicoleta@University System of Georgia@Georgia Institute of Technology"
Residuals and Diagnostics for Ordinal Regression Models: A Surrogate Approach,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,17,"Ordinal outcomes are common in scientific research and everyday practice, and we often rely on regression models to make inference. A long-standing problem with such regression analyses is the lack of effective diagnostic tools for validating model assumptions. The difficulty arises from the fact that an ordinal variable has discrete values that are labeled with, but not, numerical values. The values merely represent ordered categories. In this article, we propose a surrogate approach to defining residuals for an ordinal outcome Y. The idea is to define a continuous variable S as a ""surrogate"" of Y and then obtain residuals based on S. For the general class of cumulative link regression models, we study the residual's theoretical and graphical properties. We show that the residual has null properties similar to those of the common residuals for continuous outcomes. Our numerical studies demonstrate that the residual has power to detect misspecification with respect to (1) mean structures; (2) link functions; (3) heteroscedasticity; (4) proportionality; and (5) mixed populations. The proposed residual also enables us to develop numeric measures for goodness of fit using classical distance notions. Our results suggest that compared to a previously defined residual, our residual can reveal deeper insights into model diagnostics. We stress that this work focuses on residual analysis, rather than hypothesis testing. The latter has limited utility as it only provides a single p-value, whereas our residual can reveal what components of the model are misspecified and advise how to make improvements. Supplementary materials for this article are available online.","Goodness of fit,Logistic odds model,Model diagnostics,Probit model","Liu, Dungang@University of Cincinnati::Zhang, Heping@Yale University"
A Tailored Multivariate Mixture Model for Detecting Proteins of Concordant Change Among Virulent Strains of Clostridium Perfringens,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,50,"Necrotic enteritis (NE) is a serious disease of poultry caused by the bacterium C. perfringens. To identify proteins of C. perfringens that confer virulence with respect to NE, the protein secretions of four NE disease-producing strains and one baseline nondisease-producing strain of C. perfringens were examined. The problem then becomes a clustering task, for the identification of two extreme groups of proteins that were produced at either concordantly higher or concordantly lower levels across all four disease-producing strains compared to the baseline, when most of the proteins do not exhibit significant change across all strains. However, the existence of some nuisance proteins of discordant change may severely distort any biologically meaningful cluster pattern. We develop a tailored multivariate clustering approach to robustly identify the proteins of concordant change. Using a three-component normal mixture model as the skeleton, our approach incorporates several constraints to account for biological expectations and data characteristics. More importantly, we adopt a sparse mean-shift parameterization in the reference distribution, coupled with a regularized estimation approach, to flexibly accommodate proteins of discordant change. We explore the connections and differences between our approach and other robust clustering methods, and resolve the issue of unbounded likelihood under an eigenvalue-ratio condition. Simulation studies demonstrate the superior performance of our method compared with a number of alternative approaches. Our protein analysis along with further biological investigations may shed light on the discovery of the complete set of virulence factors in NE.","Clustering,Multivariate mixture model,Penalized estimation,Proteomics,Robust estimation","Chen, Kun@University of Connecticut::Mishra, Neha@University of Connecticut::Smyth, Joan@University of Connecticut::Bar, Haim@University of Connecticut::Schifano, Elizabeth@University of Connecticut::Kuo, Lynn@University of Connecticut::Chen, Ming-Hui@University of Connecticut"
Nonparametric Adjustment for Measurement Error in Time-to-Event Data: Application to Risk Prediction Models,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,27,"Mismeasured time-to-event data used as a predictor in risk prediction models will lead to inaccurate predictions. This arises in the context of self-reported family history, a time-to-event predictor often measured with error, used in Mendelian risk prediction models. Using validation data, we propose a method to adjust for this type of error. We estimate the measurement error process using a nonparametric smoothed Kaplan-Meier estimator, and use Monte Carlo integration to implement the adjustment. We apply our method to simulated data in the context of both Mendelian and multivariate survival prediction models. Simulations are evaluated using measures of mean squared error of prediction (MSEP), area under the response operating characteristics curve (ROC-AUC), and the ratio of observed to expected number of events. These results show that our method mitigates the effects of measurement error mainly by improving calibration and total accuracy. We illustrate our method in the context of Mendelian risk prediction models focusing on misreporting of breast cancer, fitting the measurement error model on data from the University of California at Irvine, and applying our method to counselees from the Cancer Genetics Network. We show that our method improves overall calibration, especially in low risk deciles. Supplementary materials for this article are available online.","Carrier status prediction,Family history,Mismeasured covariates,Smoothed Kaplan-Meier estimator,Survival analysis","Braun, Danielle@Harvard T.H. Chan School of Public Health@VA Boston Healthcare System@Harvard University@Dana-Farber Cancer Institute::Gorfine, Malka@Technion Israel Institute of Technology@Tel Aviv University::Katki, Hormuzd A.@NIH National Cancer Institute (NCI)@National Institutes of Health (NIH) - USA::Ziogas, Argyrios@University of California System@University of California Irvine::Parmigiani, Giovanni@Harvard T.H. Chan School of Public Health@VA Boston Healthcare System@Harvard University@Dana-Farber Cancer Institute"
Statistics: Essential Now More Than Ever,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,15,"Each year, the Journal of the American Statistical Association publishes the presidential address from the Joint Statistical Meetings. Here, we present the 2017 address verbatim save for the addition of references and a few minor editorial corrections.","Asian initiative,Big data,Collaborative analyses,House of statistics,Quality,Statistical communication","Nussbaum, Barry D.@Unknow"
A Permutation Test for the Regression Kink Design,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,39,"The regression kink (RK) design is an increasingly popular empirical method for estimating causal effects of policies, such as the effect of unemployment benefits on unemployment duration. Using simulation studies based on data from existing RK designs, we empirically document that the statistical significance of RK estimators based on conventional standard errors can be spurious. In the simulations, false positives arise as a consequence of nonlinearities in the underlying relationship between the outcome and the assignment variable, confirming concerns about the misspecification bias of discontinuity estimators pointed out by Calonico, Cattaneo, and Titiunik. As a complement to standard RK inference, we propose that researchers construct a distribution of placebo estimates in regions with and without a policy kink and use this distribution to gauge statistical significance. Under the assumption that the location of the kink point is random, this permutation test has exact size in finite samples for testing a sharp null hypothesis of no effect of the policy on the outcome. We implement simulation studies based on existing RK applications that estimate the effect of unemployment benefits on unemployment duration and show that our permutation test as well as inference procedures proposed by Calonico, Cattaneo, and Titiunik improve upon the size of standard approaches, while having sufficient power to detect an effect of unemployment benefits on unemployment duration.","Permutation test,Policy discontinuities,Randomization inference","Ganong, Peter@University of Chicago@National Bureau of Economic Research::Jaeger, Simon@Massachusetts Institute of Technology (MIT)@IZA Institute Labor Economics@National Bureau of Economic Research@Ifo Institut"
Estimating the Number of Sources in Magnetoencephalography Using Spiked Population Eigenvalues,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,26,"Magnetoencephalography (MEG) is an advanced imaging technique used to measure the magnetic fields outside the human head produced by the electrical activity inside the brain. Various source localization methods in MEG require the knowledge of the underlying active sources, which are identified by a priori. Common methods used to estimate the number of sources include principal component analysis or information criterion methods, both of which make use of the eigenvalue distribution of the data, thus avoiding solving the time-consuming inverse problem. Unfortunately, all these methods are very sensitive to the signal-to-noise ratio (SNR), as examining the sample extreme eigenvalues does not necessarily reflect the perturbation of the population ones. To uncover the unknown sources from the very noisy MEG data, we introduce a framework, referred to as the intrinsic dimensionality (ID) of the optimal transformation for the SNR rescaling functional. It is defined as the number of the spiked population eigenvalues of the associated transformed data matrix. It is shown that the ID yields a more reasonable estimate for the number of sources than its sample counterparts, especially when the SNR is small. By means of examples, we illustrate that the new method is able to capture the number of signal sources in MEG that can escape PCA or other information criterion-based methods.","Brain imaging,Eigenthresholding,Intrinsic dimensionality,Inverse MEG problem,Spiked eigenvalues","Yao, Zhigang@National University of Singapore::Zhang, Ye@Orebro University::Bai, Zhidong@Northeast Normal University - China::Eddy, William F.@Carnegie Mellon University"
"A Unified Framework for Fitting Bayesian Semiparametric Models to Arbitrarily Censored Survival Data, Including Spatially Referenced Data",JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,75,"A comprehensive, unified approach to modeling arbitrarily censored spatial survival data is presented for the three most commonly used semiparametric models: proportional hazards, proportional odds, and accelerated failure time. Unlike many other approaches, all manner of censored survival times are simultaneously accommodated including uncensored, interval censored, current-status, left and right censored, and mixtures of these. Left-truncated data are also accommodated leading to models for time-dependent covariates. Both georeferenced (location exactly observed) and areally observed (location known up to a geographic unit such as a county) spatial locations are handled; formal variable selection makes model selection especially easy. Model fit is assessed with conditional Cox-Snell residual plots, and model choice is carried out via log pseudo marginal likelihood (LPML) and deviance information criterion (DIC). Baseline survival is modeled with a novel transformed Bernstein polynomial prior. All models are fit via a new function which calls efficient compiled C++ in the R package spBayesSurv. The methodology is broadly illustrated with simulations and real data applications. An important finding is that proportional odds and accelerated failure time models often fit significantly better than the commonly used proportional hazards model.","Bernstein polynomial,Interval censored data,Spatial frailty models,Variable selection","Zhou, Haiming@Northern Illinois University::Hanson, Timothy@Medtronic@University of South Carolina System@University of South Carolina Columbia"
Nested Hierarchical Functional Data Modeling and Inference for the Analysis of Functional Plant Phenotypes,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,28,"In a plant science Root Image Study, the process of seedling roots bending in response to gravity is recorded using digital cameras, and the bending rates are modeled as functional plant phenotype data. The functional phenotypes are collected from seeds representing a large variety of genotypes and have a three-level nested hierarchical structure, with seeds nested in groups nested in genotypes. The seeds are imaged on different days of the lunar cycle, and an important scientific question is whether there are lunar effects on root bending. We allow the mean function of the bending rate to depend on the lunar day and model the phenotypic variation between genotypes, groups of seeds imaged together, and individual seeds by hierarchical functional random effects. We estimate the covariance functions of the functional random effects by a fast penalized tensor product spline approach, perform multi-level functional principal component analysis (FPCA) using the best linear unbiased predictor of the principal component scores, and improve the efficiency of mean estimation by iterative decorrelation. We choose the number of principal components using a conditional Akaike information criterion and test the lunar day effect using generalized likelihood ratio test statistics based on the marginal and conditional likelihoods. We also propose a permutation procedure to evaluate the null distribution of the test statistics. Our simulation studies show that our model selection criterion selects the correct number of principal components with remarkably high frequency, and the likelihood-based tests based on FPCA have higher power than a test based on working independence.","Akaike information criterion,Functional data analysis,Generalized likelihood ratio test,Penalized splines,Permutation test,Principal components","Xu, Yuhang@University of Nebraska System@University of Nebraska Lincoln::Li, Yehua@University of California Riverside@University of California System::Nettleton, Dan@Iowa State University"
Disentangling Bias and Variance in Election Polls,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,16,"It is well known among researchers and practitioners that election polls suffer from a variety of sampling and nonsampling errors, often collectively referred to as total survey error. Reported margins of error typically only capture sampling variability, and in particular, generally ignore nonsampling errors in defining the target population (e.g., errors due to uncertainty in who will vote). Here, we empirically analyze 4221 polls for 608 state-level presidential, senatorial, and gubernatorial elections between 1998 and 2014, all of which were conducted during the final three weeks of the campaigns. Comparing to the actual election outcomes, we find that average survey error as measured by root mean square error is approximately 3.5 percentage points, about twice as large as that implied by most reported margins of error. We decompose survey error into election-level bias and variance terms. We find that average absolute election-level bias is about 2 percentage points, indicating that polls for a given election often share a common component of error. This shared error may stem from the fact that polling organizations often face similar difficulties in reaching various subgroups of the population, and that they rely on similar screening rules when estimating who will vote. We also find that average election-level variance is higher than implied by simple random sampling, in part because polling organizations often use complex sampling designs and adjustment procedures. We conclude by discussing how these results help explain polling failures in the 2016 U.S. presidential election, and offer recommendations to improve polling practice.","Margin of error,Nonsampling error,Polling bias,Total survey error","Shirani-Mehr, Houshmand@Stanford University::Rothschild, David@Microsoft::Goel, Sharad@Stanford University::Gelman, Andrew@Columbia University"
Model Selection for High-Dimensional Quadratic Regression via Regularization,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,38,"Quadratic regression (QR) models naturally extend linear models by considering interaction effects between the covariates. To conduct model selection in QR, it is important to maintain the hierarchical model structure between main effects and interaction effects. Existing regularization methods generally achieve this goal by solving complex optimization problems, which usually demands high computational cost and hence are not feasible for high-dimensional data. This article focuses on scalable regularization methods for model selection in high-dimensional QR. We first consider two-stage regularization methods and establish theoretical properties of the two-stage LASSO. Then, a new regularization method, called regularization algorithm under marginality principle (RAMP), is proposed to compute a hierarchy-preserving regularization solution path efficiently. Both methods are further extended to solve generalized QR models. Numerical results are also shown to demonstrate performance of the methods.","Generalized quadratic regression,Interaction selection,LASSO,Marginality principle,Variable selection","Hao, Ning@University of Arizona::Feng, Yang@Columbia University::Zhang, Hao Helen@University of Arizona"
Unsupervised Self-Normalized Change-Point Testing for Time Series,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,32,"We propose a new self-normalized method for testing change points in the time series setting. Self-normalization has been celebrated for its ability to avoid direct estimation of the nuisance asymptotic variance and its flexibility of being generalized to handle quantities other than the mean. However, it was developed and mainly studied for constructing confidence intervals for quantities associated with a stationary time series, and its adaptation to change-point testing can be nontrivial as direct implementation can lead to tests with nonmonotonic power. Compared with existing results on using self-normalization in this direction, the current article proposes a new self-normalized change-point test that does not require prespecifying the number of total change points and is thus unsupervised. In addition, we propose a new contrast-based approach in generalizing self-normalized statistics to handle quantities other than the mean, which is specifically tailored for change-point testing. Monte Carlo simulations are presented to illustrate the finite-sample performance of the proposed method.","Change-point testing,Contrast-based method,CUSUM process,Global warming,Self-normalization","Zhang, Ting@Boston University::Lavitas, Liliya@Boston University"
Multivariate Functional Principal Component Analysis for Data Observed on Different (Dimensional) Domains,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,31,"Existing approaches for multivariate functional principal component analysis are restricted to data on the same one-dimensional interval. The presented approach focuses on multivariate functional data on different domains that may differ in dimension, such as functions and images. The theoretical basis for multivariate functional principal component analysis is given in terms of a Karhunen-Loeve Theorem. For the practically relevant case of a finite Karhunen-Loeve representation, a relationship between univariate and multivariate functional principal component analysis is established. This offers an estimation strategy to calculate multivariate functional principal components and scores based on their univariate counterparts. For the resulting estimators, asymptotic results are derived. The approach can be extended to finite univariate expansions in general, not necessarily orthonormal bases. It is also applicable for sparse functional data or data with measurement error. A flexible R implementation is available on CRAN. The new method is shown to be competitive to existing approaches for data observed on a common one-dimensional domain. The motivating application is a neuroimaging study, where the goal is to explore how longitudinal trajectories of a neuropsychological test score covary with FDG-PET brain scans at baseline.","Dimension reduction,Functional data analysis,Image analysis,Multivariate functional data","Happ, Clara@University of Munich::Greven, Sonja@University of Munich"
Variable Selection for Skewed Model-Based Clustering: Application to the Identification of Novel Sleep Phenotypes,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,75,"In sleep research, applying finite mixture models to sleep characteristics captured through multiple data types, including self-reported sleep diary, a wrist monitor capturing movement (actigraphy), and brain waves (polysomnography), may suggest new phenotypes that reflect underlying disease mechanisms. However, a direct mixture model application is challenging because there are many sleep variables from which to choose, and sleep variables are often highly skewed even in homogenous samples. Moreover, previous sleep research findings indicate that some of the most clinically interesting solutions will be those that incorporate all three data types. Thus, we present two novel skewed variable selection algorithms based on the multivariate skew normal (MSN) distribution: one that selects the best set of variables ignoring data type and another that embraces the exploratory nature of clustering and suggests multiple statistically plausible sets of variables that each incorporate all data types. Through a simulation study, we empirically compare our approach with other asymmetric and normal dimension reduction strategies for clustering. Finally, we demonstrate our methods using a sample of older adults with and without insomnia. The proposed MSN-based variable selection algorithm appears to be suitable for both MSN and multivariate normal cluster distributions, especially with moderate to large-sample sizes. Supplementary materials for this article are available online.","Insomnia,Mixture model,Research domain criteria,Skewed data","Wallace, Meredith L.@Pennsylvania Commonwealth System of Higher Education (PCSHE)@University of Pittsburgh::Buysse, Daniel J.@Pennsylvania Commonwealth System of Higher Education (PCSHE)@University of Pittsburgh::Germain, Anne@Pennsylvania Commonwealth System of Higher Education (PCSHE)@University of Pittsburgh::Hall, Martica H.@Pennsylvania Commonwealth System of Higher Education (PCSHE)@University of Pittsburgh::Iyengar, Satish@Pennsylvania Commonwealth System of Higher Education (PCSHE)@University of Pittsburgh"
Weighted False Discovery Rate Control in Large-Scale Multiple Testing,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,40,"The use of weights provides an effective strategy to incorporate prior domain knowledge in large-scale inference. This article studies weighted multiple testing in a decision-theoretical framework. We develop oracle and data-driven procedures that aim to maximize the expected number of true positives subject to a constraint on the weighted false discovery rate. The asymptotic validity and optimality of the proposed methods are established. The results demonstrate that incorporating informative domain knowledge enhances the interpretability of results and precision of inference. Simulation studies show that the proposed method controls the error rate at the nominal level, and the gain in power over existing methods is substantial in many settings. An application to a genome-wide association study is discussed. Supplementary materials for this article are available online.","Class weights,Decision weights,Multiple testing with groups,Prioritized subsets,Value-to-cost ratio,Weighted p-value","Basu, Pallavi@Tel Aviv University::Cai, T. Tony@University of Pennsylvania::Das, Kiranmoy@Indian Statistical Institute Kolkata@Indian Statistical Institute::Sun, Wenguang@University of Southern California"
Statistical Methods for Standard Membrane-Feeding Assays to Measure Transmission Blocking or Reducing Activity in Malaria,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,24,"Transmission blocking vaccines for malaria are not designed to directly protect vaccinated people from malaria disease, but to reduce the probability of infecting other people by interfering with the growth of the malaria parasite in mosquitoes. Standard membrane-feeding assays compare the growth of parasites in mosquitoes from a test sample (using antibodies from a vaccinated person) compared to a control sample. There is debate about whether to estimate the transmission reducing activity (TRA) which compares the mean number of parasites between test and control samples, or transmission blocking activity (TBA) which compares the proportion of infected mosquitoes. TBA appears biologically more important since each mosquito with any parasites is potentially infective; however, TBA is less reproducible and may be an overly strict criterion for screening vaccine candidates. Through a statistical model, we show that the TBA estimand depends on mu(c), the mean number of parasites in the control mosquitoes, a parameter not easily experimentally controlled. We develop a standardized TBA estimator based on the model and a given target value for mu(c) which has better mean squared error than alternative methods. We discuss types of statistical inference needed for using these assays for vaccine development.","Assay normalization,Negative binomial,Plasmodium falciparum,Standardization,Zero inflation","Swihart, Bruce J.@NIH National Institute of Allergy & Infectious Diseases (NIAID)@National Institutes of Health (NIH) - USA::Fay, Michael P.@NIH National Institute of Allergy & Infectious Diseases (NIAID)@National Institutes of Health (NIH) - USA::Miura, Kazutoyo@NIH National Institute of Allergy & Infectious Diseases (NIAID)@National Institutes of Health (NIH) - USA"
Latent Variable Poisson Models for Assessing the Regularity of Circadian Patterns over Time,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,34,"Many researchers in biology and medicine have focused on trying to understand biological rhythms and their potential impact on disease. A common biological rhythm is circadian, where the cycle repeats itself every 24 hours. However, a disturbance of the circadian pattern may be indicative of future disease. In this article, we develop new statistical methodology for assessing the degree of disturbance or irregularity in a circadian pattern for count sequences that are observed over time in a population of individuals. We develop a latent variable Poisson modeling approach with both circadian and stochastic short-term trend (autoregressive latent process) components that allow for individual variation in the degree of each component. A parameterization is proposed for modeling covariate dependence on the proportion of these two model components across individuals. In addition, we incorporate covariate dependence in the overall mean, the magnitude of the trend, and the phase-shift of the circadian pattern. Innovative Markov chain Monte Carlo sampling is used to carry out Bayesian posterior computation. Several variations of the proposed models are considered and compared using the deviance information criterion. We illustrate this methodology with longitudinal physical activity count data measured in a longitudinal cohort of adolescents.","Autoregressive process,Circadian pattern,Longitudinal count data,Physical activity,Random effect,Serial correlation,Shrinkage prior,Stick-breaking prior","Kim, Sungduk@NIH National Cancer Institute (NCI)@National Institutes of Health (NIH) - USA::Albert, Paul S.@NIH National Cancer Institute (NCI)@National Institutes of Health (NIH) - USA"
Polynomial Accelerated Solutions to a Large Gaussian Model for Imaging Biofilms: In Theory and Finite Precision,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,47,"Three-dimensional confocal scanning laser microscope images offer dramatic visualizations of living biofilms before and after interventions. Here, we use confocal microscopy to study the effect of a treatment over time that causes a biofilm to swell and contract due to osmotic pressure changes. From these data (the video is provided in the supplementary materials), our goal is to reconstruct biofilm surfaces, to estimate the effect of the treatment on the biofilm's volume, and to quantify the related uncertainties. We formulate the associated massive linear Bayesian inverse problem and then solve it using iterative samplers from large multivariate Gaussians that exploit well-established polynomial acceleration techniques from numerical linear algebra. Because of a general equivalence with linear solvers, these polynomial accelerated iterative samplers have known convergence rates, stopping criteria, and perform well in finite precision. An explicit algorithm is provided, for the first time, for an iterative sampler that is accelerated by the synergistic implementation of preconditioned conjugate gradient and Chebyshev polynomials. Supplementary materials for this article are available online.","Bayesian methods,Computationally intensive methods,Finite precision,Gibbs sampling","Parker, Albert E.@Montana State University@Montana State University System@Montana State University Bozeman::Pitts, Betsey@Montana State University@Montana State University Bozeman@Montana State University System::Lorenz, Lindsey@Montana State University@Montana State University Bozeman@Montana State University System::Stewart, Philip S.@Montana State University Bozeman@Montana State University System@Montana State University"
Inference Under Covariate-Adaptive Randomization,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,26,"This article studies inference for the average treatment effect in randomized controlled trials with covariate-adaptive randomization. Here, by covariate-adaptive randomization, we mean randomization schemes that first stratify according to baseline covariates and then assign treatment status so as to achieve balance within each stratum. Our main requirement is that the randomization scheme assigns treatment status within each stratum so that the fraction of units being assigned to treatment within each stratum has a well behaved distribution centered around a proportion as the sample size tends to infinity. Such schemes include, for example, Efron's biased-coin design and stratified block randomization. When testing the null hypothesis that the average treatment effect equals a prespecified value in such settings, we first show the usual two-sample t-test is conservative in the sense that it has limiting rejection probability under the null hypothesis no greater than and typically strictly less than the nominal level. We show, however, that a simple adjustment to the usual standard error of the two-sample t-test leads to a test that is exact in the sense that its limiting rejection probability under the null hypothesis equals the nominal level. Next, we consider the usual t-test (on the coefficient on treatment assignment) in a linear regression of outcomes on treatment assignment and indicators for each of the strata. We show that this test is exact for the important special case of randomization schemes with , but is otherwise conservative. We again provide a simple adjustment to the standard errors that yields an exact test more generally. Finally, we study the behavior of a modified version of a permutation test, which we refer to as the covariate-adaptive permutation test, that only permutes treatment status for units within the same stratum. When applied to the usual two-sample t-statistic, we show that this test is exact for randomization schemes with and that additionally achieve what we refer to as strong balance. For randomization schemes with , this test may have limiting rejection probability under the null hypothesis strictly greater than the nominal level. When applied to a suitably adjusted version of the two-sample t-statistic, however, we show that this test is exact for all randomization schemes that achieve strong balance, including those with . A simulation study confirms the practical relevance of our theoretical results. We conclude with recommendations for empirical practice and an empirical illustration. Supplementary materials for this article are available online.","Covariate-adaptive randomization,Efron's biased-coin design,Permutation test,Randomized controlled trial,Stratified block randomization,Two-sample t-test","Bugni, Federico A.@Duke University::Canay, Ivan A.@Northwestern University::Shaikh, Azeem M.@University of Chicago"
On Estimation of the Hazard Function From Population-Based Case-Control Studies,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,30,"The population-based case-control study design has been widely used for studying the etiology of chronic diseases. It is well established that the Cox proportional hazards model can be adapted to the case-control study and hazard ratios can be estimated by (conditional) logistic regression model with time as either a matched set or a covariate. However, the baseline hazard function, a critical component in absolute risk assessment, is unidentifiable, because the ratio of cases and controls is controlled by the investigators and does not reflect the true disease incidence rate in the population. In this article, we propose a simple and innovative approach, which makes use of routinely collected family history information, to estimate the baseline hazard function for any logistic regression model that is fit to the risk factor data collected on cases and controls. We establish that the proposed baseline hazard function estimator is consistent and asymptotically normal and show via simulation that it performs well in finite samples. We illustrate the proposed method by a population-based case-control study of prostate cancer where the association of various risk factors is assessed and the family history information is used to estimate the baseline hazard function.","Copula model,Family history,Marginal hazard function,Multivariate survival analysis","Hsu, Li@Fred Hutchinson Cancer Center::Gorfine, Malka@Tel Aviv University::Zucker, David@Hebrew University of Jerusalem"
Robust High-Dimensional Volatility Matrix Estimation for High-Frequency Factor Model,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,38,"High-frequency financial data allow us to estimate large volatility matrices with relatively short time horizon. Many novel statistical methods have been introduced to address large volatility matrix estimation problems from a high-dimensional Ito process with microstructural noise contamination. Their asymptotic theories require sub-Gaussian or some finite high-order moments assumptions for observed log-returns. These assumptions are at odd with the heavy tail phenomenon that is pandemic in financial stock returns and new procedures are needed to mitigate the influence of heavy tails. In this article, we introduce the Huber loss function with a diverging threshold to develop a robust realized volatility estimation. We show that it has the sub-Gaussian concentration around the volatility with only finite fourth moments of observed log-returns. With the proposed robust estimator as input, we further regularize it by using the principal orthogonal component thresholding (POET) procedure to estimate the large volatility matrix that admits an approximate factor structure. We establish the asymptotic theories for such low-rank plus sparse matrices. The simulation study is conducted to check the finite sample performance of the proposed estimation methods.","Concentration inequality,Huber loss,Low-rank matrix,Pre-averaging,Sparsity","Fan, Jianqing@Princeton University::Kim, Donggyu@Princeton University"
Modeling Persistent Trends in Distributions,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,31,"We present a nonparametric framework to model a short sequence of probability distributions that vary both due to underlying effects of sequential progression and confounding noise. To distinguish between these two types of variation and estimate the sequential-progression effects, our approach leverages an assumption that these effects follow a persistent trend. This work is motivated by the recent rise of single-cell RNA-sequencing experiments over a brief time course, which aim to identify genes relevant to the progression of a particular biological process across diverse cell populations. While classical statistical tools focus on scalar-response regression or order-agnostic differences between distributions, it is desirable in this setting to consider both the full distributions as well as the structure imposed by their ordering. We introduce a new regression model for ordinal covariates where responses are univariate distributions and the underlying relationship reflects consistent changes in the distributions over increasing levels of the covariate. This concept is formalized as a trend in distributions, which we define as an evolution that is linear under the Wasserstein metric. Implemented via a fast alternating projections algorithm, our method exhibits numerous strengths in simulations and analyses of single-cell gene expression data. Supplementary materials for this article are available online.","Batch effect,Pool adjacent violators algorithm,Quantile regression,Single cell RNA-seq,Wasserstein distance","Mueller, Jonas@Massachusetts Institute of Technology (MIT)::Jaakkola, Tommi@Massachusetts Institute of Technology (MIT)::Gifford, David@Massachusetts Institute of Technology (MIT)"
Tractable Bayesian Variable Selection: Beyond Normality,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,56,"Bayesian variable selection often assumes normality, but the effects of model misspecification are not sufficiently understood. There are sound reasons behind this assumption, particularly for large p: ease of interpretation, analytical, and computational convenience. More flexible frameworks exist, including semi- or nonparametric models, often at the cost of some tractability. We propose a simple extension that allows for skewness and thicker-than-normal tails but preserves tractability. It leads to easy interpretation and a log-concave likelihood that facilitates optimization and integration. We characterize asymptotically parameter estimation and Bayes factor rates, under certain model misspecification. Under suitable conditions, misspecified Bayes factors induce sparsity at the same rates than under the correct model. However, the rates to detect signal change by an exponential factor, often reducing sensitivity. These deficiencies can be ameliorated by inferring the error distribution, a simple strategy that can improve inference substantially. Our work focuses on the likelihood and can be combined with any likelihood penalty or prior, but here we focus on nonlocal priors to induce extra sparsity and ameliorate finite-sample effects caused by misspecification. We show the importance of considering the likelihood rather than solely the prior, for Bayesian variable selection. The methodology is in R package mombf.' Supplementary materials for this article are available online.","Bayes factors,Model misspecification,Robust regression,Two-piece errors,Variable selection","Rossell, David@Pompeu Fabra University::Rubio, Francisco J.@London School of Hygiene & Tropical Medicine@University of London"
A Bayesian Approach for Estimating Dynamic Functional Network Connectivity in fMRI Data,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,83,"Dynamic functional connectivity, that is, the study of how interactions among brain regions change dynamically over the course of an fMRI experiment, has recently received wide interest in the neuroimaging literature. Current approaches for studying dynamic connectivity often rely on ad hoc approaches for inference, with the fMRI time courses segmented by a sequence of sliding windows. We propose a principled Bayesian approach to dynamic functional connectivity, which is based on the estimation of time varying networks. Our method utilizes a hidden Markov model for classification of latent cognitive states, achieving estimation of the networks in an integrated framework that borrows strength over the entire time course of the experiment. Furthermore, we assume that the graph structures, which define the connectivity states at each time point, are related within a super-graph, to encourage the selection of the same edges among related graphs. We apply our method to simulated task -based fMRI data, where we show how our approach allows the decoupling of the task-related activations and the functional connectivity states. We also analyze data from an fMRI sensorimotor task experiment on an individual healthy subject and obtain results that support the role of particular anatomical regions in modulating interaction between executive control and attention networks.","Brain connectivity,Bayesian modeling,fMRI,Graphical models","Warnick, Ryan@Rice University::Guindani, Michele@University of California System@University of California Irvine::Erhardt, Erik@University of New Mexico::Allen, Elena@Unknow::Calhoun, Vince@University of New Mexico::Vannucci, Marina@Rice University"
Conditional Modeling of Longitudinal Data With Terminal Event,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,27,"We consider a random effects model for longitudinal data with the occurrence of an informative terminal event that is subject to right censoring. Existing methods for analyzing such data include the joint modeling approach using latent frailty and the marginal estimating equation approach using inverse probability weighting; in both cases the effect of the terminal event on the response variable is not explicit and thus not easily interpreted. In contrast, we treat the terminal event time as a covariate in a conditional model for the longitudinal data, which provides a straightforward interpretation while keeping the usual relationship of interest between the longitudinally measured response variable and covariates for times that are far from the terminal event. A two-stage semiparametric likelihood-based approach is proposed for estimating the regression parameters; first, the conditional distribution of the right-censored terminal event time given other covariates is estimated and then the likelihood function for the longitudinal event given the terminal event and other regression parameters is maximized. The method is illustrated by numerical simulations and by analyzing medical cost data for patients with end-stage renal disease. Desirable asymptotic properties are provided. Supplementary materials for this article are available online.","Cox regression,Empirical process,Mixed effects model,Pseudo-maximum likelihood estimation","Kong, Shengchun@Gilead Sciences::Nan, Bin@University of Michigan System@University of Michigan::Kalbfleisch, John D.@University of Michigan System@University of Michigan::Saran, Rajiv@University of Michigan System@University of Michigan::Hirth, Richard@University of Michigan System@University of Michigan"
Semiparametric Estimation of Longitudinal Medical Cost Trajectory,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,39,"Estimating the average monthly medical costs from disease diagnosis to a terminal event such as death for an incident cohort of patients is a topic of immense interest to researchers in health policy and health economics because patterns of average monthly costs over time reveal how medical costs vary across phases of care. The statistical challenges to estimating monthly medical costs longitudinally are multifold; the longitudinal cost trajectory (formed by plotting the average monthly costs from diagnosis to the terminal event) is likely to be nonlinear, with its shape depending on the time of the terminal event, which can be subject to right censoring. The goal of this article is to tackle this statistically challenging topic by estimating the conditional mean cost at any month t given the time of the terminal event s. The longitudinal cost trajectories with different terminal event times form a bivariate surface of t and s, under the constraint t <= s. We propose to estimate this surface using bivariate penalized splines in an expectation-maximization algorithm that treats the censored terminal event times as missing data. We evaluate the proposed model and estimation method in simulations and apply the method to the medical cost data of an incident cohort of stage IV breast cancer patients from the Surveillance, Epidemiology, and End Results-Medicare Linked Database.","Bivariate smoothing,Joint modeling,Lifetime and survival analysis,Medical cost,SEER Medicare","Li, Liang@UTMD Anderson Cancer Center@University of Texas System::Wu, Chih-Hsien@UTMD Anderson Cancer Center@University of Texas System::Ning, Jing@UTMD Anderson Cancer Center@University of Texas System::Huang, Xuelin@UTMD Anderson Cancer Center@University of Texas System::Shih, Ya-Chen Tina@UTMD Anderson Cancer Center@University of Texas System::Shen, Yu@UTMD Anderson Cancer Center@University of Texas System"
Hidden Population Size Estimation From Respondent-Driven Sampling: A Network Approach,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,86,"Estimating the size of stigmatized, hidden, or hard-to-reach populations is a major problem in epidemiology, demography, and public health research. Capture-recapture and multiplier methods are standard tools for inference of hidden population sizes, but they require random sampling of target population members, which is rarely possible. Respondent-driven sampling (RDS) is a survey method for hidden populations that relies on social link tracing. The RDS recruitment process is designed to spread through the social network connecting members of the target population. In this article, we show how to use network data revealed by RDS to estimate hidden population size. The key insight is that the recruitment chain, timing of recruitments, and network degrees of recruited subjects provide information about the number of individuals belonging to the target population who are not yet in the sample. We use a computationally efficient Bayesian method to integrate over the missing edges in the subgraph of recruited individuals. We validate the method using simulated data and apply the technique to estimate the number of people who inject drugs in St.Peters-burg, Russia.","Hidden population,Injection drug use,Network inference,Population size","Crawford, Forrest W.@Yale University::Wu, Jiacheng@Yale University::Heimer, Robert@Yale University"
Optimal Penalized Function-on-Function Regression Under a Reproducing Kernel Hilbert Space Framework,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,33,"Many scientific studies collect data where the response and predictor variables are both functions of time, location, or some other covariate. Understanding the relationship between these functional variables is a common goal in these studies. Motivated from two real-life examples, we present in this article a function-on-function regression model that can be used to analyze such kind of functional data. Our estimator of the 2D coefficient function is the optimizer of a form of penalized least squares where the penalty enforces a certain level of smoothness on the estimator. Our first result is the representer theorem which states that the exact optimizer of the penalized least squares actually resides in a data-adaptive finite-dimensional subspace although the optimization problem is defined on a function space of infinite dimensions. This theorem then allows us an easy incorporation of the Gaussian quadrature into the optimization of the penalized least squares, which can be carried out through standard numerical procedures. We also show that our estimator achieves the minimax convergence rate in mean prediction under the framework of function-on-function regression. Extensive simulation studies demonstrate the numerical advantages of our method over the existing ones, where a sparse functional data extension is also introduced. The proposed method is then applied to our motivating examples of the benchmark Canadian weather data and a histone regulation study. Supplementary materials for this article are available online.","Function-on-Function regression,Minimax convergence rate,Penalized least squares,Representer theorem,Reproducing kernel Hilbert space","Sun, Xiaoxiao@University System of Georgia@University of Georgia::Du, Pang@Virginia Polytechnic Institute & State University::Wang, Xiao@Purdue University@Purdue University System::Ma, Ping@University System of Georgia@University of Georgia"
Bayesian Approximate Kernel Regression With Variable Selection,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,51,"Nonlinear kernel regression models are often used in statistics and machine learning because they are more accurate than linear models. Variable selection for kernel regression models is a challenge partly because, unlike the linear regression setting, there is no clear concept of an effect size for regression coefficients. In this article, we propose a novel framework that provides an effect size analog for each explanatory variable in Bayesian kernel regression models when the kernel is shift-invariantfor example, the Gaussian kernel. We use function analytic properties of shift-invariant reproducing kernel Hilbert spaces (RKHS) to define a linear vector space that: (i) captures nonlinear structure, and (ii) can be projected onto the original explanatory variables. This projection onto the original explanatory variables serves as an analog of effect sizes. The specific function analytic property we use is that shift-invariant kernel functions can be approximated via random Fourier bases. Based on the random Fourier expansion, we propose a computationally efficient class of Bayesian approximate kernel regression (BAKR) models for both nonlinear regression and binary classification for which one can compute an analog of effect sizes. We illustrate the utility of BAKR by examining two important problems in statistical genetics: genomic selection (i.e.,phenotypic prediction) and association mapping (i.e.,inference of significant variants or loci). State-of-the-art methods for genomic selection and association mapping are based on kernel regression and linear models, respectively. BAKR is the first method that is competitive in both settings. Supplementary materials for this article are available online.","Effect size,Epistasis,Kernel regression,Variable selection,Random Fourier features,Statistical genetics","Crawford, Lorin@Brown University::Wood, Kris C.@Duke University::Zhou, Xiang@University of Michigan System@University of Michigan::Mukherjee, Sayan@Duke University"
Interpretable Dynamic Treatment Regimes,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,43,"Precision medicine is currently a topic of great interest in clinical and intervention science. A key component of precision medicine is that it is evidence-based, that is, data-driven, and consequently there has been tremendous interest in estimation of precision medicine strategies using observational or randomized study data. One way to formalize precision medicine is through a treatment regime, which is a sequence of decision rules, one per stage of clinical intervention, that map up-to-date patient information to a recommended treatment. An optimal treatment regime is defined as maximizing the mean of some cumulative clinical outcome if applied to a population of interest. It is well-known that even under simple generative models an optimal treatment regime can be a highly nonlinear function of patient information. Consequently, a focal point of recent methodological research has been the development of flexible models for estimating optimal treatment regimes. However, in many settings, estimation of an optimal treatment regime is an exploratory analysis intended to generate new hypotheses for subsequent research and not to directly dictate treatment to new patients. In such settings, an estimated treatment regime that is interpretable in a domain context may be of greater value than an unintelligible treatment regime built using black-box estimation methods. We propose an estimator of an optimal treatment regime composed of a sequence of decision rules, each expressible as a list of if-then statements that can be presented as either a paragraph or as a simple flowchart that is immediately interpretable to domain experts. The discreteness of these lists precludes smooth, that is, gradient-based, methods of estimation and leads to nonstandard asymptotics. Nevertheless, we provide a computationally efficient estimation algorithm, prove consistency of the proposed estimator, and derive rates of convergence. We illustrate the proposed methods using a series of simulation examples and application to data from a sequential clinical trial on bipolar disorder. Supplementary materials for this article are available online.","Decision lists,Interpretability,Precision medicine,Research-practice gap,Treatment regimes,Tree-based methods","Zhang, Yichi@Harvard University::Laber, Eric B.@North Carolina State University@University of North Carolina::Davidian, Marie@North Carolina State University@University of North Carolina::Tsiatis, Anastasios A.@North Carolina State University@University of North Carolina"
A Bayesian Variable Selection Approach Yields Improved Detection of Brain Activation From Complex-Valued fMRI,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,52,"Voxel functional magnetic resonance imaging (fMRI) time courses are complex-valued signals giving rise to magnitude and phase data. Nevertheless, most studies use only the magnitude signals and thus discard half of the data that could potentially contain important information. Methods that make use of complex-valued fMRI (CV-fMRI) data have been shown to lead to superior power in detecting active voxels when compared to magnitude-only methods, particularly for small signal-to-noise ratios (SNRs). We present a new Bayesian variable selection approach for detecting brain activation at the voxel level from CV-fMRI data. We develop models with complex-valued spike-and-slab priors on the activation parameters that are able to combine the magnitude and phase information. We present a complex-valued EM variable selection algorithm that leads to fast detection at the voxel level in CV-fMRI slices and also consider full posterior inference via Markov chain Monte Carlo (MCMC). Model performance is illustrated through extensive simulation studies, including the analysis of physically based simulated CV-fMRI slices. Finally, we use the complex-valued Bayesian approach to detect active voxels in human CV-fMRI from a healthy individual who performed unilateral finger tapping in a designed experiment. The proposed approach leads to improved detection of activation in the expected motor-related brain regions and produces fewer false positive results than other methods for CV-fMRI. Supplementary materials for this article are available online.","Bayesian modeling,Complex-valued time series,CV-fMRI,Variable selection","Yu, Cheng-Han@University of California System@University of California Santa Cruz::Prado, Raquel@University of California System@University of California Santa Cruz::Ombao, Hernando@King Abdullah University of Science & Technology::Rowe, Daniel@Marquette University"
An Efficient Surrogate Model for Emulation and Physics Extraction of Large Eddy Simulations,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,50,"In the quest for advanced propulsion and power-generation systems, high-fidelity simulations are too computationally expensive to survey the desired design space, and a new design methodology is needed that combines engineering physics, computer simulations, and statistical modeling. In this article, we propose a new surrogate model that provides efficient prediction and uncertainty quantification of turbulent flows in swirl injectors with varying geometries, devices commonly used in many engineering applications. The novelty of the proposed method lies in the incorporation of known physical properties of the fluid flow as simplifying assumptions for the statistical model. In view of the massive simulation data at hand, which is on the order of hundreds of gigabytes, these assumptions allow for accurate flow predictions in around an hour of computation time. To contrast, existing flow emulators which forgo such simplifications may require more computation time for training and prediction than is needed for conducting the simulation itself. Moreover, by accounting for coupling mechanisms between flow variables, the proposed model can jointly reduce prediction uncertainty and extract useful flow physics, which can then be used to guide further investigations. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.","Computer experiments,Kriging,Rocket injectors,Sparsity,Spatio-temporal flow,Turbulence","Mak, Simon@University System of Georgia@Georgia Institute of Technology::Sung, Chih-Li@University System of Georgia@Georgia Institute of Technology::Wang, Xingjian@University System of Georgia@Georgia Institute of Technology::Yeh, Shiang-Ting@University System of Georgia@Georgia Institute of Technology::Chang, Yu-Hung@University System of Georgia@Georgia Institute of Technology::Joseph, V. Roshan@University System of Georgia@Georgia Institute of Technology::Yang, Vigor@University System of Georgia@Georgia Institute of Technology::Wu, C. F. Jeff@University System of Georgia@Georgia Institute of Technology"
Modeling Random Effects Using Global-Local Shrinkage Priors in Small Area Estimation,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,43,"Small area estimation is becoming increasingly popular for survey statisticians. One very important program is Small Area Income and Poverty Estimation undertaken by the United States Bureau of the Census, which aims at providing estimates related to income and poverty based on American Community Survey data at the state level and even at lower levels of geography. This article introduces global-local (GL) shrinkage priors for random effects in small area estimation to capture wide area level variation when the number of small areas is very large. These priors employ two levels of parameters, global and local parameters, to express variances of area-specific random effects so that both small and large random effects can be captured properly. We show via simulations and data analysis that use of the GL priors can improve estimation results in most cases. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.","Bayesian model,Fay-Herriot model,Poverty rate,Spike-and-slab prior","Tang, Xueying@Columbia University::Ghosh, Malay@University of Florida@State University System of Florida::Ha, Neung Soo@Unknow::Sedransk, Joseph@University System of Maryland@University of Maryland College Park"
To Wait or Not to Wait: Two-Way Functional Hazards Model for Understanding Waiting in Call Centers,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,41,"Telephone call centers offer a convenient communication channel between businesses and their customers. Efficient management of call centers needs accurate modeling of customer waiting behavior, which contains important information about customer patience (how long a customer is willing to wait) and service quality (how long a customer needs to wait to get served). Hazard functions offer dynamic characterization of customer waiting behavior, and provide critical inputs for agent scheduling. Motivated by this application, we develop a two-way functional hazards (tF-Hazards) model to study customer waiting behavior as a function of two timescales, waiting duration and the time of day that a customer calls in. The model stems from a two-way piecewise constant hazard function, and imposes low-rank structure and smoothness on the hazard rates to enhance interpretability. We exploit an alternating direction method of multipliers algorithm to optimize a penalized likelihood function of the model. We carefully analyze the data from a U.S. Bank call center, and provide informative insights about customer patience and service quality patterns along waiting time and across different times of a day. The findings provide primitive inputs for call center agent staffing and scheduling, as well as for call center practitioners to understand the effect of system protocols on customer waiting behavior. Supplementary materials for this article are available online.","Alternating direction method of multipliers,Call center workforce management,Human patience,Low-rank structure,Penalized likelihood,Smooth hazard surface","Li, Gen@Columbia University::Huang, Jianhua Z.@Texas A&M University College Station@Texas A&M University System::Shen, Haipeng@University of Hong Kong"
"Dynamic Modeling of Conditional Quantile Trajectories, With Application to Longitudinal Snippet Data",JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,49,"Longitudinal data are often plagued with sparsity of time points where measurements are available. The functional data analysis perspective has been shown to provide an effective and flexible approach to address this problem for the case where measurements are sparse but their times are randomly distributed over an interval. Here, we focus on a different scenario where available data can be characterized as snippets, which are very short stretches of longitudinal measurements. For each subject, the stretch of available data is much shorter than the time frame of interest, a common occurrence in accelerated longitudinal studies. An added challenge is introduced if a time proxy that is basic for usual longitudinal modeling is not available. This situation arises in the case of Alzheimer's disease and comparable scenarios, where one is interested in time dynamics of declining performance, but the time of disease onset is unknown and chronological age does not provide a meaningful time reference for longitudinal modeling. Our main methodological contribution to address these challenges is to introduce conditional quantile trajectories for monotonic processes that emerge as solutions of a dynamic system. Our proposed estimates for these trajectories are shown to be uniformly consistent. Conditional quantile trajectories are useful descriptors of processes that quantify deterioration over time, such as hippocampal volumes in Alzheimer's patients. We demonstrate how the proposed approach can be applied to longitudinal snippets data sampled from such processes. Supplementary materials for this article are available online.","Accelerated longitudinal study,Autonomous differential equation,Functional data analysis,Hippocampal volume,Monotonic process,Nonparametric estimation,Uniform convergence","Dawson, Matthew@University of California Davis@University of California System::Mueller, Hans-Georg@University of California Davis@University of California System"
Modeling Tangential Vector Fields on a Sphere,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,56,"Physical processes that manifest as tangential vector fields on a sphere are common in geophysical and environmental sciences. These naturally occurring vector fields are often subject to physical constraints, such as being curl-free or divergence-free. We start with constructing parametric models for curl-free and divergence-free vector fields that are tangential to the unit sphere through applying the surface gradient or the surface curl operator to a scalar random potential field on the unit sphere. Using the Helmholtz-Hodge decomposition, we then construct a class of simple but flexible parametric models for general tangential vector fields, which are represented as a sum of a curl-free and a divergence-free components. We propose a likelihood-based parameter estimation procedure, and show that fast computation is possible even for large datasets when the observations are on a regular latitude-longitude grid. Characteristics and practical utility of the proposed methodology are illustrated through extensive simulation studies and an application to a dataset of ocean surface wind velocities collected by satellite-based scatterometers. We also compare our model with a bivariate Matern model and a non-stationary bivariate global model. Supplementary materials for this article are available online.","Curl-free,Divergence-free,Helmholtz-Hodge decomposition,Matern model,Ocean surface wind","Fan, Minjie@University of California Davis@University of California System::Paul, Debashis@University of California Davis@University of California System::Lee, Thomas C. M.@University of California Davis@University of California System::Matsuo, Tomoko@University of Colorado System@University of Colorado Boulder"
A Nonparametric Graphical Model for Functional Data With Application to Brain Networks Based on fMRI,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,41,"We introduce a nonparametric graphical model whose observations on vertices are functions. Many modern applications, such as electroencephalogram and functional magnetic resonance imaging (fMRI), produce data are of this type. The model is based on additive conditional independence (ACI), a statistical relation that captures the spirit of conditional independence without resorting to multi-dimensional kernels. The random functions are assumed to reside in a Hilbert space. No distributional assumption is imposed on the random functions: instead, their statistical relations are characterized nonparametrically by a second Hilbert space, which is a reproducing kernel Hilbert space whose kernel is determined by the inner product of the first Hilbert space. A precision operator is then constructed based on the second space, which characterizes ACI, and hence also the graph. The resulting estimator is relatively easy to compute, requiring no iterative optimization or inversion of large matrices. We establish the consistency and the convergence rate of the estimator. Through simulation studies we demonstrate that the estimator performs better than the functional Gaussian graphical model when the relations among vertices are nonlinear or heteroscedastic. The method is applied to an fMRI dataset to construct brain networks for patients with attention-deficit/hyperactivity disorder. Supplementary materials for this article are available online","Additive conditional independence,Additive correlation operator,Additive precision operator,EEG,fMRI,Gaussian graphical model,Reproducing kernel Hilbert space","Li, Bing@Penn State University@Pennsylvania Commonwealth System of Higher Education (PCSHE)::Solea, Eftychia@Penn State University@Pennsylvania Commonwealth System of Higher Education (PCSHE)"
Bayesian Estimation and Comparison of Moment Condition Models,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,40,"In this article, we develop a Bayesian semiparametric analysis of moment condition models by casting the problem within the exponentially tilted empirical likelihood (ETEL) framework. We use this framework to develop a fully Bayesian analysis of correctly and misspecified moment condition models. We show that even under misspecification, the Bayesian ETEL posterior distribution satisfies the Bernstein-von Mises (BvM) theorem. We also develop a unified approach based on marginal likelihoods and Bayes factors for comparing different moment-restricted models and for discarding any misspecified moment restrictions. Computation of the marginal likelihoods is by the method of Chib (1995) as extended to Metropolis-Hastings samplers in Chib and Jeliazkov in 2001. We establish the model selection consistency of the marginal likelihood and show that the marginal likelihood favors the model with the minimum number of parameters and the maximum number of valid moment restrictions. When the models are misspecified, the marginal likelihood model selection procedure selects the model that is closer to the (unknown) true data-generating process in terms of the Kullback-Leibler divergence. The ideas and results in this article broaden the theoretical underpinning and value of the Bayesian ETEL framework with many practical applications. The discussion is illuminated through several examples. Supplementary materials for this article are available online.","Bernstein-von Mises theorem,Estimating equations,Exponentially tilted empirical likelihood,Marginal likelihood,Misspecification,Model selection consistency","Chib, Siddhartha@Washington University (WUSTL)::Shin, Minchul@University of Illinois Urbana-Champaign@University of Illinois System::Simoni, Anna@Universite Paris Saclay (ComUE)@Centre National de la Recherche Scientifique (CNRS)@ENSAE ParisTech"
Reconciling Curvature and Importance Sampling Based Procedures for Summarizing Case Influence in Bayesian Models,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,25,"Methods for summarizing case influence in Bayesian models take essentially two forms: (1) use common divergence measures for calculating distances between the full-data posterior and the case-deleted posterior, and (2) measure the impact of infinitesimal perturbations to the likelihood to study local case influence. Methods based on approach (1) lead naturally to considering the behavior of case-deletion importance sampling weights (the weights used to approximate samples from the case-deleted posterior using samples from the full posterior). Methods based on approach (2) lead naturally to considering the local curvature of the Kullback-Leibler divergence of the full posterior from a geometrically perturbed quasi-posterior. By examining the connections between the two approaches, we establish a rationale for employing low-dimensional summaries of case influence obtained entirely via the variance-covariance matrix of the log importance sampling weights. We illustrate the use of the proposed diagnostics using real and simulated data. Supplementary materials are available online.","Case deletion,Covariance matrix,Graphical model diagnostics,Kullback-Leibler divergence,Principal components analysis","Thomas, Zachary M.@Eli Lilly::MacEachern, Steven N.@Ohio State University::Peruggia, Mario@Ohio State University"
Particle EM for Variable Selection,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,40,"Despite its long history of success, the EM algorithm has been vulnerable to local entrapment when the posterior/likelihood is multi-modal. This is particularly pronounced in spike-and-slab posterior distributions for Bayesian variable selection. The main thrust of this article is to introduce the particle EM algorithm, a new population-based optimization strategy that harvests multiple modes in search spaces that present many local maxima. Motivated by nonparametric variational Bayes strategies, particle EM achieves this goal by deploying an ensemble of interactive repulsive particles. These particles are geared toward uncharted areas of the posterior, providing a more comprehensive summary of its topography than simple parallel EM deployments. A sequential Monte Carlo variant of particle EM is also proposed that explores a sequence of annealed posteriors by sampling from a set of mutually avoiding particles. Particle EM outputs a deterministic reconstruction of the posterior distribution for approximate fully Bayes inference by capturing its essential modes and mode weights. This reconstruction reflects model selection uncertainty and is supported by asymptotic considerations, which indicate that the requisite number of particles need not be large in the presence of sparsity (when p > n). Supplementary materials for this article are available online.","EM algorithm,Ensemble optimization,Multimodality,Spike-and-slab","Rockova, Veronika@University of Chicago"
A Massive Data Framework for M-Estimators with Cubic-Rate,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,24,"The divide and conquer method is a common strategy for handling massive data. In this article, we study the divide and conquer method for cubic-rate estimators under the massive data framework. We develop a general theory for establishing the asymptotic distribution of the aggregated M-estimators using a weighted average with weights depending on the subgroup sample sizes. Under certain condition on the growing rate of the number of subgroups, the resulting aggregated estimators are shown to have faster convergence rate and asymptotic normal distribution, which are more tractable in both computation and inference than the original M-estimators based on pooled data. Our theory applies to a wide class of M-estimators with cube root convergence rate, including the location estimator, maximum score estimator, and value search estimator. Empirical performance via simulations and a real data application also validate our theoretical findings. Supplementary materials for this article are available online.","Cubic rate asymptotics,Divide and conquer,M-estimators,Massive data","Shi, Chengchun@North Carolina State University@University of North Carolina::Lu, Wenbin@North Carolina State University@University of North Carolina::Song, Rui@North Carolina State University@University of North Carolina"
A Powerful Bayesian Test for Equality of Means in High Dimensions,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,27,"We develop a Bayes factor-based testing procedure for comparing two population means in high-dimensional settings. In large-p-small-n settings, Bayes factors based on proper priors require eliciting a large and complex p x p covariance matrix, whereas Bayes factors based on Jeffrey's prior suffer the same impediment as the classical Hotelling T-2 test statistic as they involve inversion of ill-formed sample covariance matrices. To circumvent this limitation, we propose that the Bayes factor be based on lower dimensional random projections of the high-dimensional data vectors. We choose the prior under the alternative to maximize the power of the test for a fixed threshold level, yielding a restricted most powerful Bayesian test (RMPBT). The final test statistic is based on the ensemble of Bayes factors corresponding to multiple replications of randomly projected data. We show that the test is unbiased and, under mild conditions, is also locally consistent. We demonstrate the efficacy of the approach through simulated and real data examples. Supplementary materials for this article are available online.","Bayes factor,Random projection,Restricted most powerful Bayesian tests,Testing of hypotheses","Zoh, Roger S.@Texas A&M University College Station@Texas A&M University System::Sarkar, Abhra@Duke University::Carroll, Raymond J.@Texas A&M University College Station@Texas A&M University System@University of Technology Sydney::Mallick, Bani K.@Texas A&M University College Station@Texas A&M University System"
Post-Selection Inference Following Aggregate Level Hypothesis Testing in Large-Scale Genomic Data,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,29,"In many genomic applications, hypotheses tests are performed for powerful identification of signals by aggregating test-statistics across units within naturally defined classes. Following class-level testing, it is naturally of interest to identify the lower level units which contain true signals. Testing the individual units within a class without taking into account the fact that the class was selected using an aggregate-level test-statistic, will produce biased inference. We develop a hypothesis testing framework that guarantees control for false positive rates conditional on the fact that the class was selected. Specifically, we develop procedures for calculating unit level p-values that allows rejection of null hypotheses controlling for two types of conditional error rates, one relating to family-wise rate and the other relating to false discovery rate. We use simulation studies to illustrate validity and power of the proposed procedure in comparison to several possible alternatives. We illustrate the power of the method in a natural application involving whole-genome expression quantitative trait loci (eQTL) analysis across 17 tissue types using data from The Cancer Genome Atlas (TCGA) Project. Supplementary materials for this article are available online.","Conditional p-value,False discovery rate,Multiple testing,Selective inference","Heller, Ruth@NIH National Cancer Institute (NCI)@National Institutes of Health (NIH) - USA@Tel Aviv University::Chatterjee, Nilanjan@Johns Hopkins Bloomberg School of Public Health@Johns Hopkins University::Krieger, Abba@University of Pennsylvania::Shi, Jianxin@NIH National Cancer Institute (NCI)@National Institutes of Health (NIH) - USA"
Efficient Estimation for Semiparametric Structural Equation Models With Censored Data,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,34,"Structural equation modeling is commonly used to capture complex structures of relationships among multiple variables, both latent and observed. We propose a general class of structural equation models with a semiparametric component for potentially censored survival times. We consider nonparametric maximum likelihood estimation and devise a combined expectation-maximization and Newton-Raphson algorithm for its implementation. We establish conditions for model identifiability and prove the consistency, asymptotic normality, and semiparametric efficiency of the estimators. Finally, we demonstrate the satisfactory performance of the proposed methods through simulation studies and provide an application to a motivating cancer study that contains a variety of genomic variables. Supplementary materials for this article are available online.","Integrative analysis,Joint modeling,Latent variables,Model identifiability,Nonparametric maximum likelihood estimation,Survival analysis","Wong, Kin Yau@University of North Carolina@University of North Carolina Chapel Hill::Zeng, Donglin@University of North Carolina@University of North Carolina Chapel Hill::Lin, D. Y.@University of North Carolina@University of North Carolina Chapel Hill"
Testing for Inequality Constraints in Singular Models by Trimming or Winsorizing the Variance Matrix,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,45,"There are many applications in which a statistic follows, at least asymptotically, a normal distribution with a singular or nearly singular variance matrix. A classic example occurs in linear regression models under multicollinearity but there are many more such examples. There is well-developed theory for testing linear equality constraints when the alternative is two-sided and the variance matrix is either singular or nonsingular. In recent years, there is considerable, and growing, interest in developing methods for situations in which the estimated variance matrix is nearly singular. However, there is no corresponding methodology for addressing one-sided, that is, constrained or ordered alternatives. In this article, we develop a unified framework for analyzing such problems. Our approach may be viewed as the trimming or winsorizing of the eigenvalues of the corresponding variance matrix. The proposed methodology is applicable to a wide range of scientific problems and to a variety of statistical models in which inequality constraints arise. We illustrate the methodology using data from a gene expression microarray experiment obtained from the NIEHS' Fibroid Growth Study. Supplementary materials for this article are available online.","Constrained and ordered inference,Generalized inverse,Modified likelihood ratio test (mLRT),Moore-Penrose inverse,(nearly) Singular variance matrix","Davidov, Ori@University of Haifa::Jelsema, Casey M.@West Virginia University::Peddada, Shyamal@NIH National Institute of Environmental Health Sciences (NIEHS)@National Institutes of Health (NIH) - USA"
"Partial Identification of the Average Treatment Effect Using Instrumental Variables: Review of Methods for Binary Instruments, Treatments, and Outcomes",JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,61,"Several methods have been proposed for partially or point identifying the average treatment effect (ATE) using instrumental variable (IV) type assumptions. The descriptions of these methods are widespread across the statistical, economic, epidemiologic, and computer science literature, and the connections between the methods have not been readily apparent. In the setting of a binary instrument, treatment, and outcome, we review proposed methods for partial and point identification of the ATE under IV assumptions, express the identification results in a common notation and terminology, and propose a taxonomy that is based on sets of identifying assumptions. We further demonstrate and provide software for the application of these methods to estimate bounds. Supplementary materials for this article are available online.","Average treatment effect,Causal graphical model,Instrument,Instrumental variable,Partial identification,Single world intervention graph","Swanson, Sonja A.@Harvard T.H. Chan School of Public Health@Erasmus University Medical Center@Erasmus University Rotterdam::Hernan, Miguel A.@Harvard T.H. Chan School of Public Health@Harvard University::Miller, Matthew@Harvard T.H. Chan School of Public Health@Northeastern University::Robins, James M.@Harvard T.H. Chan School of Public Health::Richardson, Thomas S.@University of Washington@University of Washington Seattle"
Bayesian Semiparametric Multivariate Density Deconvolution,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,38,"We consider the problem of multivariate density deconvolution when interest lies in estimating the distribution of a vector valued random variable X but precise measurements on X are not available, observations being contaminated by measurement errors U. The existing sparse literature on the problem assumes the density of the measurement errors to be completely known. We propose robust Bayesian semiparametric multivariate deconvolution approaches when the measurement error density of U is not known but replicated proxies are available for at least some individuals. Additionally, we allow the variability of U to depend on the associated unobserved values of X through unknown relationships, which also automatically includes the case of multivariate multiplicative measurement errors. Basic properties of finite mixture models, multivariate normal kernels, and exchangeable priors are exploited in novel ways to meet modeling and computational challenges. Theoretical results showing the flexibility of the proposed methods in capturing a wide variety of data-generating processes are provided. We illustrate the efficiency of the proposed methods in recovering the density of X through simulation experiments. The methodology is applied to estimate the joint consumption pattern of different dietary components from contaminated 24 h recalls. Supplementary materials for this article are available online.","B-splines,Conditional heteroscedasticity,Latent factor analyzers,Measurement errors,Mixture models,Multivariate density deconvolution,Regularization,Shrinkage","Sarkar, Abhra@Duke University@University of Texas Austin@University of Texas System::Pati, Debdeep@Texas A&M University College Station@Florida State University@Texas A&M University System@State University System of Florida::Chakraborty, Antik@Texas A&M University College Station@Texas A&M University System::Mallick, Bani K.@Texas A&M University College Station@Texas A&M University System::Carroll, Raymond J.@Texas A&M University College Station@Texas A&M University System@University of Technology Sydney"
Learning Optimal Personalized Treatment Rules in Consideration of Benefit and Risk: With an Application to Treating Type 2 Diabetes Patients With Insulin Therapies,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,38,"Individualized medical decision making is often complex due to patient treatment response heterogeneity. Pharmacotherapy may exhibit distinct efficacy and safety profiles for different patient populations. An optimal treatment that maximizes clinical benefit for a patient may also lead to concern of safety due to a high risk of adverse events. Thus, to guide individualized clinical decision making and deliver optimal tailored treatments, maximizing clinical benefit should be considered in the context of controlling for potential risk. In this work, we propose two approaches to identify personalized optimal treatment strategy that maximizes clinical benefit under a constraint on the average risk. We derive the theoretical optimal treatment rule under the risk constraint and draw an analogy to the Neyman-Pearson lemma to prove the theorem. We present algorithms that can be easily implemented by any off-the-shelf quadratic programming package. We conduct extensive simulation studies to show satisfactory risk control when maximizing the clinical benefit. Finally, we apply our method to a randomized trial of type 2 diabetes patients to guide optimal utilization of the first line insulin treatments based on individual patient characteristics while controlling for the rate of hypoglycemia events. We identify baseline glycated hemoglobin level, body mass index, and fasting blood glucose as three key factors among 18 biomarkers to differentiate treatment assignments, and demonstrate a successful control of the risk of hypoglycemia in both the training and testing dataset.","Benefit-risk analysis,Hypoglycemia,Machine learning,Neyman-Pearson lemma,Personalized medicine","Wang, Yuanjia@Columbia University::Fu, Haoda@Eli Lilly::Zeng, Donglin@University of North Carolina School of Medicine@University of North Carolina@University of North Carolina Chapel Hill"
"The Effect of Probing ""Don't Know"" Responses on Measurement Quality and Nonresponse in Surveys",JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,37,"In survey interviews, Don't know (DK) responses are commonly treated as missing data. One way to reduce the rate of such responses is to probe initial DK answers with a follow-up question designed to encourage respondents to give substantive, non-DK responses. However, such probing can also reduce data quality by introducing additional or differential measurement error. We propose a latent variable model for analyzing the effects of probing on responses to survey questions. The model makes it possible to separate measurement effects of probing from true differences between respondents who do and do not require probing. We analyze new data from an experiment, which compared responses to two multi-item batteries of questions with and without probing. In this study, probing reduced the rate of DK responses by around a half. However, it also had substantial measurement effects, in that probed answers were often weaker measures of constructs of interest than were unprobed answers. These effects were larger for questions on attitudes than for pseudo-knowledge questions on perceptions of external facts. The results provide evidence against the use of probing of Don't know responses, at least for the kinds of items and respondents considered in this study. Supplementary materials for this article are available online.","Interviewing,Latent class model,Latent variable model,Measurement error,Missing data,Questionnaire design","Kuha, Jouni@London School Economics & Political Science@University of London::Butt, Sarah@City University London::Katsikatsou, Myrsini@London School Economics & Political Science@University of London::Skinner, Chris J.@London School Economics & Political Science@University of London"
Analyzing Two-Stage Experiments in the Presence of Interference,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,54,"Two-stage randomization is a powerful design for estimating treatment effects in the presence of interference; that is, when one individual's treatment assignment affects another individual's outcomes. Our motivating example is a two-stage randomized trial evaluating an intervention to reduce student absenteeism in the School District of Philadelphia. In that experiment, households with multiple students were first assigned to treatment or control; then, in treated households, one student was randomly assigned to treatment. Using this example, we highlight key considerations for analyzing two-stage experiments in practice. Our first contribution is to address additional complexities that arise when household sizes vary; in this case, researchers must decide between assigning equal weight to households or equal weight to individuals. We propose unbiased estimators for a broad class of individual- and household-weighted estimands, with corresponding theoretical and estimated variances. Our second contribution is to connect two common approaches for analyzing two-stage designs: linear regression and randomization inference. We show that, with suitably chosen standard errors, these two approaches yield identical point and variance estimates, which is somewhat surprising given the complex randomization scheme. Finally, we explore options for incorporating covariates to improve precision. We confirm our analytic results via simulation studies and apply these methods to the attendance study, finding substantively meaningful spillover effects.","Causal inference under interference,Randomization inference,Student attendance,Two-stage randomization","Basse, Guillaume@Harvard University::Feller, Avi@University of California Berkeley@University of California System"
Compression and Conditional Emulation of Climate Model Output,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,35,"Numerical climate model simulations run at high spatial and temporal resolutions generate massive quantities of data. As our computing capabilities continue to increase, storing all of the data is not sustainable, and thus it is important to develop methods for representing the full datasets by smaller compressed versions. We propose a statistical compression and decompression algorithm based on storing a set of summary statistics as well as a statistical model describing the conditional distribution of the full dataset given the summary statistics. We decompress the data by computing conditional expectations and conditional simulations from the model given the summary statistics. Conditional expectations represent our best estimate of the original data but are subject to oversmoothing in space and time. Conditional simulations introduce realistic small-scale noise so that the decompressed fields are neither too smooth nor too rough compared with the original data. Considerable attention is paid to accurately modeling the original dataset1 year of daily mean temperature dataparticularly with regard to the inherent spatial nonstationarity in global fields, and to determining the statistics to be stored, so that the variation in the original data can be closely captured, while allowing for fast decompression and conditional emulation on modest computers. Supplementary materials for this article are available online.","Gaussian process,Half-spectral,Nonstationary,Spatial-temporal data,SPDE","Guinness, Joseph@North Carolina State University@University of North Carolina::Hammerling, Dorit@National Center Atmospheric Research (NCAR) - USA"
Modeling for Dynamic Ordinal Regression Relationships: An Application to Estimating Maturity of Rockfish in California,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,42,"We develop a Bayesian nonparametric framework for modeling ordinal regression relationships, which evolve in discrete time. The motivating application involves a key problem in fisheries research on estimating dynamically evolving relationships between age, length, and maturity, the latter recorded on an ordinal scale. The methodology builds from nonparametric mixture modeling for the joint stochastic mechanism of covariates and latent continuous responses. This approach yields highly flexible inference for ordinal regression functions while at the same time avoiding the computational challenges of parametric models that arise from estimation of cut-off points relating the latent continuous and ordinal responses. A novel-dependent Dirichlet process prior for time-dependent mixing distributions extends the model to the dynamic setting. The methodology is used for a detailed study of relationships between maturity, age, and length for Chilipepper rockfish, using data collected over 15 years along the coast of California. Supplementary materials for this article are available online.","Chilipepper rockfish,Dependent Dirichlet process,Dynamic density estimation,Growth curves,Ordinal regression","DeYoreo, Maria@Duke University@RAND Corporation::Kottas, Athanasios@University of California System@University of California Santa Cruz"
BNP-Seq: Bayesian Nonparametric Differential Expression Analysis of Sequencing Count Data,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,67,"We perform differential expression analysis of high-throughput sequencing count data under a Bayesian nonparametric framework, removing sophisticated ad hoc pre-processing steps commonly required in existing algorithms. We propose to use the gamma (beta) negative binomial process, which takes into account different sequencing depths using sample-specific negative binomial probability (dispersion) parameters, to detect differentially expressed genes by comparing the posterior distributions of gene-specific negative binomial dispersion (probability) parameters. These model parameters are inferred by borrowing statistical strength across both the genes and samples. Extensive experiments on both simulated and real-world RNA sequencing count data show that the proposed differential expression analysis algorithms clearly outperform previously proposed ones in terms of the areas under both the receiver operating characteristic and precision-recall curves. Supplementary materials for this article are available online.","Markov chain Monte Carlo,Negative binomial processes,Over-dispersion,RNA-Seq,Symmetric Kullback-Leibler divergence","Dadaneh, Siamak Zamani@Texas A&M University College Station@Texas A&M University System::Qian, Xiaoning@Texas A&M University College Station@Texas A&M University System@University of Texas Austin@University of Texas System::Zhou, Mingyuan@University of Texas Austin@University of Texas System"
Pair Copula Constructions for Insurance Experience Rating,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,38,"In nonlife insurance, insurers use experience rating to adjust premiums to reflect policyholders' previous claim experience. Performing prospective experience rating can be challenging when the claim distribution is complex. For instance, insurance claims are semicontinuous in that a fraction of zeros is often associated with an otherwise positive continuous outcome from a right-skewed and long-tailed distribution. Practitioners use credibility premium that is a special form of the shrinkage estimator in the longitudinal data framework. However, the linear predictor is not informative especially when the outcome follows a mixed distribution. In this article, we introduce a mixed vine pair copula construction framework for modeling semicontinuous longitudinal claims. In the proposed framework, a two-component mixture regression is employed to accommodate the zero inflation and thick tails in the claim distribution. The temporal dependence among repeated observations is modeled using a sequence of bivariate conditional copulas based on a mixed D-vine. We emphasize that the resulting predictive distribution allows insurers to incorporate past experience into future premiums in a nonlinear fashion and the classic linear predictor can be viewed as a nested case. In the application, we examine a unique claims dataset of government property insurance from the state of Wisconsin. Due to the discrepancies between the claim and premium distributions, we employ an ordered Lorenz curve to evaluate the predictive performance. We show that the proposed approach offers substantial opportunities for separating risks and identifying profitable business when compared with alternative experience rating methods. Supplementary materials for this article are available online.","Government insurance,Mixed D-vine,Mixture regression,Predictive distribution,Zero inflation","Shi, Peng@University of Wisconsin System@University of Wisconsin Madison::Yang, Lu@University of Amsterdam"
Mission CO(2)ntrol: A Statistical Scientist's Role in Remote Sensing of Atmospheric Carbon Dioxide,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,89,"Too much carbon dioxide (CO2) in the atmosphere is a threat to long-term sustainability of Earth's ecosystem. Atmospheric CO2 is a leading greenhouse gas that has increased to levels not seen since the middle Pliocene (approximately 3.6 million years ago). One of the US National Aeronautics Space Administration's (NASA) remote sensing missions is the Orbiting Carbon Observatory-2, whose principal science objective is to estimate the global geographic distribution of CO2 sources and sinks at Earth's surface, through time. This starts with raw radiances (Level 1), moves on to retrievals of the atmospheric state (Level 2), from which maps of gap-filled and de-noised geophysical variables and their uncertainties are made (Level 3). With the aid of a model of transport in the atmosphere, CO2 fluxes (Level 4) can be obtained from Level 2 data directly or possibly through Level 3. Decisions about how to mitigate or manage CO2 could be thought of as Level 5. Hierarchical statistical modeling is used to qualify and quantify the uncertainties at each level. Supplementary materials for this article are available online.","Climate change,CO2 flux inversion,Hierarchical statistical model,Kriging,Orbiting Carbon Observatory-2,Predictive distribution","Cressie, Noel@University of Wollongong"
Mission CO(2)ntrol: A Statistical Scientist's Role in Remote Sensing of Atmospheric Carbon Dioxide Comment,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,12,no abstract,,"Kuhnert, Petra M.@Commonwealth Scientific & Industrial Research Organisation (CSIRO)"
Mission CO(2)ntrol: A Statistical Scientist's Role in Remote Sensing of Atmospheric Carbon Dioxide Comment,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,16,"Based on the measurements of the OCO-2 satellite, Noel Cressie addresses a particularly hard challenge for Earth observation, arguably an extreme case in remote sensing. He is one of the very few who has expertise in most of the processing chain and his article brilliantly discusses the diverse underlying statistical challenges. In this comment, we provide a complementary view of the topic to qualify its prospects as drawn by N. Cressie at the end of his article. We first summarize the motivation of OCO-2-type programs; we then expose the corresponding challenges before discussing the prospects.","Atmospheric inversion,Climate change,Orbiting carbon observatory-2","Chevallier, Frederic@Universite Paris Saclay@CEA@Centre National de la Recherche Scientifique (CNRS)@Universite Paris Saclay (ComUE)@Universite de Versailles Saint-Quentin-En-Yvelines::Breon, Francois-Marie@Universite Paris Saclay@CEA@Centre National de la Recherche Scientifique (CNRS)@Universite Paris Saclay (ComUE)@Universite de Versailles Saint-Quentin-En-Yvelines"
Maximum Rank Reproducibility: A Nonparametric Approach to Assessing Reproducibility in Replicate Experiments,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,19,"The identification of reproducible signals from the results of replicate high-throughput experiments is an important part of modern biological research. Often little is known about the dependence structure and the marginal distribution of the data, motivating the development of a nonparametric approach to assess reproducibility. The procedure, which we call the maximum rank reproducibility (MaRR) procedure, uses a maximum rank statistic to parse reproducible signals from noise without making assumptions about the distribution of reproducible signals. Because it uses the rank scale this procedure can be easily applied to a variety of data types. One application is to assess the reproducibility of RNA-seq technology using data produced by the sequencing quality control (SEQC) consortium, which coordinated a multi-laboratory effort to assess reproducibility across three RNA-seq platforms. Our results on simulations and SEQC data show that the MaRR procedure effectively controls false discovery rates, has desirable power properties, and compares well to existing methods. Supplementary materials for this article are available online.","Association,False discovery rate,Genomics,High-throughput experiment,Irreproducible discovery rate","Philtron, Daisy@Penn State University@Pennsylvania Commonwealth System of Higher Education (PCSHE)::Lyu, Yafei@Penn State University@Pennsylvania Commonwealth System of Higher Education (PCSHE)::Li, Qunhua@Penn State University@Pennsylvania Commonwealth System of Higher Education (PCSHE)::Ghosh, Debashis@University of Colorado System@Colorado School of Public Health@University of Colorado Health Science Center"
Front-Door Versus Back-Door Adjustment With Unmeasured Confounding: Bias Formulas for Front-Door and Hybrid Adjustments With Application to a Job Training Program,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,49,"We demonstrate that the front-door adjustment can be a useful alternative to standard covariate adjustments (i.e., back-door adjustments), even when the assumptions required for the front-door approach do not hold. We do this by providing asymptotic bias formulas for the front-door approach that can be compared directly to bias formulas for the back-door approach. In some cases, this allows the tightening of bounds on treatment effects. We also show that under one-sided noncompliance, the front-door approach does not rely on the use of control units. This finding has implications for the design of studies when treatment cannot be withheld from individuals (perhaps for ethical reasons). We illustrate these points with an application to the National Job Training Partnership Act Study.","Causal inference,Post-treatment,Program evaluation,Sensitivity analysis","Glynn, Adam N.@Emory University::Kashin, Konstantin@Harvard University"
On the Long-Run Volatility of Stocks,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,18,"In this article, we investigate whether or not the volatility per period of stocks is lower over longer horizons. Taking the perspective of an investor, we evaluate the predictive variance of k-period returns under different model and prior specifications. We adopt the state-space framework of Pastor and Stambaugh to model the dynamics of expected returns and evaluate the effects of prior elicitation in the resulting volatility estimates. Part of the developments includes an extension that incorporates time-varying volatilities and covariances in a constrained prior information set-up. Our conclusion for the U.S. market, under plausible prior specifications, is that stocks are less volatile in the long run. Model assessment exercises demonstrate the models and priors supporting our main conclusions are in accordance with the data. To assess the generality of the results, we extend our analysis to a number of international equity indices. Supplementary materials for this article are available online.","Covariance matrix,Dynamic models,Long-run investing,Volatility","Carvalho, Carlos M.@University of Texas Austin@University of Texas System::Lopes, Hedibert F.@Insper::McCulloch, Robert E.@University of Chicago@Arizona State University"
Cross-Screening in Observational Studies That Test Many Hypotheses,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,33,"We discuss observational studies that test many causal hypotheses, either hypotheses about many outcomes or many treatments. To be credible an observational study that tests many causal hypotheses must demonstrate that its conclusions are neither artifacts of multiple testing nor of small biases from nonrandom treatment assignment. In a sense that needs to be defined carefully, hidden within a sensitivity analysis for nonrandom assignment is an enormous correction for multiple testing: In the absence of bias, it is extremely improbable that multiple testing alone would create an association insensitive to moderate biases. We propose a new strategy called cross-screening, different from but motivated by recent work of Bogomolov and Heller on replicability. Cross-screening splits the data in half at random, uses the first half to plan a study carried out on the second half, then uses the second half to plan a study carried out on the first half, and reports the more favorable conclusions of the two studies correcting using the Bonferroni inequality for having done two studies. If the two studies happen to concur, then they achieve Bogomolov-Heller replicability; however, importantly, replicability is not required for strong control of the family-wise error rate, and either study alone suffices for firm conclusions. In randomized studies with just a few null hypotheses, cross-screening is not an attractive method when compared with conventional methods of multiplicity control. However, cross-screening has substantially higher power when hundreds or thousands of hypotheses are subjected to sensitivity analyses in an observational study of moderate size. We illustrate the technique by comparing 46 biomarkers in individuals who consume large quantities of fish versus little or no fish. The R package CrossScreening on CRAN implements the cross-screening method. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.","Causal inference,Design sensitivity,Observational study,Replicability,Sample splitting,Sensitivity analysis","Zhao, Qingyuan@University of Pennsylvania::Small, Dylan S.@University of Pennsylvania::Rosenbaum, Paul R.@University of Pennsylvania"
Distribution-Free Predictive Inference for Regression,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,34,"We develop a general framework for distribution-free predictive inference in regression, using conformal inference. The proposed methodology allows for the construction of a prediction band for the response variable using any estimator of the regression function. The resulting prediction band preserves the consistency properties of the original estimator under standard assumptions, while guaranteeing finite-sample marginal coverage even when these assumptions do not hold. We analyze and compare, both empirically and theoretically, the two major variants of our conformal framework: full conformal inference and split conformal inference, along with a related jackknife method. These methods offer different tradeoffs between statistical accuracy (length of resulting prediction intervals) and computational efficiency. As extensions, we develop a method for constructing valid in-sample prediction intervals called rank-one-out conformal inference, which has essentially the same computational efficiency as split conformal inference. We also describe an extension of our procedures for producing prediction bands with locally varying length, to adapt to heteroscedasticity in the data. Finally, we propose a model-free notion of variable importance, called leave-one-covariate-out or LOCO inference. Accompanying this article is an R package conformalInference that implements all of the proposals we have introduced. In the spirit of reproducibility, all of our empirical results can also be easily (re)generated using this package.","Distribution-free,Model misspecification,Prediction band,Regression,Variable importance","Lei, Jing@Carnegie Mellon University::G'Sell, Max@Carnegie Mellon University::Rinaldo, Alessandro@Carnegie Mellon University::Tibshirani, Ryan J.@Carnegie Mellon University::Wasserman, Larry@Carnegie Mellon University"
Quantitative Evaluation of the Trade-Off of Strengthened Instruments and Sample Size in Observational Studies,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,55,"Weak instruments produce causal inferences that are sensitive to small failures of the assumptions underlying an instrumental variable, so strong instruments are preferred. The possibility of strengthening an instrument at the price of a reduced sample size has been proposed in the statistical literature and used in the medical literature, but there has not been a theoretical study of the trade-off of instrument strength and sample size. This trade-off and related questions are examined using the Bahadur efficiency of a test or a sensitivity analysis. A moderate increase in instrument strength is worth more than an enormous increase in sample size. This is true with a flawless instrument, and the difference is more pronounced when allowance is made for small unmeasured biases in the instrument. A new method of strengthening an instrument is proposed: it discards half the sample to learn empirically where the instrument is strong, then discards part of the remaining half to avoid areas where the instrument is weak; however, the gains in instrument strength can more than compensate for the loss of sample size. The example is drawn from a study of the effectiveness of high-level neonatal intensive care units in saving the lives of premature infants.","Bahadur efficiency,Causal effects,Design sensitivity,Instrumental variables,Observational studies,Sensitivity analysis","Ertefaie, Ashkan@University of Rochester::Small, Dylan S.@University of Pennsylvania::Rosenbaum, Paul R.@University of Pennsylvania"
Quasi-Likelihood Estimation of a Censored Autoregressive Model With Exogenous Variables,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,24,"Maximum likelihood estimation of a censored autoregressive model with exogenous variables (CARX) requires computing the conditional likelihood of blocks of data of variable dimensions. As the random block dimension generally increases with the censoring rate, maximum likelihood estimation becomes quickly numerically intractable with increasing censoring. We introduce a new estimation approach using the complete-incomplete data framework with the complete data comprising the observations were there no censoring. We introduce a system of unbiased estimating equations motivated by the complete-data score vector, for estimating a CARX model. The proposed quasi-likelihood method reduces to maximum likelihood estimation when there is no censoring, and it is computationally efficient. We derive the consistency and asymptotic normality of the quasi-likelihood estimator, under mild regularity conditions. We illustrate the efficacy of the proposed method by simulations and a real application on phosphorus concentration in river water.","Estimating equation,Maximum likelihood estimation,Regression model,Time series","Wang, Chao@University of Iowa::Chan, Kung-Sik@University of Iowa"
A Weighted Edge-Count Two-Sample Test for Multivariate and Object Data,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,14,"Two-sample tests for multivariate data and non-Euclidean data are widely used in many fields. Parametric tests are mostly restrained to certain types of data that meets the assumptions of the parametric models. In this article, we study a nonparametric testing procedure that uses graphs representing the similarity among observations. It can be applied to any data types as long as an informative similarity measure on the sample space can be defined. The classic test based on a similarity graph has a problem when the two sample sizes are different. We solve the problem by applying appropriate weights to different components of the classic test statistic. The new test exhibits substantial power gains in simulation studies. Its asymptotic permutation null distribution is derived and shown to work well under finite samples, facilitating its application to large datasets. The new test is illustrated through an analysis on a real dataset of network data.","Nonparametric test,Permutation null distribution,Similarity graph,Unequal sample sizes","Chen, Hao@University of California Davis@University of California System::Chen, Xu@Duke University::Su, Yi@University of California Davis@University of California System"
Oracle Estimation of a Change Point in High-Dimensional Quantile Regression,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,40,"In this article, we consider a high-dimensional quantile regression model where the sparsity structure may differ between two sub-populations. We develop (1)-penalized estimators of both regression coefficients and the threshold parameter. Our penalized estimators not only select covariates but also discriminate between a model with homogenous sparsity and a model with a change point. As a result, it is not necessary to know or pretest whether the change point is present, or where it occurs. Our estimator of the change point achieves an oracle property in the sense that its asymptotic distribution is the same as if the unknown active sets of regression coefficients were known. Importantly, we establish this oracle property without a perfect covariate selection, thereby avoiding the need for the minimum level condition on the signals of active covariates. Dealing with high-dimensional quantile regression with an unknown change point calls for a new proof technique since the quantile loss function is nonsmooth and furthermore the corresponding objective function is nonconvex with respect to the change point. The technique developed in this article is applicable to a general M-estimation framework with a change point, which may be of independent interest. The proposed methods are then illustrated via Monte Carlo experiments and an application to tipping in the dynamics of racial segregation. Supplementary materials for this article are available online.","High-dimensional M-estimation,LASSO,SCAD,Sparsity,Variable selection","Lee, Sokbae@London School Economics & Political Science@Columbia University@University of London::Liao, Yuan@Rutgers State University New Brunswick::Seo, Myung Hwan@Seoul National University::Shin, Youngki@McMaster University@University of Technology Sydney"
Optimal Control and Additive Perturbations Help in Estimating Ill-Posed and Uncertain Dynamical Systems,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,44,"Ordinary differential equations (ODE) are routinely calibrated on real data for estimating unknown parameters or for reverse-engineering. Nevertheless, standard statistical techniques can give disappointing results because of the complex relationship between parameters and states, which makes the corresponding estimation problem ill-posed. Moreover, ODE are mechanistic models that are prone to modeling errors, whose influences on inference are often neglected during statistical analysis. We propose a regularized estimation framework, called Tracking, which consists in adding a perturbation (L-2 function) to the original ODE. This perturbation facilitates data fitting and represents also possible model misspecifications, so that parameter estimation is done by solving a trade-off between data fidelity and model fidelity. We show that the underlying optimization problem is an optimal control problem that can be solved by the Pontryagin maximum principle for general nonlinear and partially observed ODE. The same methodology can be used for the joint estimation of finite and time-varying parameters. We show, in the case of a well-specified parametric model that our estimator is consistent and reaches the root-n rate. In addition, numerical experiments considering various sources of model misspecifications shows that Tracking still furnishes accurate estimates. Finally, we consider semiparametric estimation on both simulated data and on a real data example. Supplementary materials for this article are available online.","Asymptotics,Model uncertainty,Optimal control,Ordinary differential equation,Parametric estimation,Semiparametric estimation","Clairon, Quentin@Newcastle University - UK::Brunel, Nicolas J. -B.@Universite Paris Saclay@Universite d'Evry-Val-d'Essonne@Ecole Nationale Superieure d'Informatique pour l'Industrie et l'Entreprise (ENSIIE)@Centre National de la Recherche Scientifique (CNRS)@Universite Paris Saclay (ComUE)"
On the Use of Reproducing Kernel Hilbert Spaces in Functional Classification,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,29,"The Hajek-Feldman dichotomy establishes that two Gaussian measures are either mutually absolutely continuous with respect to each other (and hence there is a Radon-Nikodym density for each measure with respect to the other one) or mutually singular. Unlike the case of finite-dimensional Gaussian measures, there are nontrivial examples of both situations when dealing with Gaussian stochastic processes. This article provides: (a) Explicit expressions for the optimal (Bayes) rule and the minimal classification error probability in several relevant problems of supervised binary classification of mutually absolutely continuous Gaussian processes. The approach relies on some classical results in the theory of reproducing kernel Hilbert spaces (RKHS). (b) An interpretation, in terms of mutual singularity, for the so-called near perfect classification phenomenon. We show that the asymptotically optimal rule proposed by these authors can be identified with the sequence of optimal rules for an approximating sequence of classification problems in the absolutely continuous case. (c) As an application, we discuss a natural variable selection method, which essentially consists of taking the original functional data X(t), t [0, 1] to a d-dimensional marginal (X(t(1)), ..., X(t(d))), which is chosen to minimize the classification error of the corresponding Fisher's linear rule. We give precise conditions under which this discrimination method achieves the minimal classification error of the original functional problem. Supplementary materials for this article are available online.","Absolutely continuity,Mutually singular processes,Radon-Nikodym derivatives,Supervised functional classification,Variable selection","Berrendero, Jose R.@Autonomous University of Madrid::Cuevas, Antonio@Autonomous University of Madrid::Torrecilla, Jose L.@Autonomous University of Madrid@Universidad Carlos III de Madrid"
Estimation and Inference of Heterogeneous Treatment Effects using Random Forests,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,76,"Many scientific and engineering challengesranging from personalized medicine to customized marketing recommendationsrequire an understanding of treatment effect heterogeneity. In this article, we develop a nonparametric causal forest for estimating heterogeneous treatment effects that extends Breiman's widely used random forest algorithm. In the potential outcomes framework with unconfoundedness, we show that causal forests are pointwise consistent for the true treatment effect and have an asymptotically Gaussian and centered sampling distribution. We also discuss a practical method for constructing asymptotic confidence intervals for the true treatment effect that are centered at the causal forest estimates. Our theoretical results rely on a generic Gaussian theory for a large family of random forest algorithms. To our knowledge, this is the first set of results that allows any type of random forest, including classification and regression forests, to be used for provably valid statistical inference. In experiments, we find causal forests to be substantially more powerful than classical methods based on nearest-neighbor matching, especially in the presence of irrelevant covariates.","Adaptive nearest neighbors matching,Asymptotic normality,Potential outcomes,Unconfoundedness","Wager, Stefan@Stanford University::Athey, Susan@Stanford University"
Identifying Latent Structures in Restricted Latent Class Models,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,48,"This article focuses on a family of restricted latent structure models with wide applications in psychological and educational assessment, where the model parameters are restricted via a latent structure matrix to reflect prespecified assumptions on the latent attributes. Such a latent matrix is often provided by experts and assumed to be correct upon construction, yet it may be subjective and misspecified. Recognizing this problem, researchers have been developing methods to estimate the matrix from data. However, the fundamental issue of the identifiability of the latent structure matrix has not been addressed until now. The first goal of this article is to establish identifiability conditions that ensure the estimability of the structure matrix. With the theoretical development, the second part of the article proposes a likelihood-based method to estimate the latent structure from the data. Simulation studies show that the proposed method outperforms the existing approaches. We further illustrate the method through a dataset in educational assessment. Supplementary materials for this article are available online.","Cognitive diagnosis,Identifiability,Q-matrix,Restricted latent class models","Xu, Gongjun@University of Michigan System@University of Michigan::Shang, Zhuoran@University of Minnesota Twin Cities@University of Minnesota System"
"Confidence Regions for Spatial Excursion Sets From Repeated Random Field Observations, With an Application to Climate",JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,51,"The goal of this article is to give confidence regions for the excursion set of a spatial function above a given threshold from repeated noisy observations on a fine grid of fixed locations. Given an asymptotically Gaussian estimator of the target function, a pair of data-dependent nested excursion sets are constructed that are sub- and super-sets of the true excursion set, respectively, with a desired confidence. Asymptotic coverage probabilities are determined via a multiplier bootstrap method, not requiring Gaussianity of the original data nor stationarity or smoothness of the limiting Gaussian field. The method is used to determine regions in North America where the mean summer and winter temperatures are expected to increase by mid-21st century by more than 2 degrees Celsius.","Functional data,Gaussian kinematic formula,General linear model,Level sets,Multiplier bootstrap,Spatial statistics","Sommerfeld, Max@University of Gottingen::Sain, Stephan@Unknow::Schwartzman, Armin@University of California San Diego@University of California System"
A Randomized Sequential Procedure to Determine the Number of Factors,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,32,"This article proposes a procedure to estimate the number of common factors k in a static approximate factor model. The building block of the analysis is the fact that the first k eigenvalues of the covariance matrix of the data diverge, while the others stay bounded. On the grounds of this, we propose a test for the null that the ith eigenvalue diverges, using a randomized test statistic based directly on the estimated eigenvalue. The test only requires minimal assumptions on the data, and no assumptions are required on factors, loadings or idiosyncratic errors. The randomized tests are then employed in a sequential procedure to determine k. Supplementary materials for this article are available online.","Approximate factor models,Number of factors,Randomized tests","Trapani, Lorenzo@City University London"
Diagnostic Checking in Multivariate ARMA Models With Dependent Errors Using Normalized Residual Autocorrelations,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,34,"In this paper, we derive the asymptotic distribution of normalized residual empirical autocovariances and autocorrelations under weak assumptions on the noise. We propose new portmanteau statistics for vector autoregressive moving average models with uncorrelated but nonindependent innovations by using a self-normalization approach. We establish the asymptotic distribution of the proposed statistics. This asymptotic distribution is quite different from the usual chi-squared approximation used under the independent and identically distributed assumption on the noise, or the weighted sum of independent chi-squared random variables obtained under nonindependent innovations. A set of Monte Carlo experiments and an application to the daily returns of the CAC40 is presented. Supplementary materials for this article are available online.","Box-Pierce and Ljung-Box portmanteau tests,Goodness-of-fit test,Quasi-maximum likelihood estimation,Self-normalization,Weak (V)ARMA models","Mainassara, Yacouba Boubacar@Unknow::Saussereau, Bruno@Unknow"
Mixtures of g-Priors in Generalized Linear Models,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,79,"Mixtures of Zellner's g-priors have been studied extensively in linear models and have been shown to have numerous desirable properties for Bayesian variable selection and model averaging. Several extensions of g-priors to generalized linear models (GLMs) have been proposed in the literature; however, the choice of prior distribution of g and resulting properties for inference have received considerably less attention. In this article, we unify mixtures of g-priors in GLMs by assigning the truncated Compound Confluent Hypergeometric (tCCH) distribution to 1/(1 + g), which encompasses as special cases several mixtures of g-priors in the literature, such as the hyper-g, Beta-prime, truncated Gamma, incomplete inverse-Gamma, benchmark, robust, hyper-g/n, and intrinsic priors. Through an integrated Laplace approximation, the posterior distribution of 1/(1 + g) is in turn a tCCH distribution, and approximate marginal likelihoods are thus available analytically, leading to Compound Hypergeometric Information Criteria for model selection. We discuss the local geometric properties of the g-prior in GLMs and show how the desiderata for model selection proposed by Bayarri etal., such as asymptotic model selection consistency, intrinsic consistency, and measurement invariance may be used to justify the prior and specific choices of the hyper parameters. We illustrate inference using these priors and contrast them to other approaches via simulation and real data examples. The methodology is implemented in the R package BAS and freely available on CRAN. Supplementary materials for this article are available online.","Bayesian model averaging,Bayesian model selection,Hyper-g priors,Linear regression,Variable selection","Li, Yingbo@Clemson University::Clyde, Merlise A.@Duke University"
A Bayesian Machine Learning Approach for Optimizing Dynamic Treatment Regimes,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,32,"Medical therapy often consists of multiple stages, with a treatment chosen by the physician at each stage based on the patient's history of treatments and clinical outcomes. These decisions can be formalized as a dynamic treatment regime. This article describes a new approach for optimizing dynamic treatment regimes, which bridges the gap between Bayesian inference and existing approaches, like Q-learning. The proposed approach fits a series of Bayesian regression models, one for each stage, in reverse sequential order. Each model uses as a response variable the remaining payoff assuming optimal actions are taken at subsequent stages, and as covariates the current history and relevant actions at that stage. The key difficulty is that the optimal decision rules at subsequent stages are unknown, and even if these decision rules were known the relevant response variables may be counterfactual. However, posterior distributions can be derived from the previously fitted regression models for the optimal decision rules and the counterfactual response variables under a particular set of rules. The proposed approach averages over these posterior distributions when fitting each regression model. An efficient sampling algorithm for estimation is presented, along with simulation studies that compare the proposed approach with Q-learning. Supplementary materials for this article are available online.","Approximate dynamic programming,Backward induction,Bayesian additive regression trees,Gibbs sampling,Potential outcomes","Murray, Thomas A.@UTMD Anderson Cancer Center@University of Texas System::Yuan, Ying@UTMD Anderson Cancer Center@University of Texas System::Thall, Peter F.@UTMD Anderson Cancer Center@University of Texas System"
Placebo Response as a Latent Characteristic: Application to Analysis of Sequential Parallel Comparison Design Studies,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,34,"In clinical trials, placebo response can affect the inference about efficacy of the studied treatment. It is important to have a robust way to classify trial subjects with respect to their response to placebo. Simple, criterion-based classification may lead to classification error and bias the inference. The uncertainty about placebo response characteristic has to be factored into the treatment effect estimation. We propose a novel approach that views the placebo response as a latent characteristic and the study sample as an unlabeled mixture of placebo responders and placebo nonresponders. The likelihood-based methodology is used to estimate the treatment effect corrected for placebo response as defined within sequential parallel comparison design.","Clinical trials,EM algorithm,Latent variable,Placebo effect,SPCD","Rybin, Denis@Boston University::Lew, Robert@Boston University::Pencina, Michael J.@Unknow::Fava, Maurizio@Massachusetts General Hospital@VA Boston Healthcare System@Harvard University::Doros, Gheorghe@Boston University"
Tracking the Impact of Media on Voter Choice in Real Time: A Bayesian Dynamic Joint Model,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,54,"Commonly used methods of evaluating the impact of marketing communications during political elections struggle to account for respondents' exposures to these communications due to the problems associated with recall bias. In addition, they completely fail to account for the impact of mediated or earned communications, such as newspaper articles or television news, that are typically not within the control of the advertising party, nor are they effectively able to monitor consumers' perceptual responses over time. This study based on a new data collection technique using cell-phone text messaging (called real-time experience tracking or RET) offers the potential to address these weaknesses. We propose an RET-based model of the impact of communications and apply it to a unique choice situation: voting behavior during the 2010 UK general election, which was dominated by three political parties. We develop a Bayesian zero-inflated dynamic multinomial choice model that enables the joint modeling of: the interplay and dynamics associated with the individual voter's choice intentions over time, actual vote, and the heterogeneity in the exposure to marketing communications over time. Results reveal the differential impact over time of paid and earned media, demonstrate a synergy between the two, and show the particular importance of exposure valence and not just frequency, contrary to the predominant practitioner emphasis on share-of-voice metrics. Results also suggest that while earned media have a reducing impact on voting intentions as the final choice approaches, their valence continues to influence the final vote: a difference between drivers of intentions and behavior that implies that exposure valence remains critically important close to the final brand choice. Supplementary materials for this article are available online.","Advertising,Multinomial logit model,Paid and earned media,Real-time experience tracking,Zero-inflation","Pareek, Bhuvanesh@Indian Institute of Management Indore::Ghosh, Pulak@Indian Institute of Management Bangalore::Wilson, Hugh N.@Cranfield University::Macdonald, Emma K.@Cranfield University::Baines, Paul@Cranfield University"
Malware Family Discovery Using Reversible Jump MCMC Sampling of Regimes,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,28,"Malware is computer software that has either been designed or modified with malicious intent. Hundreds of thousands of new malware threats appear on the internet each day. This is made possible through reuse of known exploits in computer systems that have not been fully eradicated; existing pieces of malware can be trivially modified and combined to create new malware, which is unknown to anti-virus programs. Finding new software with similarities to known malware is therefore an important goal in cyber-security. A dynamic instruction trace of a piece of software is the sequence of machine language instructions it generates when executed. Statistical analysis of a dynamic instruction trace can help reverse engineers infer the purpose and origin of the software that generated it. Instruction traces have been successfully modeled as simple Markov chains, but empirically there are change points in the structure of the traces, with recurring regimes of transition patterns. Here, reversible jump Markov chain Monte Carlo for change point detection is extended to incorporate regime-switching, allowing regimes to be inferred from malware instruction traces. A similarity measure for malware programs based on regime matching is then used to infer the originating families, leading to compelling performance results.","Change point analysis,Dynamic instruction trace,Regime-switching,Reversible jump Markov chain Monte Carlo","Bolton, Alexander D.@Imperial College London::Heard, Nicholas A.@Imperial College London@University of Bristol"
Bayesian Semiparametric Mixed Effects Markov Models With Application to Vocalization Syntax,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,41,"Studying the neurological, genetic, and evolutionary basis of human vocal communication mechanisms using animal vocalization models is an important field of neuroscience. The datasets typically comprise structured sequences of syllables or songs produced by animals from different genotypes under different social contexts. It has been difficult to come up with sophisticated statistical methods that appropriately model animal vocal communication syntax. We address this need by developing a novel Bayesian semiparametric framework for inference in such datasets. Our approach is built on a novel class of mixed effects Markov transition models for the songs that accommodate exogenous influences of genotype and context as well as animal-specific heterogeneity. Crucial advantages of the proposed approach include its ability to provide insights into key scientific queries related to global and local influences of the exogenous predictors on the transition dynamics via automated tests of hypotheses. The methodology is illustrated using simulation experiments and the aforementioned motivating application in neuroscience. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.","Bayesian nonparametrics,Categorical sequences,Markov models,Mixed effects models,Mouse vocalization experiments","Sarkar, Abhra@University of Texas Austin@University of Texas System::Chabout, Jonathan@Unknow::Macopson, Joshua Jones@Unknow::Jarvis, Erich D.@Howard Hughes Medical Institute@Rockefeller University::Dunson, David B.@Duke University"
Fast Moment Estimation for Generalized Latent Dirichlet Models,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,32,"We develop a generalized method of moments (GMM) approach for fast parameter estimation in a new class of Dirichlet latent variable models with mixed data types. Parameter estimation via GMM has computational and statistical advantages over alternative methods, such as expectation maximization, variational inference, and Markov chain Monte Carlo. A key computational advantage of our method, Moment Estimation for latent Dirichlet models (MELD), is that parameter estimation does not require instantiation of the latent variables. Moreover, performance is agnostic to distributional assumptions of the observations. We derive population moment conditions after marginalizing out the sample-specific Dirichlet latent variables. The moment conditions only depend on component mean parameters. We illustrate the utility of our approach on simulated data, comparing results from MELD to alternative methods, and we show the promise of our approach through the application to several datasets.Supplementary materials for this article are available online.","Generalized method of moments,Latent Dirichlet allocation,Latent variables,Mixed membership model,Mixed scale data,Tensor factorization","Zhao, Shiwen@Duke University::Engelhardt, Barbara E.@Princeton University::Mukherjee, Sayan@Duke University::Dunson, David B.@Duke University"
Efficient Estimation of the Nonparametric Mean and Covariance Functions for Longitudinal and Sparse Functional Data,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,41,"We consider the estimation of mean and covariance functions for longitudinal and sparse functional data by using the full quasi-likelihood coupling a modification of the local kernel smoothing method. The proposed estimators are shown to be consistent, asymptotically normal, and semiparametrically efficient in terms of their linear functionals. Their superiority to the competitors is further illustrated numerically through simulation studies. The method is applied to analyze AIDS study and atmospheric study. Supplementary materials for this article are available online.","Curse of dimensionality,Full quasi-likelihood function,Gaussian process,Longitudinal data,Nonparametric structure,Semiparametric efficiency","Zhou, Ling@Southwestern University of Finance & Economics - China::Lin, Huazhen@Southwestern University of Finance & Economics - China::Liang, Hua@George Washington University"
Probabilities of Concurrent Extremes,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,51,"The statistical modeling of spatial extremes has been an active area of recent research with a growing domain of applications. Much of the existing methodology, however, focuses on the magnitudes of extreme events rather than on their timing. To address this gap, this article investigates the notion of extremal concurrence. Suppose that daily temperatures are measured at several synoptic stations. We say that extremes are concurrent if record maximum temperatures occur simultaneously, that is, on the same day for all stations. It is important to be able to understand, quantify, and model extremal concurrence. Under general conditions, we show that the finite sample concurrence probability converges to an asymptotic quantity, deemed extremal concurrence probability. Using Palm calculus, we establish general expressions for the extremal concurrence probability through the max-stable process emerging in the limit of the component-wise maxima of the sample. Explicit forms of the extremal concurrence probabilities are obtained for various max-stable models and several estimators are introduced. In particular, we prove that the pairwise extremal concurrence probability for max-stable vectors is precisely equal to the Kendall's . The estimators are evaluated from simulations and applied to study temperature extremes in the United States. Results demonstrate that concurrence probability can be used to study, for example, the effect of global climate phenomena such as the El Nino Southern Oscillation (ENSO) or global warming on the spatial structure and areal impact of extremes.","Kendall's,Max-stable process,Poisson point process,Slyvniak formula,Temperature extremes","Dombry, Clement@Centre National de la Recherche Scientifique (CNRS)::Ribatet, Mathieu@Universite de Montpellier::Stoev, Stilian@University of Michigan System@University of Michigan"
Linear Hypothesis Testing in Dense High-Dimensional Linear Models,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,45,"We propose a methodology for testing linear hypothesis in high-dimensional linear models. The proposed test does not impose any restriction on the size of the model, that is, model sparsity or the loading vector representing the hypothesis. Providing asymptotically valid methods for testing general linear functions of the regression parameters in high-dimensions is extremely challengingespecially without making restrictive or unverifiable assumptions on the number of nonzero elements. We propose to test the moment conditions related to the newly designed restructured regression, where the inputs are transformed and augmented features. These new features incorporate the structure of the null hypothesis directly. The test statistics are constructed in such a way that lack of sparsity in the original model parameter does not present a problem for the theoretical justification of our procedures. We establish asymptotically exact control on Type I error without imposing any sparsity assumptions on model parameter or the vector representing the linear hypothesis. Our method is also shown to achieve certain optimality in detecting deviations from the null hypothesis. We demonstrate the favorable finite-sample performance of the proposed methods, via a number of numerical and a real data example. Supplementary materials for this article are available online.","Dantzig,High-dimensional linear models,Inference,Lasso,Linear Testing,Nonsparse models","Zhu, Yinchu@University of California San Diego@University of California System::Bradic, Jelena@University of California San Diego@University of California System"
Over-Dispersed Age-Period-Cohort Models,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,40,"We consider inference and forecasting for aggregate data organized in a two-way table with age and cohort as indices, but without measures of exposure. This is modeled using a Poisson likelihood with an age-period-cohort structure for the mean while allowing for over-dispersion. We propose a repetitive structure that keeps the dimension of the table fixed while increasing the latent exposure. For this, we use a class of infinitely divisible distributions which include a variety of compound Poisson models and Poisson mixture models. This results in asymptotic F inference and t forecast distributions.","Chain-ladder model,Forecasting,Generalized linear model,Inference,Infinitely divisible,Two-way table","Harnau, Jonas@University of Oxford::Nielsen, Bent@University of Oxford"
Sparse Pairwise Likelihood Estimation for Multivariate Longitudinal Mixed Models,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,39,"It is becoming increasingly common in longitudinal studies to collect and analyze data on multiple responses. For example, in the social sciences we may be interested in uncovering the factors driving mental health of individuals over time, where mental health is measured using a set of questionnaire items. One approach to analyzing such multi-dimensional data is multivariate mixed models, an extension of the standard univariate mixed model to handle multiple responses. Estimating multivariate mixed models presents a considerable challenge however, let alone performing variable selection to uncover which covariates are important in driving each response. Motivated by composite likelihood ideas, we propose a new approach for estimation and fixed effects selection in multivariate mixed models, called approximate pairwise likelihood estimation and shrinkage (APLES). The method works by constructing a quadratic approximation to each term in the pairwise likelihood function, and then augmenting this approximate pairwise likelihood with a penalty that encourages both individual and group coefficient sparsity. This leads to a relatively fast method of selection, as we can use coordinate ascent type methods to then construct the full regularization path for the model. Our method is the first to extend penalized likelihood estimation to multivariate generalized linear mixed models. We show that the APLES estimator attains a composite likelihood version of the oracle property. We propose a new information criterion for selecting the tuning parameter, which employs a dynamic model complexity penalty to facilitate aggressive shrinkage, and demonstrate that it asymptotically leads to selection consistency, that is, leads to the true model being selected. A simulation study demonstrates that the APLES estimator outperforms several univariate selection methods based on analyzing each outcome separately. Supplementary materials for this article are available online.","Composite likelihood,LASSO,Mixed models,Multivariate longitudinal data,Pairwise fitting,Penalized likelihood,Variable selection","Hui, Francis K. C.@Australian National University::Mueller, Samuel@University of Sydney::Welsh, A. H.@Australian National University"
Sparsity Oriented Importance Learning for High-Dimensional Linear Regression,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,57,"With now well-recognized nonnegligible model selection uncertainty, data analysts should no longer be satisfied with the output of a single final model from a model selection process, regardless of its sophistication. To improve reliability and reproducibility in model choice, one constructive approach is to make good use of a sound variable importance measure. Although interesting importance measures are available and increasingly used in data analysis, little theoretical justification has been done. In this article, we propose a new variable importance measure, sparsity oriented importance learning (SOIL), for high-dimensional regression from a sparse linear modeling perspective by taking into account the variable selection uncertainty via the use of a sensible model weighting. The SOIL method is theoretically shown to have the inclusion/exclusion property: When the model weights are properly around the true model, the SOIL importance can well separate the variables in the true model from the rest. In particular, even if the signal is weak, SOIL rarely gives variables not in the true model significantly higher important values than those in the true model. Extensive simulations in several illustrative settings and real-data examples with guided simulations show desirable properties of the SOIL importance in contrast to other importance measures. Supplementary materials for this article are available online.","Adaptive regression by mixing,Model averaging,Reliability and reproducibility,Variable importance","Ye, Chenglong@University of Minnesota Twin Cities@University of Minnesota System::Yang, Yi@McGill University::Yang, Yuhong@University of Minnesota Twin Cities@University of Minnesota System"
"Scalable Bayesian Modeling, Monitoring, and Analysis of Dynamic Network Flow Data",JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,38,"Traffic flow count data in networks arise in many applications, such as automobile or aviation transportation, certain directed social network contexts, and Internet studies. Using an example of Internet browser traffic flow through site-segments of an international news website, we present Bayesian analyses of two linked classes of models which, in tandem, allow fast, scalable, and interpretable Bayesian inference. We first develop flexible state-space models for streaming count data, able to adaptively characterize and quantify network dynamics efficiently in real-time. We then use these models as emulators of more structured, time-varying gravity models that allow formal dissection of network dynamics. This yields interpretable inferences on traffic flow characteristics, and on dynamics in interactions among network nodes. Bayesian monitoring theory defines a strategy for sequential model assessment and adaptation in cases when network flow data deviate from model-based predictions. Exploratory and sequential monitoring analyses of evolving traffic on a network of web site-segments in e-commerce demonstrate the utility of this coupled Bayesian emulation approach to analysis of streaming network count data.","Bayesian model emulation,Decouple/recouple,Dynamic gravity model,Dynamic network flow model,Monitoring and anomaly detection","Chen, Xi@Duke University::Irie, Kaoru@University of Tokyo::Banks, David@Duke University::Haslinger, Robert@Unknow::Thomas, Jewell@Unknow::West, Mike@Duke University"
Analysis of Gap Times Based on Panel Count Data With Informative Observation Times and Unknown Start Time,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,29,"In biomedical studies, one is often interested in repeat events with longitudinal observations occurring only intermittently, resulting in panel count data. The first stage of labor, measured through unit-increments of cervical dilation in pregnant women, provides such an example. Obstetricians are interested in assessing the gap time distribution of per-unit increments of cervical dilation for better management of labor process. Typically, only intermittent medical examinations for cervical dilation occur after (already dilated) women get admitted to hospital. The observation frequency is very likely correlated to how fast/slow she dilates. Thus, one could view such data as panel count data with informative observation times and unknown start time. Here, we propose semiparametric proportional rate models for the event process and the observation process, with a multiplicative subject-specific frailty variable capturing the correlation between the two processes. Inference procedures for the gap times between consecutive events are proposed when the start times are known as well when unknown, using likelihood-based approach and estimating equations. The methodology is assessed through simulation study and through large sample property. A detailed analysis using the proposed methods is applied to data from two studies: the Collaborative Perinatal Project and the Consortium on Safe Labor. Supplementary materials for this article are available online.","Frailty,Nonhomogenous Poisson process,Semiparametric,Unknown start time","Ma, Ling@NIH Eunice Kennedy Shriver National Institute of Child Health & Human Development (NICHD)@National Institutes of Health (NIH) - USA::Sundaram, Rajeshwari@NIH Eunice Kennedy Shriver National Institute of Child Health & Human Development (NICHD)@National Institutes of Health (NIH) - USA"
Block-Diagonal Covariance Selection for High-Dimensional Gaussian Graphical Models,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,42,"Gaussian graphical models are widely used to infer and visualize networks of dependencies between continuous variables. However, inferring the graph is difficult when the sample size is small compared to the number of variables. To reduce the number of parameters to estimate in the model, we propose a nonasymptotic model selection procedure supported by strong theoretical guarantees based on an oracle type inequality and a minimax lower bound. The covariance matrix of the model is approximated by a block-diagonal matrix. The structure of this matrix is detected by thresholding the sample covariance matrix, where the threshold is selected using the slope heuristic. Based on the block-diagonal structure of the covariance matrix, the estimation problem is divided into several independent problems: subsequently, the network of dependencies between variables is inferred using the graphical lasso algorithm in each block. The performance of the procedure is illustrated on simulated data. An application to a real gene expression dataset with a limited sample size is also presented: the dimension reduction allows attention to be objectively focused on interactions among smaller subsets of genes, leading to a more parsimonious and interpretable modular network. Supplementary materials for this article are available online.","Adaptive minimax theory,Graphical lasso,Network inference,Nonasymptotic model selection,Variable selection","Devijver, Emilie@KU Leuven::Gallopin, Melina@University of Paris Descartes@Centre National de la Recherche Scientifique (CNRS)@CNRS - National Institute for Mathematical Sciences (INSMI)@Universite Sorbonne Paris Cite-USPC (ComUE)"
Error Variance Estimation in Ultrahigh-Dimensional Additive Models,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,22,"Error variance estimation plays an important role in statistical inference for high-dimensional regression models. This article concerns with error variance estimation in high-dimensional sparse additive model. We study the asymptotic behavior of the traditional mean squared errors, the naive estimate of error variance, and show that it may significantly underestimate the error variance due to spurious correlations that are even higher in nonparametric models than linear models. We further propose an accurate estimate for error variance in ultrahigh-dimensional sparse additive model by effectively integrating sure independence screening and refitted cross-validation techniques. The root n consistency and the asymptotic normality of the resulting estimate are established. We conduct Monte Carlo simulation study to examine the finite sample performance of the newly proposed estimate. A real data example is used to illustrate the proposed methodology. Supplementary materials for this article are available online.","Feature screening,Refitted cross-validation,Sparse additive model,Variance estimation","Chen, Zhao@Penn State University@Pennsylvania Commonwealth System of Higher Education (PCSHE)@Princeton University::Fan, Jianqing@Fudan University@Princeton University::Li, Runze@Penn State University@Pennsylvania Commonwealth System of Higher Education (PCSHE)"
Multiple Testing of Submatrices of a Precision Matrix With Applications to Identification of Between Pathway Interactions,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,44,"Making accurate inference for gene regulatory networks, including inferring about pathway-by-pathway interactions, is an important and difficult task. Motivated by such genomic applications, we consider multiple testing for conditional dependence between subgroups of variables. Under a Gaussian graphical model framework, the problem is translated into simultaneous testing for a collection of submatrices of a high-dimensional precision matrix with each submatrix summarizing the dependence structure between two subgroups of variables.A novel multiple testing procedure is proposed and both theoretical and numerical properties of the procedure are investigated. Asymptotic null distribution of the test statistic for an individual hypothesis is established and the proposed multiple testing procedure is shown to asymptotically control the false discovery rate (FDR) and false discovery proportion (FDP) at the prespecified level under regularity conditions. Simulations show that the procedure works well in controlling the FDR and has good power in detecting the true interactions. The procedure is applied to a breast cancer gene expression study to identify between pathway interactions. Supplementary materials for this article are available online.","Between pathway interactions,Conditional dependence,Covariance structure,False discovery proportion,False discovery rate,Gaussian graphical model,Multiple testing,Precision matrix,Testing submatrices","Xia, Yin@Fudan University::Cai, Tianxi@Harvard T.H. Chan School of Public Health@Harvard University::Cai, T. Tony@University of Pennsylvania"
Mixture Models With a Prior on the Number of Components,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,113,"A natural Bayesian approach for mixture models with an unknown number of components is to take the usual finite mixture model with symmetric Dirichlet weights, and put a prior on the number of componentsthat is, to use a mixture of finite mixtures (MFM). The most commonly used method of inference for MFMs is reversible jump Markov chain Monte Carlo, but it can be nontrivial to design good reversible jump moves, especially in high-dimensional spaces. Meanwhile, there are samplers for Dirichlet process mixture (DPM) models that are relatively simple and are easily adapted to new applications. It turns out that, in fact, many of the essential properties of DPMs are also exhibited by MFMsan exchangeable partition distribution, restaurant process, random measure representation, and stick-breaking representationand crucially, the MFM analogues are simple enough that they can be used much like the corresponding DPM properties. Consequently, many of the powerful methods developed for inference in DPMs can be directly applied to MFMs as well; this simplifies the implementation of MFMs and can substantially improve mixing. We illustrate with real and simulated data, including high-dimensional gene expression data used to discriminate cancer subtypes. Supplementary materials for this article are available online.","Bayesian,Clustering,Density estimation,Model selection,Nonparametric","Miller, Jeffrey W.@Harvard University::Harrison, Matthew T.@Brown University"
On Inverse Probability Weighting for Nonmonotone Missing at Random Data,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,48,"The development of coherent missing data models to account for nonmonotone missing at random (MAR) data by inverse probability weighting (IPW) remains to date largely unresolved. As a consequence, IPW has essentially been restricted for use only in monotone MAR settings. We propose a class of models for nonmonotone missing data mechanisms that spans the MAR model, while allowing the underlying full data law to remain unrestricted. For parametric specifications within the proposed class, we introduce an unconstrained maximum likelihood estimator for estimating the missing data probabilities which is easily implemented using existing software. To circumvent potential convergence issues with this procedure, we also introduce a constrained Bayesian approach to estimate the missing data process which is guaranteed to yield inferences that respect all model restrictions. The efficiency of standard IPW estimation is improved by incorporating information from incomplete cases through an augmented estimating equation which is optimal within a large class of estimating equations. We investigate the finite-sample properties of the proposed estimators in extensive simulations and illustrate the new methodology in an application evaluating key correlates of preterm delivery for infants born to HIV-infected mothers in Botswana, Africa. Supplementary materials for this article are available online.","Augmented IPW,Bayes,Nonmonotone missing at random data","Sun, BaoLuo@Harvard T.H. Chan School of Public Health@Harvard University::Tchetgen, Eric J. Tchetgen@Harvard T.H. Chan School of Public Health@Harvard University"
Embracing the Blessing of Dimensionality in Factor Models,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,26,"Factor modeling is an essential tool for exploring intrinsic dependence structures among high-dimensional random variables. Much progress has been made for estimating the covariance matrix from a high-dimensional factor model. However, the blessing of dimensionality has not yet been fully embraced in the literature: much of the available data are often ignored in constructing covariance matrix estimates. If our goal is to accurately estimate a covariance matrix of a set of targeted variables, shall we employ additional data, which are beyond the variables of interest, in the estimation? In this article, we provide sufficient conditions for an affirmative answer, and further quantify its gain in terms of Fisher information and convergence rate. In fact, even an oracle-like result (as if all the factors were known) can be achieved when a sufficiently large number of variables is used. The idea of using data as much as possible brings computational challenges. A divide-and-conquer algorithm is thus proposed to alleviate the computational burden, and also shown not to sacrifice any statistical accuracy in comparison with a pooled analysis. Simulation studies further confirm our advocacy for the use of full data, and demonstrate the effectiveness of the above algorithm. Our proposal is applied to a microarray data example that shows empirical benefits of using more data. Supplementary materials for this article are available online.","Asymptotic normality,Auxiliary data,Divide-and-conquer,Factor model,Fisher information,High-dimensionality","Li, Quefeng@University of North Carolina School of Medicine@University of North Carolina@University of North Carolina Chapel Hill::Cheng, Guang@Purdue University@Purdue University System::Fan, Jianqing@Fudan University@Princeton University::Wang, Yuyan@Princeton University"
Balancing Covariates via Propensity Score Weighting,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,42,"Covariate balance is crucial for unconfounded descriptive or causal comparisons. However, lack of balance is common in observational studies. This article considers weighting strategies for balancing covariates. We define a general class of weightsthe balancing weightsthat balance the weighted distributions of the covariates between treatment groups. These weights incorporate the propensity score to weight each group to an analyst-selected target population. This class unifies existing weighting methods, including commonly used weights such as inverse-probability weights as special cases. General large-sample results on nonparametric estimation based on these weights are derived. We further propose a new weighting scheme, the overlap weights, in which each unit's weight is proportional to the probability of that unit being assigned to the opposite group. The overlap weights are bounded, and minimize the asymptotic variance of the weighted average treatment effect among the class of balancing weights. The overlap weights also possess a desirable small-sample exact balance property, based on which we propose a new method that achieves exact balance for means of any selected set of covariates. Two applications illustrate these methods and compare them with other approaches.","Balancing weights,Causal inference,Clinical equipoise,Confounding,Exact balance,Overlap weights","Li, Fan@Duke University::Morgan, Kari Lock@Penn State University@Pennsylvania Commonwealth System of Higher Education (PCSHE)::Zaslavsky, Alan M.@Harvard University"
Correlated Random Measures,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,65,"We develop correlated random measures, random measures where the atom weights can exhibit a flexible pattern of dependence, and use them to develop powerful hierarchical Bayesian nonparametric models. Hierarchical Bayesian nonparametric models are usually built from completely random measures, a Poisson-process-based construction in which the atom weights are independent. Completely random measures imply strong independence assumptions in the corresponding hierarchical model, and these assumptions are often misplaced in real-world settings. Correlated random measures address this limitation. They model correlation within the measure by using a Gaussian process in concert with the Poisson process. With correlated random measures, for example, we can develop a latent feature model for which we can infer both the properties of the latent features and their dependency pattern. We develop several other examples as well. We study a correlated random measure model of pairwise count data. We derive an efficient variational inference algorithm and show improved predictive performance on large datasets of documents, web clicks, and electronic health records. Supplementary materials for this article are available online.","Bayesian nonparametrics,Gaussian processes,Health records,Hierarchical models,Poisson processes,Random measures","Ranganath, Rajesh@Princeton University::Blei, David M.@Columbia University"
The Spike-and-Slab LASSO,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,36,"Despite the wide adoption of spike-and-slab methodology for Bayesian variable selection, its potential for penalized likelihood estimation has largely been overlooked. In this article, we bridge this gap by cross-fertilizing these two paradigms with the Spike-and-Slab LASSO procedure for variable selection and parameter estimation in linear regression. We introduce a new class of self-adaptive penalty functions that arise from a fully Bayes spike-and-slab formulation, ultimately moving beyond the separable penalty framework. A virtue of these nonseparable penalties is their ability to borrow strength across coordinates, adapt to ensemble sparsity information and exert multiplicity adjustment. The Spike-and-Slab LASSO procedure harvests efficient coordinate-wise implementations with a path-following scheme for dynamic posterior exploration. We show on simulated data that the fully Bayes penalty mimics oracle performance, providing a viable alternative to cross-validation. We develop theory for the separable and nonseparable variants of the penalty, showing rate-optimality of the global mode as well as optimal posterior concentration when p > n. Supplementary materials for this article are available online.","High-dimensional regression,LASSO,Penalized likelihood,Posterior concentration,Spike-and-Slab,Variable selection","Rockova, Veronika@University of Chicago::George, Edward I.@University of Pennsylvania"
Group Regularized Estimation Under Structural Hierarchy,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,37,"Variable selection for models including interactions between explanatory variables often needs to obey certain hierarchical constraints. Weak or strong structural hierarchy requires that the existence of an interaction term implies at least one or both associated main effects to be present in the model. Lately, this problem has attracted a lot of attention, but existing computational algorithms converge slow even with a moderate number of predictors. Moreover, in contrast to the rich literature on ordinary variable selection, there is a lack of statistical theory to show reasonably low error rates of hierarchical variable selection. This work investigates a new class of estimators that make use of multiple group penalties to capture structural parsimony. We show that the proposed estimators enjoy sharp rate oracle inequalities, and give the minimax lower bounds in strong and weak hierarchical variable selection. A general-purpose algorithm is developed with guaranteed convergence and global optimality. Simulations and real data experiments demonstrate the efficiency and efficacy of the proposed approach. Supplementary materials for this article are available online.","Big data computation,Hierarchical variable selection,Minimax optimality,Multi-regularization","She, Yiyuan@Florida State University@State University System of Florida::Wang, Zhifeng@Florida State University@State University System of Florida::Jiang, He@Florida State University@State University System of Florida"
Multi-Armed Bandit for Species Discovery: A Bayesian Nonparametric Approach,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,53,"Let (P-1, ..., P-J) denote J populations of animals from distinct regions. A priori, it is unknown which species are present in each region and what are their corresponding frequencies. Species are shared among populations and each species can be present in more than one region with its frequency varying across populations. In this article, we consider the problem of sequentially sampling these populations to observe the greatest number of different species. We adopt a Bayesian nonparametric approach and endow (P-1, ..., P-J) with a hierarchical Pitman-Yor process prior. As a consequence of the hierarchical structure, the J unknown discrete probability measures share the same support, that of their common random base measure. Given this prior choice, we propose a sequential rule that, at every time step, given the information available up to that point, selects the population from which to collect the next observation. Rather than picking the population with the highest posterior estimate of producing a new value, the proposed rule includes a Thompson sampling step to better balance the exploration-exploitation trade-off. We also propose an extension of the algorithm to deal with incidence data, where multiple observations are collected in a time period. The performance of the proposed algorithms is assessed through a simulation study and compared to three other strategies. Finally, we compare these algorithms using a dataset of species of trees, collected from different plots in South America. Supplementary materials for this article are available online.","Bayesian nonparametric statistic,Discovery probability,Hierarchical Pitman-Yor process,Multi-armed bandit,Species sampling models,Thompson sampling","Battiston, Marco@Bocconi University::Favaro, Stefano@University of Turin::Teh, Yee Whye@University of Oxford"
Factor Copula Models for Replicated Spatial Data,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,43,"We propose a new copula model that can be used with replicated spatial data. Unlike the multivariate normal copula, the proposed copula is based on the assumption that a common factor exists and affects the joint dependence of all measurements of the process. Moreover, the proposed copula can model tail dependence and tail asymmetry. The model is parameterized in terms of a covariance function that may be chosen from the many models proposed in the literature, such as the Matern model. For some choice of common factors, the joint copula density is given in closed form and therefore likelihood estimation is very fast. In the general case, one-dimensional numerical integration is needed to calculate the likelihood, but estimation is still reasonably fast even with large datasets. We use simulation studies to show the wide range of dependence structures that can be generated by the proposed model with different choices of common factors. We apply the proposed model to spatial temperature data and compare its performance with some popular geostatistics models. Supplementary materials for this article are available online.","Copula,Heavy tails,Non-Gaussian random field,Spatial statistics,Tail asymmetry","Krupskii, Pavel@King Abdullah University of Science & Technology::Huser, Raphael@King Abdullah University of Science & Technology::Genton, Marc G.@King Abdullah University of Science & Technology"
"A Potential Tale of Two-by-Two Tables From Completely Randomized Experiments (vol 111, pg 157, 2016)",JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,3,no abstract,,"Ding, P.@Unknow::Dasgupta, T.@Unknow"
"Bayesian Phase I/II Biomarker-based Dose Finding for Precision Medicine with Molecularly Targeted Agents (vol 112, pg 508, 2017)",JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,1,no abstract,,"Guo, Beibei@UTMD Anderson Cancer Center@University of Texas System::Yuan, Ying@UTMD Anderson Cancer Center@University of Texas System"
Bayesian Regression Trees for High-Dimensional Prediction and Variable Selection,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,35,"Decision tree ensembles are an extremely popular tool for obtaining high-quality predictions in nonparametric regression problems. Unmodified, however, many commonly used decision tree ensemble methods do not adapt to sparsity in the regime in which the number of predictors is larger than the number of observations. A recent stream of research concerns the construction of decision tree ensembles that are motivated by a generative probabilistic model, the most influential method being the Bayesian additive regression trees (BART) framework. In this article, we take a Bayesian point of view on this problem and show how to construct priors on decision tree ensembles that are capable of adapting to sparsity in the predictors by placing a sparsity-inducing Dirichlet hyperprior on the splitting proportions of the regression tree prior. We characterize the asymptotic distribution of the number of predictors included in the model and show how this prior can be easily incorporated into existing Markov chain Monte Carlo schemes. We demonstrate that our approach yields useful posterior inclusion probabilities for each predictor and illustrate the usefulness of our approach relative to other decision tree ensemble approaches on both simulated and real datasets.","Bayesian additive regression trees,Bayesian learning,Decision trees,Nonparametric regression,Random forests,Variable selection","Linero, Antonio R.@Florida State University@State University System of Florida"
Semiparametric Ultra-High Dimensional Model Averaging of Nonlinear Dynamic Time Series,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,41,"We propose two semiparametric model averaging schemes for nonlinear dynamic time series regression models with a very large number of covariates including exogenous regressors and auto-regressive lags. Our objective is to obtain more accurate estimates and forecasts of time series by using a large number of conditioning variables in a nonparametric way. In the first scheme, we introduce a kernel sure independence screening (KSIS) technique to screen out the regressors whose marginal regression (or autoregression) functions do not make a significant contribution to estimating the joint multivariate regression function; we then propose a semiparametric penalized method of model averaging marginal regression (MAMAR) for the regressors and auto-regressors that survive the screening procedure, to further select the regressors that have significant effects on estimating the multivariate regression function and predicting the future values of the response variable. In the second scheme, we impose an approximate factor modeling structure on the ultra-high dimensional exogenous regressors and use the principal component analysis to estimate the latent common factors; we then apply the penalized MAMAR method to select the estimated common factors and the lags of the response variable that are significant. In each of the two schemes, we construct the optimal combination of the significant marginal regression and autoregression functions. Asymptotic properties for these two schemes are derived under some regularity conditions. Numerical studies including both simulation and an empirical application to forecasting inflation are given to illustrate the proposed methodology. Supplementary materials for this article are available online.","Kernel smoother,Penalized MAMAR,Principal component analysis,Semiparametric approximation,Sure independence screening,Ultra-high dimensional time series","Chen, Jia@University of York - UK::Li, Degui@University of York - UK::Linton, Oliver@University of Cambridge::Lu, Zudi@University of Southampton"
On Recursive Bayesian Predictive Distributions,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,32,"A Bayesian framework is attractive in the context of prediction, but a fast recursive update of the predictive distribution has apparently been out of reach, in part because Monte Carlo methods are generally used to compute the predictive. This article shows that online Bayesian prediction is possible by characterizing the Bayesian predictive update in terms of a bivariate copula, making it unnecessary to pass through the posterior to update the predictive. In standard models, the Bayesian predictive update corresponds to familiar choices of copula but, in nonparametric problems, the appropriate copula may not have a closed-form expression. In such cases, our new perspective suggests a fast recursive approximation to the predictive density, in the spirit of Newton's predictive recursion algorithm, but without requiring evaluation of normalizing constants. Consistency of the new algorithm is shown, and numerical examples demonstrate its quality performance in finite-samples compared to fully Bayesian and kernel methods. Supplementary materials for this article are available online.","Copula,Density estimation,Nonparametric Bayes,Prediction,Recursive estimation","Hahn, P. Richard@University of Chicago::Martin, Ryan@North Carolina State University@University of North Carolina::Walker, Stephen G.@University of Texas Austin@University of Texas System"
False Discovery Rate Smoothing,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,48,"We present false discovery rate (FDR) smoothing, an empirical-Bayes method for exploiting spatial structure in large multiple-testing problems. FDR smoothing automatically finds spatially localized regions of significant test statistics. It then relaxes the threshold of statistical significance within these regions, and tightens it elsewhere, in a manner that controls the overall false discovery rate at a given level. This results in increased power and cleaner spatial separation of signals from noise. The approach requires solving a nonstandard high-dimensional optimization problem, for which an efficient augmented-Lagrangian algorithm is presented. In simulation studies, FDR smoothing exhibits state-of-the-art performance at modest computational cost. In particular, it is shown to be far more robust than existing methods for spatially dependent multiple testing. We also apply the method to a dataset from an fMRI experiment on spatial working memory, where it detects patterns that are much more biologically plausible than those detected by standard FDR-controlling methods. All code for FDR smoothing is publicly available in Python and R (https://github.com/tansey/smoothfdr). Supplementary materials for this article are available online.","Empirical Bayes,False discovery rate,FDR,Fused lasso,Multiple hypothesis testing,Spatial smoothing","Tansey, Wesley@University of Texas Austin@University of Texas System::Koyejo, Oluwasanmi@University of Illinois Urbana-Champaign@University of Illinois System::Poldrack, Russell A.@Stanford University::Scott, James G.@University of Texas Austin@University of Texas System"
Nonparametric Maximum Likelihood Estimators of Time-Dependent Accuracy Measures for Survival Outcome Under Two-Stage Sampling Designs,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,35,"Large prospective cohort studies of rare chronic diseases require thoughtful planning of study designs, especially for biomarker studies when measurements are based on stored tissue or blood specimens. Two-phase designs, including nested case-control and case-cohort sampling designs, provide cost-effective strategies for conducting biomarker evaluation studies.Existing literature for biomarker assessment under two-phase designs largely focuses on simple inverse probability weighting (IPW) estimators. Drawing on recent theoretical development on the maximum likelihood estimators for relative risk parameters in two-phase studies, we propose nonparametric maximum likelihood-based estimators to evaluate the accuracy and predictiveness of a risk prediction biomarker under both types of two-phase designs. In addition, hybrid estimators that combine IPW estimators and maximum likelihood estimation procedure are proposed to improve efficiency and alleviate computational burden. We derive large sample properties of proposed estimators and evaluate their finite sample performance using numerical studies. We illustrate new procedures using a two-phase biomarker study aiming to evaluate the accuracy of a novel biomarker, des--carboxy prothrombin, for early detection of hepatocellular carcinoma. Supplementary materials for this article are available online.","Case-cohort sampling,Negative predictive value,Nested case-control sampling,Positive predictive value,Receiver operating characteristics curve (ROC curve),Two-phase study","Liu, Dandan@Vanderbilt University::Cai, Tianxi@Harvard T.H. Chan School of Public Health@Harvard University::Lok, Anna@University of Michigan System@University of Michigan::Zheng, Yingye@Fred Hutchinson Cancer Center"
Controlling the FDR in Imperfect Matches to an Incomplete Database,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,21,"We consider the problem of controlling the false discovery rate (FDR) among discoveries from searching an incomplete database. This problem differs from the classical multiple testing setting because there are two different types of false discoveries: those arising from objects that have no match in the database and those that are incorrectly matched. We show that commonly used FDR controlling procedures are inadequate for this setup, a special case of which is tandem mass spectrum identification. We then derive a novel FDR controlling approach which extensive simulations suggest is unbiased. We also compare its performance with problem-specific as well as general FDR controlling procedures using both simulated and real mass spectrometry data.","False discovery rate,Multiple hypothesis testing,Tandem mass spectrometry","Keich, Uri@University of Sydney::Noble, William Stafford@University of Washington@University of Washington Seattle"
Modeling Motor Learning Using Heteroscedastic Functional Principal Components Analysis,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,38,"We propose a novel method for estimating population-level and subject-specific effects of covariates on the variability of functional data. We extend the functional principal components analysis framework by modeling the variance of principal component scores as a function of covariates and subject-specific random effects. In a setting where principal components are largely invariant across subjects and covariate values, modeling the variance of these scores provides a flexible and interpretable way to explore factors that affect the variability of functional data. Our work is motivated by a novel dataset from an experiment assessing upper extremity motor control, and quantifies the reduction in movement variability associated with skill learning. The proposed methods can be applied broadly to understand movement variability, in settings that include motor learning, impairment due to injury or disease, and recovery. Supplementary materials for this article are available online.","Functional data,Kinematic data,Motor control,Probabilistic PCA,Variance modeling,Variational Bayes","Backenroth, Daniel@Columbia University::Goldsmith, Jeff@Columbia University::Harran, Michelle D.@Johns Hopkins University@Columbia University::Cortes, Juan C.@Johns Hopkins University@Columbia University::Krakauer, John W.@Johns Hopkins University@Columbia University::Kitago, Tomoko@Columbia University"
Assessing Time-Varying Causal Effect Moderation in Mobile Health,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,36,"In mobile health interventions aimed at behavior change and maintenance, treatments are provided in real time to manage current or impending high-risk situations or promote healthy behaviors in near real time. Currently there is great scientific interest in developing data analysis approaches to guide the development of mobile interventions. In particular data from mobile health studies might be used to examine effect moderatorsindividual characteristics, time-varying context, or past treatment response that moderate the effect of current treatment on a subsequent response. This article introduces a formal definition for moderated effects in terms of potential outcomes, a definition that is particularly suited to mobile interventions, where treatment occasions are numerous, individuals are not always available for treatment, and potential moderators might be influenced by past treatment. Methods for estimating moderated effects are developed and compared. The proposed approach is illustrated using BASICS-Mobile, a smartphone-based intervention designed to curb heavy drinking and smoking among college students. Supplementary materials for this article are available online.","Effect modification,mHealth,Structural nested mean model","Boruvka, Audrey@University of Michigan System@University of Michigan::Almirall, Daniel@University of Michigan System@University of Michigan::Witkiewitz, Katie@University of New Mexico::Murphy, Susan A.@University of Michigan System@University of Michigan"
Functional Feature Construction for Individualized Treatment Regimes,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,63,"Evidence-based personalized medicine formalizes treatment selection as an individualized treatment regime that maps up-to-date patient information into the space of possible treatments. Available patient information may include static features such race, gender, family history, genetic and genomic information, as well as longitudinal information including the emergence of comorbidities, waxing and waning of symptoms, side-effect burden, and adherence. Dynamic information measured at multiple time points before treatment assignment should be included as input to the treatment regime. However, subject longitudinal measurements are typically sparse, irregularly spaced, noisy, and vary in number across subjects. Existing estimators for treatment regimes require equal information be measured on each subject and thus standard practice is to summarize longitudinal subject information into a scalar, ad hoc summary during data preprocessing. This reduction of the longitudinal information to a scalar feature precedes estimation of a treatment regime and is therefore not informed by subject outcomes, treatments, or covariates. Furthermore, we show that this reduction requires more stringent causal assumptions for consistent estimation than are necessary. We propose a data-driven method for constructing maximally prescriptive yet interpretable features that can be used with standard methods for estimating optimal treatment regimes. In our proposed framework, we treat the subject longitudinal information as a realization of a stochastic process observed with error at discrete time points. Functionals of this latent process are then combined with outcome models to estimate an optimal treatment regime. The proposed methodology requires weaker causal assumptions than Q-learning with an ad hoc scalar summary and is consistent for the optimal treatment regime. Supplementary materials for this article are available online.","Functional data,Personalized medicine,Regression,Treatment regimes","Laber, Eric B.@North Carolina State University@University of North Carolina::Staicu, Ana-Maria@North Carolina State University@University of North Carolina"
Quantile-Optimal Treatment Regimes,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,69,"Finding the optimal treatment regime (or a series of sequential treatment regimes) based on individual characteristics has important applications in areas such as precision medicine, government policies, and active labor market interventions. In the current literature, the optimal treatment regime is usually defined as the one that maximizes the average benefit in the potential population. This article studies a general framework for estimating the quantile-optimal treatment regime, which is of importance in many real-world applications. Given a collection of treatment regimes, we consider robust estimation of the quantile-optimal treatment regime, which does not require the analyst to specify an outcome regression model. We propose an alternative formulation of the estimator as a solution of an optimization problem with an estimated nuisance parameter. This novel representation allows us to investigate the asymptotic theory of the estimated optimal treatment regime using empirical process techniques. We derive theory involving a nonstandard convergence rate and a nonnormal limiting distribution. The same nonstandard convergence rate would also occur if the mean optimality criterion is applied, but this has not been studied. Thus, our results fill an important theoretical gap for a general class of policy search methods in the literature. The article investigates both static and dynamic treatment regimes. In addition, doubly robust estimation and alternative optimality criterion such as that based on Gini's mean difference or weighted quantiles are investigated. Numerical simulations demonstrate the performance of the proposed estimator. A data example from a trial in HIV+ patients is used to illustrate the application. Supplementary materials for this article are available online.","Dynamic treatment regime,Nonstandard asymptotics,Optimal treatment regime,Precision medicine,Quantile criterion","Wang, Lan@University of Minnesota Twin Cities@University of Minnesota System::Zhou, Yu@University of Minnesota Twin Cities@University of Minnesota System::Song, Rui@North Carolina State University@University of North Carolina::Sherwood, Ben@University of Kansas"
Edge Exchangeable Models for Interaction Networks,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,50,"Many modern network datasets arise from processes of interactions in a population, such as phone calls, email exchanges, co-authorships, and professional collaborations. In such interaction networks, the edges comprise the fundamental statistical units, making a framework for edge-labeled networks more appropriate for statistical analysis. In this context, we initiate the study of edge exchangeable network models and explore its basic statistical properties. Several theoretical and practical features make edge exchangeable models better suited to many applications in network analysis than more common vertex-centric approaches. In particular, edge exchangeable models allow for sparse structure and power law degree distributions, both of which are widely observed empirical properties that cannot be handled naturally by more conventional approaches. Our discussion culminates in the Hollywood model, which we identify here as the canonical family of edge exchangeable distributions. The Hollywood model is computationally tractable, admits a clear interpretation, exhibits good theoretical properties, and performs reasonably well in estimation and prediction as we demonstrate on real network datasets. As a generalization of the Hollywood model, we further identify the vertex components model as a nonparametric subclass of models with a convenient stick breaking construction.","Edge exchangeability,Edge-labeled network,Exchangeable random graph,Interaction data,Power-law distribution,Scale-free network,Sparse network","Crane, Harry@Rutgers State University New Brunswick::Dempsey, Walter@University of Michigan System@University of Michigan"
On the Null Distribution of Bayes Factors in Linear Regression,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,45,"We show that under the null, the is asymptotically distributed as a weighted sum of chi-squared random variables with a shifted mean. This claim holds for Bayesian multi-linear regression with a family of conjugate priors, namely, the normal-inverse-gamma prior, the g-prior, and the normal prior. Our results have three immediate impacts. First, we can compute analytically a p-value associated with a Bayes factor without the need of permutation. We provide a software package that can evaluate the p-value associated with Bayes factor efficiently and accurately. Second, the null distribution is illuminating to some intrinsic properties of Bayes factor, namely, how Bayes factor quantitatively depends on prior and the genesis of Bartlett's paradox. Third, enlightened by the null distribution of Bayes factor, we formulate a novel scaled Bayes factor that depends less on the prior and is immune to Bartlett's paradox. When two tests have an identical p-value, the test with a larger power tends to have a larger scaled Bayes factor, a desirable property that is missing for the (unscaled) Bayes factor. Supplementary materials for this article are available online.","p-Value,Scaled Bayes factor,Weighted sum of chi-squared random variables","Zhou, Quan@Baylor College of Medicine::Guan, Yongtao@Baylor College of Medicine"
Bayesian Neural Networks for Selection of Drug Sensitive Genes,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,78,"Recent advances in high-throughput biotechnologies have provided an unprecedented opportunity for biomarker discovery, which, from a statistical point of view, can be cast as a variable selection problem. This problem is challenging due to the high-dimensional and nonlinear nature of omics data and, in general, it suffers three difficulties: (i) an unknown functional form of the nonlinear system, (ii) variable selection consistency, and (iii) high-demanding computation. To circumvent the first difficulty, we employ a feed-forward neural network to approximate the unknown nonlinear function motivated by its universal approximation ability. To circumvent the second difficulty, we conduct structure selection for the neural network, which induces variable selection, by choosing appropriate prior distributions that lead to the consistency of variable selection. To circumvent the third difficulty, we implement the population stochastic approximation Monte Carlo algorithm, a parallel adaptive Markov Chain Monte Carlo algorithm, on the OpenMP platform that provides a linear speedup for the simulation with the number of cores of the computer. The numerical results indicate that the proposed method can work very well for identification of relevant variables for high-dimensional nonlinear systems. The proposed method is successfully applied to identification of the genes that are associated with anticancer drug sensitivities based on the data collected in the cancer cell line encyclopedia study. Supplementary materials for this article are available online.","Cancer cell line encyclopedia,Nonlinear variable selection,Omics data,OpenMP,Parallel Markov chain Monte Carlo","Liang, Faming@Purdue University@Purdue University System::Li, Qizhai@Chinese Academy of Sciences@Academy of Mathematics & System Sciences, CAS::Zhou, Lei@University of Florida@State University System of Florida"
Optimal Probability Weights for Inference With Constrained Precision,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,43,"Probability weights are used in many areas of research including complex survey designs, missing data analysis, and adjustment for confounding factors. They are useful analytic tools but can lead to statistical inefficiencies when they contain outlying values. This issue is frequently tackled by replacing large weights with smaller ones or by normalizing them through smoothing functions. While these approaches are practical, they are also prone to yield biased inferences. This article introduces a method for obtaining optimal weights, defined as those with smallest Euclidean distance from target weights among all sets of weights that satisfy a constraint on the variance of the resulting weighted estimator. The optimal weights yield minimum-bias estimators among all estimators with specified precision. The method is based on solving a constrained nonlinear optimization problem whose Lagrange multipliers and objective function can help assess the trade-off between bias and precision of the resulting weighted estimator. The finite-sample performance of the optimally weighted estimator is assessed in a simulation study, and its applicability is illustrated through an analysis of heterogeneity over age of the effect of the timing of treatment-initiation on long-term treatment efficacy in patient infected by human immunodeficiency virus in Sweden.","Mathematical programming,Nonlinear constrained optimization,Probability weights,Sampling weights,Weighted estimators","Santacatterina, Michele@Karolinska Institutet::Bottai, Matteo@Karolinska Institutet"
A Bayesian Phase I/II Trial Design for Immunotherapy,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,30,"Immunotherapy is an innovative treatment approach that stimulates a patient's immune system to fight cancer. It demonstrates characteristics distinct from conventional chemotherapy and stands to revolutionize cancer treatment. We propose a Bayesian phase I/II dose-finding design that incorporates the unique features of immunotherapy by simultaneously considering three outcomes: immune response, toxicity, and efficacy. The objective is to identify the biologically optimal dose, defined as the dose with the highest desirability in the risk-benefit tradeoff. An Emax model is utilized to describe the marginal distribution of the immune response. Conditional on the immune response, we jointly model toxicity and efficacy using a latent variable approach. Using the accumulating data, we adaptively randomize patients to experimental doses based on the continuously updated model estimates. A simulation study shows that our proposed design has good operating characteristics in terms of selecting the target dose and allocating patients to the target dose. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.","Bayesian adaptive design,Dose finding,Immune response,Immunotherapy,Phase I,II trial,Risk-benefit tradeoff","Liu, Suyu@UTMD Anderson Cancer Center@University of Texas System::Guo, Beibei@Louisiana State University System@Louisiana State University::Yuan, Ying@UTMD Anderson Cancer Center@University of Texas System"
Mission CO(2)ntrol: A Statistical Scientist's Role in Remote Sensing of Atmospheric Carbon Dioxide Comment,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,33,no abstract,,"Genton, Marc G.@King Abdullah University of Science & Technology::Jeong, Jaehong@King Abdullah University of Science & Technology"
Mission CO(2)ntrol: A Statistical Scientist's Role in Remote Sensing of Atmospheric Carbon Dioxide Rejoinder,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,14,no abstract,,"Cressie, Noel@University of Wollongong"
Minimax Optimal Procedures for Locally Private Estimation,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,55,"Working under a model of privacy in which data remain private even from the statistician, we study the tradeoff between privacy guarantees and the risk of the resulting statistical estimators. We develop private versions of classical information-theoretical bounds, in particular those due to Le Cam, Fano, and Assouad. These inequalities allow for a precise characterization of statistical rates under local privacy constraints and the development of provably (minimax) optimal estimation procedures. We provide a treatment of several canonical families of problems: mean estimation and median estimation, generalized linear models, and nonparametric density estimation. For all of these families, we provide lower and upper bounds that match up to constant factors, and exhibit new (optimal) privacy-preserving mechanisms and computationally efficient estimators that achieve the bounds. Additionally, we present a variety of experimental results for estimation problems involving sensitive data, including salaries, censored blog posts and articles, and drug abuse; these experiments demonstrate the importance of deriving optimal procedures. Supplementary materials for this article are available online.","Differential privacy,Minimax bounds,Optimal estimators","Duchi, John C.@Stanford University::Jordan, Michael I.@University of California System@University of California Berkeley::Wainwright, Martin J.@University of California System@University of California Berkeley"
Minimax Optimal Procedures for Locally Private Estimation Rejoinder,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,28,no abstract,,"Duchi, John C.@Stanford University::Jordan, Michael I.@University of California Berkeley@University of California System::Wainwright, Martin J.@University of California Berkeley@University of California System"
Martingale Difference Divergence Matrix and Its Application to Dimension Reduction for Stationary Multivariate Time Series,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,39,"In this article, we introduce a new methodology to perform dimension reduction for a stationary multivariate time series. Our method is motivated by the consideration of optimal prediction and focuses on the reduction of the effective dimension in conditional mean of time series given the past information. In particular, we seek a contemporaneous linear transformation such that the transformed time series has two parts with one part being conditionally mean independent of the past. To achieve this goal, we first propose the so-called martingale difference divergence matrix (MDDM), which can quantify the conditional mean independence of V R-p given U R-q and also encodes the number and form of linear combinations of V that are conditional mean independent of U. Our dimension reduction procedure is based on eigen-decomposition of the cumulative martingale difference divergence matrix, which is an extension of MDDM to the time series context. Interestingly, there is a static factor model representation for our dimension reduction framework and it has subtle difference from the existing static factor model used in the time series literature. Some theory is also provided about the rate of convergence of eigenvalue and eigenvector of the sample cumulative MDDM in the fixed-dimensional setting. Favorable finite sample performance is demonstrated via simulations and real data illustrations in comparison with some existing methods. Supplementary materials for this article are available online.","Conditional mean,Low rank,Nonlinear dependence,Principal components","Lee, Chung Eun@University of Illinois Urbana-Champaign@University of Illinois System::Shao, Xiaofeng@University of Illinois Urbana-Champaign@University of Illinois System"
Exact p-Values for Network Interference,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,34,"We study the calculation of exact p-values for a large class of nonsharp null hypotheses about treatment effects in a setting with data from experiments involving members of a single connected network. The class includes null hypotheses that limit the effect of one unit's treatment status on another according to the distance between units, for example, the hypothesis might specify that the treatment status of immediate neighbors has no effect, or that units more than two edges away have no effect. We also consider hypotheses concerning the validity of sparsification of a network (e.g., based on the strength of ties) and hypotheses restricting heterogeneity in peer effects (so that, e.g., only the number or fraction treated among neighboring units matters). Our general approach is to define an artificial experiment, such that the null hypothesis that was not sharp for the original experiment is sharp for the artificial experiment, and such that the randomization analysis for the artificial experiment is validated by the design of the original experiment.","Fisher exact p-values,Interactions,Randomization inference,Spillovers,SUTVA","Athey, Susan@National Bureau of Economic Research@Stanford University::Eckles, Dean@Massachusetts Institute of Technology (MIT)::Imbens, Guido W.@National Bureau of Economic Research@Stanford University"
Network Cross-Validation for Determining the Number of Communities in Network Data,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,51,"The stochastic block model (SBM) and its variants have been a popular tool for analyzing large network data with community structures. In this article, we develop an efficient network cross-validation (NCV) approach to determine the number of communities, as well as to choose between the regular stochastic block model and the degree corrected block model (DCBM). The proposed NCV method is based on a block-wise node-pair splitting technique, combined with an integrated step of community recovery using sub-blocks of the adjacency matrix. We prove that the probability of under-selection vanishes as the number of nodes increases, under mild conditions satisfied by a wide range of popular community recovery algorithms. The solid performance of our method is also demonstrated in extensive simulations and two data examples. Supplementary materials for this article are available online.","Block-wise node-pair splitting,Community recovery,Cross-validation,Model selection,Stochastic block models","Chen, Kehui@Pennsylvania Commonwealth System of Higher Education (PCSHE)@University of Pittsburgh::Lei, Jing@Carnegie Mellon University"
ECA: High-Dimensional Elliptical Component Analysis in Non-Gaussian Distributions,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,73,"We present a robust alternative to principal component analysis (PCA)called elliptical component analysis (ECA)for analyzing high-dimensional, elliptically distributed data. ECA estimates the eigenspace of the covariance matrix of the elliptical data. To cope with heavy-tailed elliptical distributions, a multivariate rank statistic is exploited. At the model-level, we consider two settings: either that the leading eigenvectors of the covariance matrix are nonsparse or that they are sparse. Methodologically, we propose ECA procedures for both nonsparse and sparse settings. Theoretically, we provide both nonasymptotic and asymptotic analyses quantifying the theoretical performances of ECA. In the nonsparse setting, we show that ECA's performance is highly related to the effective rank of the covariance matrix. In the sparse setting, the results are twofold: (i) we show that the sparse ECA estimator based on a combinatoric program attains the optimal rate of convergence; (ii) based on some recent developments in estimating sparse leading eigenvectors, we show that a computationally efficient sparse ECA estimator attains the optimal rate of convergence under a suboptimal scaling. Supplementary materials for this article are available online.","Multivariate Kendall's tau,Elliptical component analysis,Sparse principal component analysis,Optimality property,Robust estimators,Elliptical distribution","Han, Fang@University of Washington@University of Washington Seattle::Liu, Han@Princeton University"
Classified Mixed Model Prediction,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,26,"Many practical problems are related to prediction, where the main interest is at subject (e.g., personalized medicine) or (small) sub-population (e.g., small community) level. In such cases, it is possible to make substantial gains in prediction accuracy by identifying a class that a new subject belongs to. This way, the new subject is potentially associated with a random effect corresponding to the same class in the training data, so that method of mixed model prediction can be used to make the best prediction. We propose a new method, called classified mixed model prediction (CMMP), to achieve this goal. We develop CMMP for both prediction of mixed effects and prediction of future observations, and consider different scenarios where there may or may not be a match of the new subject among the training-data subjects. Theoretical and empirical studies are carried out to study the properties of CMMP, including prediction intervals based on CMMP, and its comparison with existing methods. In particular, we show that, even if the actual match does not exist between the class of the new observations and those of the training data, CMMP still helps in improving prediction accuracy. Two real-data examples are considered. Supplementary materials for this article are available online.","CMMP,Future observation,Linear mixed model,Mean squared prediction error,Mixed effects,Prediction interval","Jiang, Jiming@University of California Davis@University of California System::Rao, J. Sunil@University of Miami::Fan, Jie@University of Miami::Thuan Nguyen@Oregon Health & Science University"
A General Framework for Estimation and Inference From Clusters of Features,JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION,25,"Applied statistical problems often come with prespecified groupings to predictors. It is natural to test for the presence of simultaneous group-wide signal for groups in isolation, or for multiple groups together. Current tests for the presence of such signals include the classical F-test or a t-test on unsupervised group prototypes (either group centroids or first principal components). In this article, we propose test statistics that aim for power improvements over these classical approaches. In particular, we first create group prototypes, with reference to the response, and then test with likelihood ratio statistics incorporating only these prototypes. We propose a model, called the prototype model, which naturally models this two-step procedure. Furthermore, we introduce an inferential schema detailing the unique considerations for different combinations of prototype formation and univariate/multivariate testing models. The prototype model also suggests new applications to estimation and prediction. Prototype formation often relies on variable selection, which invalidates classical Gaussian test theory. We use recent advances in selective inference to account for selection in the prototyping step and retain test validity. Simulation experiments suggest that our testing procedure enjoys more power than do classical approaches. Supplementary materials for this article are available online.","Hypothesis testing,Likelihood ratio statistics,Selective inference","Reid, Stephen@Stanford University::Taylor, Jonathan@Stanford University::Tibshirani, Robert@Stanford University"
Partial likelihood estimation of isotonic proportional hazards models,BIOMETRIKA,18,"We consider the estimation of the semiparametric proportional hazards model with an unspecified baseline hazard function where the effect of a continuous covariate is assumed to be monotone. Previous work on nonparametric maximum likelihood estimation for isotonic proportional hazard regression with right-censored data is computationally intensive, lacks theoretical justification, and may be prohibitive in large samples. In this paper, partial likelihood estimation is studied. An iterative quadratic programming method is considered, which has performed well with likelihoods for isotonic parametric regression models. However, the iterative quadratic programming method for the partial likelihood cannot be implemented using standard pool-adjacent-violators techniques, increasing the computational burden and numerical instability. The iterative convex minorant algorithm which uses pool-adjacent-violators techniques has also been shown to perform well in related parametric likelihood set-ups, but evidences computational difficulties under the proportional hazards model. An alternative pseudo-iterative convex minorant algorithm is proposed which exploits the pool-adjacent-violators techniques, is theoretically justified, and exhibits computational stability. A separate estimator of the baseline hazard function is provided. The algorithms are extended to models with time-dependent covariates. Simulation studies demonstrate that the pseudo-iterative convex minorant algorithm may yield orders-of-magnitude reduction in computing time relative to the iterative quadratic programming method and the iterative convex minorant algorithm, with moderate reductions in the bias and variance of the estimators. Analysis of data from a recent HIV prevention study illustrates the practical utility of the isotonic methodology in estimating nonlinear, monotonic covariate effects.","Algorithmic convergence,Concavity,Constrained partial ikelihood,Isotonic regression,Shape-restricted inference,Robustness","Chung, Yunro@Fred Hutchinson Cancer Center::Ivanova, Anastasia@University of North Carolina School of Medicine@University of North Carolina@University of North Carolina Chapel Hill::Hudgens, Michael G.@University of North Carolina School of Medicine@University of North Carolina@University of North Carolina Chapel Hill::Fine, Jason P.@University of North Carolina School of Medicine@University of North Carolina@University of North Carolina Chapel Hill"
Dual regression,BIOMETRIKA,19,"We propose dual regression as an alternative to quantile regression for the global estimation of conditional distribution functions. Dual regression provides the interpretational power of quantile regression while avoiding the need to repair intersecting conditional quantile surfaces. We introduce a mathematical programming characterization of conditional distribution functions which, in its simplest form, is the dual program of a simultaneous estimator for linear location-scale models, and use it to specify and estimate a flexible class of conditional distribution functions. We present asymptotic theory for the corresponding empirical dual regression process.","Convex approximation,Duality,Mathematical programming,Method of moments,Monotonicity,Quantile regression","Spady, R. H.@University of Oxford::Stouli, S.@University of Bristol"
Robust and consistent variable selection in high-dimensional generalized linear models,BIOMETRIKA,63,"Generalized linear models are popular for modelling a large variety of data. We consider variable selection through penalized methods by focusing on resistance issues in the presence of outlying data and other deviations from assumptions. We highlight the weaknesses of widely-used penalized M-estimators, propose a robust penalized quasilikelihood estimator, and show that it enjoys oracle properties in high dimensions and is stable in a neighbourhood of the model. We illustrate its finite-sample performance on simulated and real data.","Contamination neighbourhood,Generalized linear model,Infinitesimal robustness,Lasso,Oracle estimator,Robust quasilikelihood","Avella-Medina, Marco@Massachusetts Institute of Technology (MIT)::Ronchetti, Elvezio@University of Geneva"
A randomization-based perspective on analysis of variance: a test statistic robust to treatment effect heterogeneity,BIOMETRIKA,37,"Fisher randomization tests for Neyman's null hypothesis of no average treatment effect are considered in a finite-population setting associated with completely randomized experiments involving more than two treatments. The consequences of using the F statistic to conduct such a test are examined, and we argue that under treatment effect heterogeneity, use of the F statistic in the Fisher randomization test can severely inflate the Type I error under Neyman's null hypothesis. We propose to use an alternative test statistic, derive its asymptotic distributions under Fisher's and Neyman's null hypotheses, and demonstrate its advantages through simulations.","Additivity,Fisher randomization test,Null hypothesis,One-way layout","Ding, Peng@University of California System@University of California Berkeley::Dasgupta, Tirthankar@Rutgers State University New Brunswick"
Testing for the presence of significant covariates through conditional marginal regression,BIOMETRIKA,24,"Researchers sometimes have a priori information on the relative importance of predictors that can be used to screen out covariates. An important question is whether any of the discarded covariates have predictive power when the most relevant predictors are included in the model. We consider testing whether any discarded covariate is significant conditional on some pre-chosen covariates. We propose a maximum-type test statistic and show that it has a nonstandard asymptotic distribution, giving rise to the conditional adaptive resampling test. To accommodate signals of unknown sparsity, we develop a hybrid test statistic, which is a weighted average of maximum- and sum-type statistics. We prove the consistency of the test procedure under general assumptions and illustrate how it can be used as a stopping rule in forward regression. We show, through simulation, that the proposed method provides adequate control of the familywise error rate with competitive power for both sparse and dense signals, even in high-dimensional cases, and we demonstrate its advantages in cases where the covariates are heavily correlated. We illustrate the application of our method by analysing an expression quantitative trait locus dataset.","Adaptive resampling,Conditional marginal regression,Forward selection,Hybrid test","Tang, Yanlin@Tongji University::Wang, Huixia Judy@George Washington University::Barut, Emre@George Washington University"
A robust goodness-of-fit test for generalized autoregressive conditional heteroscedastic models,BIOMETRIKA,40,"The estimation of time series models with heavy-tailed innovations has been widely discussed, but corresponding goodness-of-fit tests have attracted less attention, primarily because the autocorrelation function commonly used in constructing goodness-of-fit tests necessarily imposes certain moment conditions on the innovations. As a bounded random variable has finite moments of all orders, we address the problem by first transforming the residuals with a bounded function. More specifically, we consider the sample autocorrelation function of the transformed absolute residuals of a fitted generalized autoregressive conditional heteroscedastic model. With the corresponding residual empirical distribution function naturally employed as the transformation, a robust goodness-of-fit test is then constructed. The asymptotic distributions of the test statistic under the null hypothesis and local alternatives are derived, and Monte Carlo experiments are conducted to examine finite-sample properties. The proposed test is shown to be more powerful than existing tests when the innovations are heavy-tailed.","Conditional heteroscedastic model,Goodness-of-fit test,Heavy tail,Residual empirical process,Robustness","Zheng, Yao@University of Hong Kong::Li, Wai Keung@University of Hong Kong::Li, Guodong@University of Hong Kong"
Shape-constrained partial identification of a population mean under unknown probabilities of sample selection,BIOMETRIKA,14,"Estimating a population mean from a sample obtained with unknown selection probabilities is important in the biomedical and social sciences. Using a ratio estimator, Aronow & Lee (2013) proposed a method for partial identification of the mean by allowing the unknown selection probabilities to vary arbitrarily between two fixed values. In this paper, we show how to use auxiliary shape constraints on the population outcome distribution, such as symmetry or log-concavity, to obtain tighter bounds on the population mean. We use this method to estimate the performance of Aymara students, an ethnic minority in the north of Chile, in a national educational standardized test. We implement this method in the R package scbounds.","Partial identification,Sensitivity analysis,Survey sampling","Miratrix, L. W.@Harvard University::Wager, S.@Stanford University::Zubizarreta, J. R.@Harvard University"
Two-sample tests of high-dimensional means for compositional data,BIOMETRIKA,17,"Compositional data are ubiquitous in many scientific endeavours. Motivated by microbiome and metagenomic research, we consider a two-sample testing problem for high-dimensional compositional data and formulate a testable hypothesis of compositional equivalence for the means of two latent log basis vectors. We propose a test through the centred log-ratio transformation of the compositions. The asymptotic null distribution of the test statistic is derived and its power against sparse alternatives is investigated. A modified test for paired samples is also considered. Simulations show that the proposed tests can be significantly more powerful than tests that are applied to the raw and log-transformed compositions. The usefulness of our tests is illustrated by applications to gut microbiome composition in obesity and Crohn's disease.","Basis,Centred log-ratio transformation,Compositional equivalence,Extreme value distribution,Microbiome,Sparse alternative","Cao, Yuanpei@University of Pennsylvania::Lin, Wei@Peking University::Li, Hongzhe@University of Pennsylvania"
Simple least squares estimator for treatment effects using propensity score residuals,BIOMETRIKA,20,"Propensity score matching is widely used to control covariates when analysing the effects of a nonrandomized binary treatment. However, it requires several arbitrary decisions, such as how many matched subjects to use and how to choose them. In this paper a simple least squares estimator is proposed, where the treatment, and possibly the response variable, is replaced by the propensity score residual. The proposed estimator controls covariates semiparametrically if the propensity score function is correctly specified. Furthermore, it is numerically stable and relatively easy to use, compared with alternatives such as matching, regression imputation, weighting, and doubly robust estimators. The proposed estimator also has a simple valid asymptotic variance estimator that works well in small samples. The least squares estimator is extended to multiple treatments and noncontinuously distributed responses. A simulation study demonstrates that it has lower mean squared error than its competitors.","Binary treatment,Generalized propensity score,Multiple treatments,Propensity score","Lee, Myoung-Jae@Korea University"
Optimal discrimination designs for semiparametric models,BIOMETRIKA,33,"Much work on optimal discrimination designs assumes that the models of interest are fully specified, apart from unknown parameters. Recent work allows errors in the models to be nonnormally distributed but still requires the specification of the mean structures. Otsu (2008) proposed optimal discriminating designs for semiparametric models by generalizing the Kullback-Leibler optimality criterion proposed by L<remove>pez-Fidalgo et al. (2007). This paper develops a relatively simple strategy for finding an optimal discrimination design. We also formulate equivalence theorems to confirm optimality of a design and derive relations between optimal designs found here for discriminating semiparametric models and those commonly used in optimal discrimination design problems.","Continuous design,Equivalence theorem,Kullback-Leibler divergence,T-optimality,Variational calculus","Dette, H.@Ruhr University Bochum::Guchenko, R.@Saint Petersburg State University::Melas, V. B.@Saint Petersburg State University::Wong, W. K.@University of California Los Angeles@University of California System"
Miscellanea On causal estimation using U-statistics,BIOMETRIKA,15,"We introduce a general class of causal estimands which extends the familiar notion of average treatment effect. The class is defined by a contrast function, prespecified to quantify the relative favourability of one outcome over another, averaged over the marginal distributions of two potential outcomes. Natural estimators arise in the form of U-statistics. We derive both a naive inverse propensity score weighted estimator and a class of locally efficient and doubly robust estimators. The usefulness of our theory is illustrated by two examples, one for causal estimation with ordinal outcomes, and the other for causal tests that are robust with respect to outliers.","Average treatment effect,Double robustness,Locally efficient estimation,Mann-Whitney test,Potential outcome,Semiparametric inference","Mao, Lu@University of Wisconsin System@University of Wisconsin Madison"
On overfitting and post-selection uncertainty assessments,BIOMETRIKA,6,"In a regression context, when the relevant subset of explanatory variables is uncertain, it is common to use a data-driven model selection procedure. Classical linear model theory, applied naively to the selected submodel, may not be valid because it ignores the selected submodel's dependence on the data. We provide an explanation of this phenomenon, in terms of overfitting, for a class of model selection criteria.","Akaike information criterion,Bayesian information criterion,Model selection,Regression","Hong, L.@Unknow::Kuffner, T. A.@Washington University (WUSTL)::Martin, R.@North Carolina State University@University of North Carolina"
A conditional composite likelihood ratio test with boundary constraints,BIOMETRIKA,16,Composite likelihood has been widely used in applications. The asymptotic distribution of the composite likelihood ratio statistic at the boundary of the parameter space is a complicated mixture of weighted chi(2) distributions. In this paper we propose a conditional test with data-dependent degrees of freedom. We consider a modification of the composite likelihood which satisfies the second-order Bartlett identity. We show that the modified composite likelihood ratio statistic given the number of estimated parameters lying on the boundary converges to a simple chi(2) distribution. This conditional testing procedure is validated through simulation studies.,"Boundary problem,Composite likelihood,Likelihood ratio test,Nonstandard inference","Chen, Yong@University of Pennsylvania::Huang, Jing@University of Pennsylvania::Ning, Yang@Cornell University::Liang, Kung-Yee@National Yang Ming University::Lindsay, Bruce G.@Unknow"
On bias reduction and incidental parameters,BIOMETRIKA,19,Firth (1993) introduced a method for reducing the bias of the maximum likelihood estimator. Here it is shown that the approach is also effective in reducing the sensitivity of inferential procedures to incidental parameters.,"Bartlett identity,Binary matched pairs,Modified profile likelihood,Two-index asymptotics","Lunardon, N.@University of Milano-Bicocca"
Choosing between methods of combining p-values,BIOMETRIKA,12,"Combining p-values from independent statistical tests is a popular approach to meta-analysis, particularly when the data underlying the tests are either no longer available or are difficult to combine. Numerous p-value combination methods appear in the literature, each with different statistical properties, yet often the final choice used in a meta-analysis can seem arbitrary, as if all effort has been expended in building the models that gave rise to the p-values. Birnbaum (1954) showed that any reasonable p-value combiner must be optimal against some alternative hypothesis. Starting from this perspective and recasting each method of combining p-values as a likelihood ratio test, we present theoretical results for some standard combiners that provide guidance on how a powerful combiner might be chosen in practice.","Edgington's method,Fisher's method,George's method,Meta-analysis,Pearson's method,Stouffer's method,Tippett's method","Heard, N. A.@Imperial College London::Rubin-Delanchy, P.@University of Bristol"
"On the degrees of freedom of reduced-rank estimators in multivariate regression (vol 102, pg 457, 2015)",BIOMETRIKA,2,no abstract,,"Mukherjee, A.@Unknow::Chen, K.@Unknow::Wang, N.@Unknow::Zhu, J.@Unknow"
Convergence of regression-adjusted approximate Bayesian computation,BIOMETRIKA,29,"We present asymptotic results for the regression-adjusted version of approximate Bayesian computation introduced by Beaumont et al. (2002). We show that for an appropriate choice of the bandwidth, regression adjustment will lead to a posterior that, asymptotically, correctly quantifies uncertainty. Furthermore, for such a choice of bandwidth we can implement an importance sampling algorithm to sample from the posterior whose acceptance probability tends to unity as the data sample size increases. This compares favourably to results for standard approximate Bayesian computation, where the only way to obtain a posterior that correctly quantifies uncertainty is to choose a much smaller bandwidth, one for which the acceptance probability tends to zero and hence for which Monte Carlo error will dominate.","Approximate Bayesian computation,Importance sampling,Local-linear regression,Partial information","Li, Wentao@Newcastle University - UK::Fearnhead, Paul@Lancaster University"
Adaptive multigroup confidence intervals with constant coverage,BIOMETRIKA,20,"Commonly used interval procedures for multigroup data attain their nominal coverage rates across a population of groups on average, but their actual coverage rate for a given group will be above or below the nominal rate, depending on the group mean. While correct coverage for a given group can be achieved with a standard t-interval, this approach is not adaptive to the available information about the distribution of group-specific means. In this article we construct confidence intervals that have a constant frequentist coverage rate and that make use of information about across-group heterogeneity, resulting in constant-coverage intervals that are narrower than standard t-intervals on average across groups. Such intervals are constructed by inverting biased Bayes-optimal tests for the mean of each group, where the prior distribution for a given group is estimated with data from the other groups.","Biased test,Confidence region,Hierarchical model,Multilevel data,Shrinkage","Yu, C.@University of Washington@University of Washington Seattle::Hoff, P. D.@Duke University"
Testing independence for multivariate time series via the auto-distance correlation matrix,BIOMETRIKA,41,"We introduce the matrix multivariate auto-distance covariance and correlation functions for time series, discuss their interpretation and develop consistent estimators for practical implementation. We also develop a test of the independent and identically distributed hypothesis for multivariate time series data and show that it performs better than the multivariate Ljung-Box test. We discuss computational aspects and present a data example to illustrate the method.","Characteristic function,Correlation,Stationarity,U-statistic,Wild bootstrap","Fokianos, K.@University of Cyprus::Pitsillou, M.@University of Cyprus"
A frequency domain analysis of the error distribution from noisy high-frequency data,BIOMETRIKA,27,"Data observed at a high sampling frequency are typically assumed to be an additive composite of a relatively slow-varying continuous-time component, a latent stochastic process or smooth random function, and measurement error. Supposing that the latent component is an Ito diffusion process, we propose to estimate the measurement error density function by applying a deconvolution technique with appropriate localization. Our estimator, which does not require equally-spaced observed times, is consistent and minimax rate-optimal. We also investigate estimators of the moments of the error distribution and their properties, propose a frequency domain estimator for the integrated volatility of the underlying stochastic process, and show that it achieves the optimal convergence rate. Simulations and an application to real data validate our analysis.","Deconvolution,Fourier transform,Functional data,High-frequency data,Measurement error,Smoothing","Chang, Jinyuan@Southwestern University of Finance & Economics - China::Delaigle, Aurore@University of Melbourne::Hall, Peter@University of Melbourne::Tang, Chengyong@Pennsylvania Commonwealth System of Higher Education (PCSHE)@Temple University"
Bayesian precision and covariance matrix estimation for graphical Gaussian models with edge and vertex symmetries,BIOMETRIKA,17,"Graphical Gaussian models with edge and vertex symmetries were introduced by Hojsgaard & Lauritzen (2008), who gave an algorithm for computing the maximum likelihood estimate of the precision matrix for such models. In this paper, we take a Bayesian approach to its estimation. We consider only models with symmetry constraints and which thus form a natural exponential family with the precision matrix as the canonical parameter. We identify the Diaconis-Ylvisaker conjugate prior for these models, develop a scheme to sample from the prior and posterior distributions, and thus obtain estimates of the posterior mean of the precision and covariance matrices. Such a sampling scheme is essential for model selection in coloured graphical Gaussian models. In order to verify the precision of our estimates, we derive an analytic expression for the expected value of the precision matrix when the graph underlying our model is a tree, a complete graph on three vertices, or a decomposable graph on four vertices with various symmetries, and we compare our estimates with the posterior mean of the precision matrix and the expected mean of the coloured graphical Gaussian model, that is, of the covariance matrix. We also verify the accuracy of our estimates on simulated data.","Coloured graph,Conditional independence,Conjugate prior,Covariance estimation,Precision matrix,Symmetry","Massam, H.@York University - Canada::Li, Q.@Sun Yat Sen University::Gao, X.@York University - Canada"
On the number of principal components in high dimensions,BIOMETRIKA,36,"We consider how many components to retain in principal component analysis when the dimension is much higher than the number of observations. To estimate the number of components, we propose to sequentially test skewness of the squared lengths of residual scores that are obtained by removing leading principal components. The residual lengths are asymptotically left-skewed if all principal components with diverging variances are removed, and right-skewed otherwise. The proposed estimator is shown to be consistent, performs well in high-dimensional simulation studies, and provides reasonable estimates in examples.","High-dimensional data,Principal component analysis,Skewness test","Jung, Sungkyu@Pennsylvania Commonwealth System of Higher Education (PCSHE)@University of Pittsburgh::Lee, Myung Hee@Cornell University::Ahn, Jeongyoun@University System of Georgia@University of Georgia"
Semiparametric regression analysis for composite endpoints subject to componentwise censoring,BIOMETRIKA,14,"Composite endpoints with censored data are commonly used as study outcomes in clinical trials. For example, progression-free survival is a widely used composite endpoint, with disease progression and death as the two components. Progression-free survival time is often defined as the time from randomization to the earlier occurrence of disease progression or death from any cause. The censoring times of the two components could be different for patients not experiencing the endpoint event. Conventional approaches, such as taking the minimum of the censoring times of the two components as the censoring time for progression-free survival time, may suffer from efficiency loss and could produce biased estimates of the treatment effect. We propose a new likelihood-based approach that decomposes the endpoints and models both the progression-free survival time and the time from disease progression to death. The censoring times for different components are distinguished. The approach makes full use of available information and provides a direct and improved estimate of the treatment effect on progression-free survival time. Simulations demonstrate that the proposed method outperforms several other approaches and is robust against various model misspecifications. An application to a prostate cancer clinical trial is provided.","Composite endpoint,Hazard ratio,Progression-free survival,Semiparametric model","Diao, Guoqing@George Mason University::Zeng, Donglin@University of North Carolina@University of North Carolina Chapel Hill::Ke, Chunlei@Amgen::Ma, Haijun@Amgen::Jiang, Qi@Amgen::Ibrahim, Joseph G.@University of North Carolina@University of North Carolina Chapel Hill"
Design-based maps for continuous spatial populations,BIOMETRIKA,19,"We analyse the estimation of values of a survey variable throughout a continuum of points in a study area when a sample of points is selected by a probabilistic sampling scheme. At each point, the value is estimated using an inverse distance weighting interpolator, and maps of the survey variable can be obtained. We investigate the design-based asymptotic properties of the interpolator when the study area remains fixed and the number of sampled points approaches infinity, and we derive conditions ensuring design-based asymptotic unbiasedness and consistency. The conditions essentially require the existence of a pointwise or uniformly continuous function describing the behaviour of the survey variable and the use of spatially balanced designs to select points. Finally, we propose a computationally simple mean squared error estimator.","Design consistency,Sampling,Spatial interpolation","Fattorini, L.@University of Siena::Marcheselli, M.@University of Siena::Pisani, C.@University of Siena::Pratelli, L.@Unknow"
Theoretical limits of microclustering for record linkage,BIOMETRIKA,35,"There has been substantial recent interest in record linkage, where one attempts to group the records pertaining to the same entities from one or more large databases that lack unique identifiers. This can be viewed as a type of microclustering, with few observations per cluster and a very large number of clusters. We show that the problem is fundamentally hard from a theoretical perspective and, even in idealized cases, accurate entity resolution is effectively impossible unless the number of entities is small relative to the number of records and/or the separation between records from different entities is extremely large. These results suggest conservatism in interpretation of the results of record linkage, support collection of additional data to more accurately disambiguate the entities, and motivate a focus on coarser inference. For example, results from a simulation study suggest that sometimes one may obtain accurate results for population size estimation even when fine-scale entity resolution is inaccurate.","Closed population estimation,Clustering,Entity resolution,Microclustering,Record linkage,Small clusters","Johndrow, J. E.@Stanford University::Lum, K.@Unknow::Dunson, D. B.@Duke University"
A non-model-based approach to bandwidth selection for kernel estimators of spatial intensity functions,BIOMETRIKA,26,"We propose a new bandwidth selection method for kernel estimators of spatial point process intensity functions. The method is based on an optimality criterion motivated by the Campbell formula applied to the reciprocal intensity function. The new method is fully nonparametric, does not require knowledge of higher-order moments, and is not restricted to a specific class of point process. Our approach is computationally straightforward and does not require numerical approximation of integrals.","Bandwidth selection,Campbell formula,Intensity function,Kernel estimation,Point process","Cronie, O.@Umea University::van Lieshout, M. N. M.@University of Twente"
Bootstrapping volatility functionals: a local and nonparametric perspective,BIOMETRIKA,14,"Volatility functionals are widely used in financial econometrics. In the literature, they are estimated with realized volatility functionals using high-frequency data. In this paper we introduce a nonparametric local bootstrap method that resamples the high-frequency returns with replacement in local windows shrinking to zero. While the block bootstrap in time series (Hall et al., 1995) aims to reduce correlation, the local bootstrap is intended to eliminate the heterogeneity of volatility. We prove that the local bootstrap distribution of the studentized realized volatility functional is first-order accurate. We present Edgeworth expansions of the studentized realized variance with and without local bootstrapping in the absence of the leverage effect and jumps. The expansions show that the local bootstrap distribution of the studentized realized variance is second-order accurate. Extensive simulation studies verify the theory.","Bootstrap,Ito process,Realized volatility functional","Kong, Xin-Bing@Nanjing Audit University::Xu, Shao-Jun@Shanghai University of Finance & Economics::Zhou, Wang@National University of Singapore"
On the connection between maximin distance designs and orthogonal designs,BIOMETRIKA,21,Maximin distance designs and orthogonal designs are widely used in computer and physical experiments. We characterize a broad class of maximin distance designs by establishing new bounds on the minimum intersite distance for mirror-symmetric and general U-type designs. We showthat maximin distance designs and orthogonal designs are closely related and coincide under some conditions.,"Computer experiment,Latin hypercube design,Mirror-symmetric design,Regular cross-polytope,U-type design","Wang, Yaping@Peking University::Yang, Jianfeng@Nankai University::Xu, Hongquan@University of California Los Angeles@University of California System"
Asymptotic inference of causal effects with observational studies trimmed by the estimated propensity scores,BIOMETRIKA,28,"Causal inference with observational studies often relies on the assumptions of unconfoundedness and overlap of covariate distributions in different treatment groups. The overlap assumption is violated when some units have propensity scores close to 0 or 1, so both practical and theoretical researchers suggest dropping units with extreme estimated propensity scores. However, existing trimming methods often do not incorporate the uncertainty in this design stage and restrict inference to only the trimmed sample, due to the nonsmoothness of the trimming. We propose a smooth weighting, which approximates sample trimming and has better asymptotic properties. An advantage of our estimator is its asymptotic linearity, which ensures that the bootstrap can be used to make inference for the target population, incorporating uncertainty arising from both design and analysis stages. We extend the theory to the average treatment effect on the treated, suggesting trimming samples with estimated propensity scores close to 1.","Bootstrap,Limited overlap,Nonsmooth estimator,Potential outcome,Unconfoundedness","Yang, S.@North Carolina State University@University of North Carolina::Ding, P.@University of California System@University of California Berkeley"
Uniformly minimum variance conditionally unbiased estimation in multi-arm multi-stage clinical trials,BIOMETRIKA,23,"Multi-arm multi-stage clinical trials compare several experimental treatments with a control treatment, with poorly performing treatments dropped at interim analyses. This leads to inferential challenges, including the construction of unbiased treatment effect estimators. A number of estimators which are unbiased conditional on treatment selection have been proposed, but are specific to certain selection rules, may ignore the comparison to the control and are not all minimum variance. We obtain estimators for treatment effects compared to the control that are uniformly minimum variance unbiased conditional on selection with any specified rule or stopping for futility.","Adaptive seamless design,Drop-the-loser design,Point estimation,Treatment selection","Stallard, Nigel@University of Warwick::Kimani, Peter K.@University of Warwick"
"Asymptotic properties of penalized spline estimators (vol 96, pg 529, 2009)",BIOMETRIKA,1,no abstract,,"Claeskens, G.@Unknow::Krivobokova, T.@Unknow::Opsomer, J. D.@Unknow"
On edge correction of conditional and intrinsic autoregressions,BIOMETRIKA,26,"This paper discusses edge correction for a large class of conditional and intrinsic autoregressions on two-dimensional finite regular arrays. The proposed method includes a novel reparameterization, retains the simple neighbourhood structure, ensures the nonnegative definiteness of the precision matrix, and enables scalable matrix-free statistical computation. The edge correction provides new insight into how higher-order differencing enters into the precision matrix of a conditional autoregression.","Discrete cosine transform,Gaussian Markov random field,Matrix-free computation,Positive polynomial,Sparse precision matrix,Spatial statistics","Mondal, D.@Oregon State University"
Scalar-on-image regression via the soft-thresholded Gaussian process,BIOMETRIKA,31,"This work concerns spatial variable selection for scalar-on-image regression. We propose a new class of Bayesian nonparametric models and develop an efficient posterior computational algorithm. The proposed soft-thresholded Gaussian process provides large prior support over the class of piecewise-smooth, sparse, and continuous spatially varying regression coefficient functions. In addition, under some mild regularity conditions the soft-thresholded Gaussian process prior leads to the posterior consistency for parameter estimation and variable selection for scalar-on-image regression, even when the number of predictors is larger than the sample size. The proposed method is compared to alternatives via simulation and applied to an electroencephalography study of alcoholism.","Electroencephalography,Gaussian process,Posterior consistency,Spatial variable selection","Kang, Jian@University of Michigan System@University of Michigan::Reich, Brian J.@North Carolina State University@University of North Carolina::Staicu, Ana-Maria@North Carolina State University@University of North Carolina"
A structural Markov property for decomposable graph laws that allows control of clique intersections,BIOMETRIKA,10,"We present a new kind of structural Markov property for probabilistic laws on decomposable graphs, which allows the explicit control of interactions between cliques and so is capable of encoding some interesting structure. We prove the equivalence of this property to an exponential family assumption, and discuss identifiability, modelling, inferential and computational implications.","Conditional independence,Graphical model,Hub model,Markov random field,Model determination,Random graph","Green, Peter J.@University of Technology Sydney::Thomas, Alun@Utah System of Higher Education@University of Utah"
Approximate Bayesian inference under informative sampling,BIOMETRIKA,31,"Statistical inference with complex survey data is challenging because the sampling design can be informative, and ignoring it can produce misleading results. Current methods of Bayesian inference under complex sampling assume that the sampling design is noninformative for the specified model. In this paper, we propose a Bayesian approach which uses the sampling distribution of a summary statistic to derive the posterior distribution of the parameters of interest. Asymptotic properties of the method are investigated. It is directly applicable to combining information from two independent surveys and to calibration estimation in survey sampling. A simulation study confirms that it can provide valid estimation under informative sampling. We apply it to a measurement error problem using data from the Korean Longitudinal Study of Aging.","Analytic inference,Complex sampling,Predictive inference,Pseudolikelihood","Wang, Z.@Iowa State University::Kim, J. K.@Iowa State University::Yang, S.@North Carolina State University@University of North Carolina"
Kernel-based covariate functional balancing for observational studies,BIOMETRIKA,31,"Covariate balance is often advocated for objective causal inference since it mimics randomization in observational data. Unlike methods that balance specific moments of covariates, our proposal attains uniform approximate balance for covariate functions in a reproducing-kernel Hilbert space. The corresponding infinite-dimensional optimization problem is shown to have a finite-dimensional representation in terms of an eigenvalue optimization problem. Large-sample results are studied, and numerical examples show that the proposed method achieves better balance with smaller sampling variability than existing methods.","Average treatment effect,Eigenvalue optimization,Reproducing-kernel Hilbert space,Sobolev space","Wong, Raymond K. W.@Texas A&M University College Station@Texas A&M University System::Chan, Kwun Chuen Gary@University of Washington@University of Washington Seattle"
Targeted learning ensembles for optimal individualized treatment rules with time-to-event outcomes,BIOMETRIKA,36,"We consider estimation of an optimal individualized treatment rule when a high-dimensional vector of baseline variables is available. Our optimality criterion is with respect to delaying the expected time to occurrence of an event of interest. We use semiparametric efficiency theory to construct estimators with properties such as double robustness. We propose two estimators of the optimal rule, which arise from considering two loss functions aimed at directly estimating the conditional treatment effect and recasting the problem in terms of weighted classification using the 0-1 loss function. Our estimated rules are ensembles that minimize the crossvalidated risk of a linear combination in a user-supplied library of candidate estimators. We prove oracle inequalities bounding the finite-sample excess risk of the estimator. The bounds depend on the excess risk of the oracle selector and a doubly robust term related to estimation of the nuisance parameters. We discuss the convergence rates of our estimator to the oracle selector, and illustrate our methods by analysis of a phase III randomized study testing the efficacy of a new therapy for the treatment of breast cancer.","Double robustness,Individualized treatment rule,Oracle inequality,Super-learning","Diaz, I.@Cornell University::Savenkov, O.@Cornell University::Ballman, K.@Cornell University"
A semiparametric extension of the stochastic block model for longitudinal networks,BIOMETRIKA,45,"We propose an extension of the stochastic block model for recurrent interaction events in continuous time, where every individual belongs to a latent group and conditional interactions between two individuals follow an inhomogeneous Poisson process with intensity driven by the individuals' latent groups. We show that the model is identifiable and estimate it with a semiparametric variational expectation-maximization algorithm. We develop two versions of the method, one using a nonparametric histogram approach with an adaptive choice of the partition size, and the other using kernel intensity estimators. We select the number of latent groups by an integrated classification likelihood criterion. We demonstrate the performance of our procedure on synthetic experiments, analyse two datasets to illustrate the utility of our approach, and comment on competing methods.","Dynamic interaction,Expectation-maximization algorithm,Link stream,Longitudinal network,Semiparametric model,Temporal network,Variational approximation","Matias, C.@Sorbonne Universite@Centre National de la Recherche Scientifique (CNRS)::Rebafka, T.@Sorbonne Universite::Villers, F.@Sorbonne Universite"
Assessing replicability of findings across two studies of multiple features,BIOMETRIKA,29,"Replicability analysis aims to identify the overlapping signals across independent studies that examine the same features. For this purpose we develop hypothesis testing procedures that first select the promising features from each of two studies separately. Only those features selected in both studies are then tested. The proposed procedures have theoretical guarantees regarding their control of the familywise error rate or false discovery rate on the replicability claims. They can also be used for signal discovery in each study separately, with the desired error control. Their power for detecting truly replicable findings is compared to alternatives. We illustrate the procedures on behavioural genetics data.","Adaptive procedure,False discovery rate,Familywise error rate,Meta-analysis,Multiple testing,Replicability analysis","Bogomolov, Marina@Technion Israel Institute of Technology::Heller, Ruth@Tel Aviv University"
When is the first spurious variable selected by sequential regression procedures?,BIOMETRIKA,23,"Applied statisticians use sequential regression procedures to rank explanatory variables and, in settings of low correlations between variables and strong true effect sizes, expect that variables at the top of this ranking are truly relevant to the response. In a regime of certain sparsity levels, however, we show that the lasso, forward stepwise regression, and least angle regression include the first spurious variable unexpectedly early. We derive a sharp prediction of the rank of the first spurious variable for these three procedures, demonstrating that it occurs earlier and earlier as the regression coefficients become denser. This phenomenon persists for statistically independent Gaussian random designs and arbitrarily large true effects. We gain insight by identifying the underlying cause and then introduce a simple visualization tool termed the double-ranking diagram to improve on these methods. We obtain the first result establishing the exact equivalence between the lasso and least angle regression in the early stages of solution paths beyond orthogonal designs. This equivalence implies that many important model selection results concerning the lasso can be carried over to least angle regression.","False variable,Familywise error rate,Forward stepwise regression,Lasso,Least angle regression","Su, Weijie J.@University of Pennsylvania"
Asymptotic normality of interpoint distances for high-dimensional data with applications to the two-sample problem,BIOMETRIKA,24,"Interpoint distances have applications in many areas of probability and statistics. Thanks to their simplicity of computation, interpoint distance-based procedures are particularly appealing for analysing small samples of high-dimensional data. In this paper, we first study the asymptotic distribution of interpoint distances in the high-dimension, low-sample-size setting and show that it is normal under regularity conditions. We then construct a powerful test for the two-sample problem, which is consistent for detecting location and scale differences. Simulations show that the test compares favourably with existing distance-based tests.","Asymptotic normality,High-dimensional data,Interpoint distance,Strong mixing condition,Two-sample problem","Li, Jun@University of California Riverside@University of California System"
Shrinking characteristics of precision matrix estimators,BIOMETRIKA,32,"We propose a framework to shrink a user-specified characteristic of a precision matrix estimator that is needed to fit a predictive model. Estimators in our framework minimize the Gaussian negative loglikelihood plus an L-1 penalty on a linear or affine function evaluated at the optimization variable corresponding to the precision matrix. We establish convergence rate bounds for these estimators and propose an alternating direction method of multipliers algorithm for their computation. Our simulation studies show that our estimators can perform better than competitors when they are used to fit predictive models. In particular, we illustrate cases where our precision matrix estimators perform worse at estimating the population precision matrix but better at prediction.","Alternating direction method of multipliers,Linear discriminant analysis,Majorize-minimize,Precision matrix estimation,Prediction","Molstad, Aaron J.@Fred Hutchinson Cancer Center::Rothman, Adam J.@University of Minnesota Twin Cities@University of Minnesota System"
High-dimensional peaks-over-threshold inference,BIOMETRIKA,44,"Max-stable processes are increasingly widely used for modelling complex extreme events, but existing fitting methods are computationally demanding, limiting applications to a few dozen variables. r-Pareto processes are mathematically simpler and have the potential advantage of incorporating all relevant extreme events, by generalizing the notion of a univariate exceedance. In this paper we investigate the use of proper scoring rules for high-dimensional peaks-overthreshold inference, focusing on extreme-value processes associated with log-Gaussian random functions, and compare gradient score estimators with the spectral and censored likelihood estimators for regularly varying distributions with normalized marginals, using data with several hundred locations. When simulating from the true model, the spectral estimator performs best, closely followed by the gradient score estimator, but censored likelihood estimation performs better with simulations from the domain of attraction, though it is outperformed by the gradient score in cases of weak extremal dependence. We illustrate the potential and flexibility of our ideas by modelling extreme rainfall on a grid with 3600 locations, based on exceedances for locally intense and for spatially accumulated rainfall, and discuss diagnostics of model fit. The differences between the two fitted models highlight how the definition of rare events affects the estimated dependence structure.","Functional regular variation,Gradient score,Pareto process,Peaks-over-threshold analysis,Proper scoring rule,Statistics of extremes","de Fondeville, R.@Ecole Polytechnique Federale de Lausanne::Davison, A. C.@Ecole Polytechnique Federale de Lausanne"
A structural break test for extremal dependence in beta-mixing random vectors,BIOMETRIKA,36,"We derive a structural break test for extremal dependence in beta-mixing, possibly high-dimensional random vectors with either asymptotically dependent or asymptotically independent components. Existing tests require serially independent observations with asymptotically dependent components. To avoid estimating a long-run variance, we use self-normalization, which obviates the need to estimate the coefficient of tail dependence when components are asymptotically independent. Simulations show favourable empirical size and power of the test, which we apply to S&P 500 and DAX log-returns. We find evidence for one break in the coefficient of tail dependence for the upper and lower joint tail at the beginning of the 2007-08 financial crisis, leading to more extremal co-movement.","beta-mixing,Extremal dependence,Self-normalization,Structural break test","Hoga, Y.@University of Duisburg Essen"
Asymptotic post-selection inference for the Akaike information criterion,BIOMETRIKA,39,"Ignoring the model selection step in inference after selection is harmful. In this paper we study the asymptotic distribution of estimators after model selection using the Akaike information criterion. First, we consider the classical setting in which a true model exists and is included in the candidate set of models. We exploit the overselection property of this criterion in constructing a selection region, and we obtain the asymptotic distribution of estimators and linear combinations thereof conditional on the selected model. The limiting distribution depends on the set of competitive models and on the smallest overparameterized model. Second, we relax the assumption on the existence of a true model and obtain uniform asymptotic results. We use simulation to study the resulting post-selection distributions and to calculate confidence regions for the model parameters, and we also apply the method to a diabetes dataset.","Akaike information criterion,Confidence region,Likelihood model,Model selection,Post-selection inference","Charkhi, Ali@KU Leuven::Claeskens, Gerda@KU Leuven"
On Bayes factors for the linear model,BIOMETRIKA,13,"We show that the Bayes factor for testing whether a subset of coefficients are zero in the normal linear regression model gives the uniformly most powerful test amongst the class of invariant tests discussed in Lehmann & Romano (2005) if the prior distributions for the regression coefficients are in a specific class of distributions. The priors in this class can have any elliptical distribution, with a specific scale matrix, for the subset of coefficients that are being tested. We also show under mild conditions that the Bayes factor gives the uniformly most powerful invariant test only if the prior for the coefficients being tested is an elliptical distribution with this scale matrix. The implications are discussed.","F-distribution,Monotone function,Test statistic,Uniformly most powerful invariant test","Shively, T. S.@University of Texas Austin@University of Texas System::Walker, S. G.@University of Texas Austin@University of Texas System"
Sequential rerandomization,BIOMETRIKA,26,"The seminal work of Morgan & Rubin (2012) considers rerandomization for all the units at one time. In practice, however, experimenters may have to rerandomize units sequentially. For example, a clinician studying a rare disease may be unable to wait to perform an experiment until all the experimental units are recruited. Our work offers a mathematical framework for sequential rerandomization designs, where the experimental units are enrolled in groups. We formulate an adaptive rerandomization procedure for balancing treatment/control assignments over some continuous or binary covariates, using Mahalanobis distance as the imbalance measure. We prove in our key result that given the same number of rerandomizations, in expected value, under certain mild assumptions, sequential rerandomization achieves better covariate balance than rerandomization at one time.","Experimental design,Mahalanobis distance,Noncentral chi-squared distribution,Sequential enrolment","Zhou, Quan@Rice University::Ernst, Philip A.@Rice University::Morgan, Kari Lock@Penn State University@Pennsylvania Commonwealth System of Higher Education (PCSHE)::Rubin, Donald B.@Harvard University::Zhang, Anru@University of Wisconsin System@University of Wisconsin Madison"
"Information-theoretic optimality of observation-driven time series models for continuous responses (vol 102, pg 325, 2015)",BIOMETRIKA,1,no abstract,,"Blasques, F.@Unknow::Koopman, S. J.@Unknow::Lucas, A.@Unknow"
Symmetric rank covariances: a generalized framework for nonparametric measures of dependence,BIOMETRIKA,28,"The need to test whether two random vectors are independent has spawned many competing measures of dependence. We focus on nonparametric measures that are invariant under strictly increasing transformations, such as Kendall's tau, Hoeffding's D, and the Bergsma-Dassios sign covariance. Each exhibits symmetries that are not readily apparent from their definitions. Making these symmetries explicit, we define a new class of multivariate nonparametric measures of dependence that we call symmetric rank covariances. This new class generalizes the above measures and leads naturally to multivariate extensions of the Bergsma-Dassios sign covariance. Symmetric rank covariances may be estimated unbiasedly using U-statistics, for which we prove results on computational efficiency and large-sample behaviour. The algorithms we develop for their computation include, to the best of our knowledge, the first efficient algorithms for Hoeffding's D statistic in the multivariate setting.","Dependence,Hoeffding's D,Independence testing,Kendall's tau,U-statistic","Weihs, L.@University of Washington@University of Washington Seattle::Drton, M.@University of Washington@University of Washington Seattle::Meinshausen, N.@ETH Zurich"
Local polynomial regression with correlated errors in random design and unknown correlation structure,BIOMETRIKA,18,"Automated or data-driven bandwidth selection methods tend to break down in the presence of correlated errors. While this problem has previously been studied in the fixed design setting for kernel regression, the results were applicable only when there is knowledge about the correlation structure. This article generalizes these results to the random design setting and addresses the problem in situations where no prior knowledge about the correlation structure is available. We establish the asymptotic optimality of our proposed bandwidth selection criterion based on kernels K satisfying K(0) = 0.","Autocorrelation,Correlated errors,Crossvalidation,Local polynomial regression","De Brabanter, K.@Iowa State University::Cao, F.@Iowa State University::Gijbels, I.@KU Leuven::Opsomer, J.@Colorado State University"
Bayesian spatial monotonic multiple regression,BIOMETRIKA,47,"We consider monotonic, multiple regression for contiguous regions. The regression functions vary regionally and may exhibit spatial structure. We develop Bayesian nonparametric methodology that permits estimation of both continuous and discontinuous functional shapes using marked point process and reversible jump Markov chain Monte Carlo techniques. Spatial dependence is incorporated by a flexible prior distribution which is tuned using crossvalidation and Bayesian optimization. We derive the mean and variance of the prior induced by the marked point process approach. Asymptotic results show consistency of the estimated functions. Posterior realizations enable variable selection, the detection of discontinuities and prediction. In simulations and in an application to a Norwegian insurance dataset, our method shows better performance than existing approaches.","Crossvalidation,Isotonic regression,Marked point process,Optimization,Reversible jump Markov chain Monte Carlo algorithm,Spatial dependence","Rohrbeck, C.@Lancaster University::Costain, D. A.@Lancaster University::Frigessi, A.@University of Oslo"
Joint testing and false discovery rate control in high-dimensional multivariate regression,BIOMETRIKA,45,"Multivariate regression with high-dimensional covariates has many applications in genomic and genetic research, in which some covariates are expected to be associated with multiple responses. This paper considers joint testing for regression coefficients over multiple responses and develops simultaneous testing methods with false discovery rate control. The test statistic is based on inverse regression and bias-corrected group lasso estimates of the regression coefficients and is shown to have an asymptotic chi-squared null distribution. A row-wise multiple testing procedure is developed to identify the covariates associated with the responses. The procedure is shown to control the false discovery proportion and false discovery rate at a prespecified level asymptotically. Simulations demonstrate the gain in power, relative to entrywise testing, in detecting the covariates associated with the responses. The test is applied to an ovarian cancer dataset to identify the microRNA regulators that regulate protein expression.","Bias-corrected group lasso,Error rate control,Multiple phenotypes,Row-wise multiple testing","Xia, Yin@Fudan University::Cai, T. Tony@University of Pennsylvania::Li, Hongzhe@University of Pennsylvania"
Optimal pseudolikelihood estimation in the analysis of multivariate missing data with nonignorable nonresponse,BIOMETRIKA,27,"Tang et al. (2003) considered a regression model with missing response, where the missingness mechanism depends on the value of the response variable and hence is nonignorable. They proposed three pseudolikelihood estimators, based on different treatments of the probability distribution of the completely observed covariates. The first assumes the distribution of the covariate to be known, the second estimates this distribution parametrically, and the third estimates the distribution nonparametrically. While it is not hard to show that the second estimator is more efficient than the first, Tang et al. (2003) only conjectured that the third estimator is more efficient than the first two. In this paper, we investigate the asymptotic behaviour of the third estimator by deriving a closed-form representation of its asymptotic variance. We then prove that the third estimator is more efficient than the other two. Our result can be straightforwardly applied to missingness mechanisms that are more general than that in Tang et al. (2003).","Efficiency,Missing data,Nonignorable nonresponse,Pseudolikelihood estimator","Zhao, Jiwei@State University of New York (SUNY) System@State University of New York (SUNY) Buffalo::Ma, Yanyuan@Penn State University@Pennsylvania Commonwealth System of Higher Education (PCSHE)"
Robust estimation of high-dimensional covariance and precision matrices,BIOMETRIKA,32,"High-dimensional data are often most plausibly generated from distributions with complex structure and leptokurtosis in some or all components. Covariance and precision matrices provide a useful summary of such structure, yet the performance of popular matrix estimators typically hinges upon a sub-Gaussianity assumption. This paper presents robust matrix estimators whose performance is guaranteed for a much richer class of distributions. The proposed estimators, under a bounded fourth moment assumption, achieve the same minimax convergence rates as do existing methods under a sub-Gaussianity assumption. Consistency of the proposed estimators is also established under the weak assumption of bounded 2 + epsilon moments for epsilon is an element of (0, 2). The associated convergence rates depend on epsilon.","Constrained l(1)-minimization,Leptokurtosis,Minimax rate,Robustness,Thresholding","Avella-Medina, Marco@Massachusetts Institute of Technology (MIT)::Battey, Heather S.@Imperial College London::Fan, Jianqing@Princeton University::Li, Quefeng@University of North Carolina@University of North Carolina Chapel Hill"
On the asymptotic efficiency of approximate Bayesian computation estimators,BIOMETRIKA,29,"Many statistical applications involve models for which it is difficult to evaluate the likelihood, but from which it is relatively easy to sample. Approximate Bayesian computation is a likelihood-free method for implementing Bayesian inference in such cases. We present results on the asymptotic variance of estimators obtained using approximate Bayesian computation in a large data limit. Our key assumption is that the data are summarized by a fixed-dimensional summary statistic that obeys a central limit theorem. We prove asymptotic normality of the mean of the approximate Bayesian computation posterior. This result also shows that, in terms of asymptotic variance, we should use a summary statistic that is of the same dimension as the parameter vector, p, and that any summary statistic of higher dimension can be reduced, through a linear transformation, to dimension p in a way that can only reduce the asymptotic variance of the posterior mean. We look at how the Monte Carlo error of an importance sampling algorithm that samples from the approximate Bayesian computation posterior affects the accuracy of estimators. We give conditions on the importance sampling proposal distribution such that the variance of the estimator will be of the same order as that of the maximum likelihood estimator based on the summary statistics used. This suggests an iterative importance sampling algorithm, which we evaluate empirically on a stochastic volatility model.","Approximate Bayesian computation,Dimension reduction,Importance sampling,Partial information,Proposal distribution","Li, Wentao@Newcastle University - UK::Fearnhead, Paul@Lancaster University"
Constructing dynamic treatment regimes over indefinite time horizons,BIOMETRIKA,55,"Existing methods for estimating optimal dynamic treatment regimes are limited to cases where a utility function is optimized over a fixed time period. We develop an estimation procedure for the optimal dynamic treatment regime over an indefinite time period and derive associated largesample results. The proposed method can be used to estimate the optimal dynamic treatment regime in chronic disease settings. We illustrate this by simulating a dataset corresponding to a cohort of patients with diabetes that mimics the third wave of the National Health and Nutrition Examination Survey, and examining the performance of the proposed method in controlling the level of haemoglobin A1c.","Action-value function,Backward induction,Causal inference,Temporal difference residual","Ertefaie, Ashkan@University of Rochester::Strawderman, Robert L.@University of Rochester"
A Durbin-Levinson regularized estimator of high-dimensional autocovariance matrices,BIOMETRIKA,27,"The autocovariance matrix of a stationary random process plays a central role in prediction theory and time series analysis. When the dimension of the matrix is of the same order of magnitude as the number of observations, the sample autocovariance matrix gives an inconsistent estimator. In the nonparametric framework, recent proposals have concentrated on banding and tapering the sample autocovariance matrix. We introduce an alternative approach via a modified Durbin-Levinson algorithm that receives as input the banded and tapered sample partial autocorrelations and returns a consistent and positive-definite estimator of the autocovariance matrix. We establish the convergence rate of our estimator and characterize the properties of the optimal linear predictor obtained from it. The computational complexity of the latter is of the order of the square of the banding parameter, which renders our method scalable for high-dimensional time series.","Optimal linear prediction,Partial autocorrelation function,Toeplitz system","Proietti, Tommaso@University of Rome Tor Vergata::Giovannelli, Alessandro@University of Rome Tor Vergata"
A convex formulation for high-dimensional sparse sliced inverse regression,BIOMETRIKA,44,"Sliced inverse regression is a popular tool for sufficient dimension reduction, which replaces covariates with a minimal set of their linear combinations without loss of information on the conditional distribution of the response given the covariates. The estimated linear combinations include all covariates, making results difficult to interpret and perhaps unnecessarily variable, particularly when the number of covariates is large. In this paper, we propose a convex formulation for fitting sparse sliced inverse regression in high dimensions. Our proposal estimates the subspace of the linear combinations of the covariates directly and performs variable selection simultaneously. We solve the resulting convex optimization problem via the linearized alternating direction methods of multiplier algorithm, and establish an upper bound on the subspace distance between the estimated and the true subspaces. Through numerical studies, we show that our proposal is able to identify the correct covariates in the high-dimensional setting.","Convex optimization,Dimension reduction,Nonparametric regression,Principal fitted component","Tan, Kean Ming@University of Minnesota Twin Cities@University of Minnesota System::Wang, Zhaoran@Northwestern University::Zhang, Tong@Tencent::Liu, Han@Tencent::Cook, R. Dennis@University of Minnesota Twin Cities@University of Minnesota System"
Functional prediction through averaging estimated functional linear regression models,BIOMETRIKA,32,"Prediction is often the primary goal of data analysis. In this work, we propose a novel model averaging approach to the prediction of a functional response variable. We develop a crossvalidation model averaging estimator based on functional linear regression models in which the response and the covariate are both treated as random functions. We show that the weights chosen by the method are asymptotically optimal in the sense that the squared error loss of the predicted function is as small as that of the infeasible best possible averaged function. When the true regression relationship belongs to the set of candidate functional linear regression models, the averaged estimator converges to the true model and can estimate the regression parameter functions at the same rate as under the true model. Monte Carlo studies and a data example indicate that in most cases the approach performs better than model selection.","Asymptotic optimality,Crossvalidation,Functional data,Model averaging,Weighting","Zhang, Xinyu@Chinese Academy of Sciences::Chiou, Jeng-Min@Academia Sinica - Taiwan::Ma, Yanyuan@Penn State University@Pennsylvania Commonwealth System of Higher Education (PCSHE)"
Selective inference with unknown variance via the square-root lasso,BIOMETRIKA,14,"There has been much recent work on inference after model selection in situations where the noise level is known. However, the error variance is rarely known in practice and its estimation is difficult in high-dimensional settings. In this work we propose using the square-root lasso, also known as the scaled lasso, to perform inference for selected coefficients and the noise level simultaneously. The square-root lasso has the property that the choice of a reasonable tuning parameter does not depend on the noise level in the data. We provide valid p-values and confidence intervals for coefficients after variable selection and estimates for the model-specific variance. Our estimators perform better in simulations than other estimators of the noise variance. These results make inference after model selection significantly more applicable.","Confidence interval,Hypothesis testing,Lasso,Model selection,Square-root lasso","Tian, Xiaoying@Unknow::Loftus, Joshua R.@New York University::Taylor, Jonathan E.@Stanford University"
"A test of weak separability for multi-way functional data, with application to brain connectivity studies",BIOMETRIKA,36,"This paper concerns the modelling of multi-way functional data where double or multiple indices are involved. We introduce a concept of weak separability. The weakly separable structure supports the use of factorization methods that decompose the signal into its spatial and temporal components. The analysis reveals interesting connections to the usual strongly separable covariance structure, and provides insights into tensor methods for multi-way functional data. We propose a formal test for the weak separability hypothesis, where the asymptotic null distribution of the test statistic is a chi-squared-type mixture. The method is applied to study brain functional connectivity derived from source localized magnetoencephalography signals during motor tasks.","Asymptotics,Functional principal component,Hypothesis testing,Marginal kernel,Separable covariance,Spatio-temporal data,Tensor product","Lynch, Brian@Pennsylvania Commonwealth System of Higher Education (PCSHE)@University of Pittsburgh::Chen, Kehui@Pennsylvania Commonwealth System of Higher Education (PCSHE)@University of Pittsburgh"
Wild residual bootstrap inference for penalized quantile regression with heteroscedastic errors,BIOMETRIKA,23,"We consider a heteroscedastic regression model in which some of the regression coefficients are zero but it is not known which ones. Penalized quantile regression is a useful approach for analysing such data. By allowing different covariates to be relevant for modelling conditional quantile functions at different quantile levels, it provides a more complete picture of the conditional distribution of a response variable than mean regression. Existing work on penalized quantile regression has been mostly focused on point estimation. Although bootstrap procedures have recently been shown to be effective for inference for penalized mean regression, they are not directly applicable to penalized quantile regression with heteroscedastic errors. We prove that a wild residual bootstrap procedure for unpenalized quantile regression is asymptotically valid for approximating the distribution of a penalized quantile regression estimator with an adaptive L-1 penalty and that a modified version can be used to approximate the distribution of a L-1-penalized quantile regression estimator. The new methods do not require estimation of the unknown error density function. We establish consistency, demonstrate finite-sample performance, and illustrate the applications on a real data example.","Adaptive lasso,Confidence interval,Lasso,Penalized quantile regression,Wild bootstrap","Wang, Lan@University of Minnesota Twin Cities@University of Minnesota System::Van Keilegom, Ingrid@KU Leuven::Maidman, Adam@University of Minnesota Twin Cities@University of Minnesota System"
A bootstrap recipe for post-model-selection inference under linear regression models,BIOMETRIKA,35,"We propose a general bootstrap recipe for estimating the distributions of post-model-selection least squares estimators under a linear regression model. The recipe constrains residual bootstrapping within the most parsimonious, approximately correct, models to yield a distribution estimator which is consistent provided any wrong candidate model is sufficiently separated from the approximately correct ones. Our theory applies to a broad class of model selection methods based on information criteria or sparse estimation. The empirical performance of our procedure is illustrated with simulated data.","Bootstrap,Least squares estimator,Post-model-selection,Regression","Lee, S. M. S.@University of Hong Kong::Wu, Y.@University of Waterloo"
The change-plane Cox model,BIOMETRIKA,14,"We propose a projection pursuit technique in survival analysis for finding lower-dimensional projections that exhibit differentiated survival outcomes. This idea is formally introduced as the change-plane Cox model, a nonregular Cox model with a change-plane in the covariate space that divides the population into two subgroups whose hazards are proportional. The proposed technique offers a potential framework for principled subgroup discovery. Estimation of the change-plane is accomplished via likelihood maximization over a data-driven sieve constructed using sliced inverse regression. Consistency of the sieve procedure for the change-plane parameters is established. In simulations the sieve estimator demonstrates better classification performance for subgroup identification than alternatives.","Latent supervised learning,Projection pursuit,Random projection,Sieve estimation,Sliced inverse regression,Subgroup discovery","Wei, Susan@University of Melbourne::Kosorok, Michael R.@University of North Carolina@University of North Carolina Chapel Hill"
Transforming cumulative hazard estimates,BIOMETRIKA,26,"Time-to-event outcomes are often evaluated on the hazard scale, but interpreting hazards may be difficult. Recently in the causal inference literature concerns have been raised that hazards actually have a built-in selection bias that prevents simple causal interpretations. This is a problem even in randomized controlled trials, where hazard ratios have become a standard measure of treatment effects. Modelling on the hazard scale is nevertheless convenient, for example to adjust for covariates; using hazards for intermediate calculations may therefore be desirable. In this paper we present a generic method for transforming hazard estimates consistently to other scales at which these built-in selection biases are avoided. The method is based on differential equations and generalizes a well-known relation between the Nelson-Aalen and Kaplan-Meier estimators. Using the martingale central limit theorem, we show that covariances can be estimated consistently for a large class of estimators, thus allowing for rapid calculation of confidence intervals. Hence, given cumulative hazard estimates based on, for example, Aalen's additive hazard model, we can obtain many other parameters without much more effort. We give several examples and the associated estimators. Coverage and convergence speed are explored via simulations, and the results suggest that reliable estimates can be obtained in real-life scenarios.","Asymptotics,Delta method,Hazard,Marginal effect measure,Survival analysis","Ryalen, Pal C.@University of Oslo::Stensrud, Mats J.@University of Oslo::Roysland, Kjetil@University of Oslo"
Integrative linear discriminant analysis with guaranteed error rate improvement,BIOMETRIKA,33,"Multiple types of data measured on a common set of subjects arise in many areas. Numerous empirical studies have found that integrative analysis of such data can result in better statistical performance in terms of prediction and feature selection. However, the advantages of integrative analysis have mostly been demonstrated empirically. In the context of two-class classification, we propose an integrative linear discriminant analysis method and establish a theoretical guarantee that it achieves a smaller classification error than running linear discriminant analysis on each data type individually. We address the issues of outliers and missing values, frequently encountered in integrative analysis, and illustrate our method through simulations and a neuroimaging study of Alzheimer's disease.","Bayes error,High-dimensional classification,Integrative analysis,Linear discriminant analysis,Multi-type data,Regularization","Li, Quefeng@University of North Carolina@University of North Carolina Chapel Hill::Li, Lexin@University of California Berkeley@University of California System"
Principal ignorability in mediation analysis: through and beyond sequential ignorability,BIOMETRIKA,17,"In causal mediation analysis, the definitions of the natural direct and indirect effects involve potential outcomes that can never be observed, so-called a priori counterfactuals. This conceptual challenge translates into issues in identification, which requires strong and often unverifiable assumptions, including sequential ignorability. Alternatively, we can deal with post-treatment variables using the principal stratification framework, where causal effects are defined as comparisons of observable potential outcomes. We establish a novel bridge between mediation analysis and principal stratification, which helps to clarify and weaken the commonly used identifying assumptions for natural direct and indirect effects. Using principal stratification, we show how sequential ignorability extrapolates from observable potential outcomes to a priori counterfactuals, and propose alternative weaker principal ignorability-type assumptions. We illustrate the key concepts using a clinical trial.","Causal inference,Identification,Potential outcome,Principal stratification","Forastiere, Laura@University of Florence::Mattei, Alessandra@University of Florence::Ding, Peng@University of California System@University of California Berkeley"
Identifying causal effects with proxy variables of an unmeasured confounder,BIOMETRIKA,23,"We consider a causal effect that is confounded by an unobserved variable, but for which observed proxy variables of the confounder are available. We show that with at least two independent proxy variables satisfying a certain rank condition, the causal effect can be nonparametrically identified, even if the measurement error mechanism, i.e., the conditional distribution of the proxies given the confounder, may not be identified. Our result generalizes the identification strategy of Kuroki & Pearl (2014), which rests on identification of the measurement error mechanism. When only one proxy for the confounder is available, or when the required rank condition is not met, we develop a strategy for testing the null hypothesis of no causal effect.","Confounder,Identification,Measurement error,Negative control,Proxy","Miao, Wang@Peking University::Geng, Zhi@Peking University::Tchetgen, Eric J. Tchetgen@Harvard University"
Regression-assisted inference for the average treatment effect in paired experiments,BIOMETRIKA,16,"In paired randomized experiments, individuals in a given matched pair may differ on prognostically important covariates despite the best efforts of practitioners. We examine the use of regression adjustment to correct for persistent covariate imbalances after randomization, and present two regression-assisted estimators for the sample average treatment effect in paired experiments. Using the potential outcomes framework, we prove that these estimators are consistent for the sample average treatment effect under mild regularity conditions even if the regression model is improperly specified, and describe how asymptotically conservative confidence intervals can be constructed. We demonstrate that the variances of the regressionassisted estimators are no larger than that of the standard difference-in-means estimator asymptotically, and illustrate the proposed methods by simulation. The analysis does not require a superpopulation model, a constant treatment effect, or the truth of the regression model, and hence provides inference for the sample average treatment effect with the potential to increase power without unrealistic assumptions.","Agnostic regression,Average treatment effect,Causal inference,Covariance adjustment,Finite-sample inference,Paired experiments","Fogarty, Colin B.@Massachusetts Institute of Technology (MIT)"
Statistical sparsity,BIOMETRIKA,13,"The main contribution of this paper is a mathematical definition of statistical sparsity, which is expressed as a limiting property of a sequence of probability distributions. The limit is characterized by an exceedance measure H and a rate parameter. > 0, both of which are unrelated to sample size. The definition encompasses all sparsity models that have been suggested in the signal-detection literature. Sparsity implies that. is small, and a sparse approximation is asymptotic in the rate parameter, typically with error o(.) in the sparse limit.. 0. To first order in sparsity, the sparse signal plus Gaussian noise convolution depends on the signal distribution only through its rate parameter and exceedance measure. This is one of several asymptotic approximations implied by the definition, each of which is most conveniently expressed in terms of the zeta transformation of the exceedance measure. One implication is that two sparse families having the same exceedance measure are inferentially equivalent and cannot be distinguished to first order. Thus, aspects of the signal distribution that have a negligible effect on observables can be ignored with impunity, leaving only the exceedance measure to be considered. From this point of view, scale models and inverse-power measures seem particularly attractive.","Convolution,Exceedance measure,False discovery,Infinite divisibility,Levy measure,Post-selection inference,Signal activity,Signal-detection threshold,Tail inflation","McCullagh, Peter@University of Chicago::Polson, Nicholas G.@Unknow"
A test for the absence of aliasing or local white noise in locally stationary wavelet time series,BIOMETRIKA,38,"Aliasing is often overlooked in time series analysis but can seriously distort the spectrum, the autocovariance and their estimates. We show that dyadic subsampling of a locally stationary wavelet process, which can cause aliasing, results in a process that is the sum of asymptotic white noise and another locally stationary wavelet process with a modified spectrum. We develop a test for the absence of aliasing in a locally stationary wavelet series at a fixed location, and illustrate its application on simulated data and a wind energy time series. A useful by-product is a new test for local white noise. The tests are robust with respect to model misspecification in that the analysis and synthesis wavelets do not need to be identical. Hence, in principle, the tests work irrespective of which wavelet is used to analyse the time series, although in practice there is a trade-off between increasing statistical power and time localization of the test.","Aliasing,Local spectrum,Sample rate,Subsampling,White noise,Wind energy","Eckley, I. A.@Lancaster University::Nason, G. P.@University of Bristol"
Model-assisted design of experiments in the presence of network-correlated outcomes,BIOMETRIKA,38,"In this paper we consider how to assign treatment in a randomized experiment in which the correlation among the outcomes is informed by a network available pre-intervention. Working within the potential outcome causal framework, we develop a class of models that posit such a correlation structure among the outcomes. We use these models to develop restricted randomization strategies for allocating treatment optimally, by minimizing the mean squared error of the estimated average treatment effect. Analytical decompositions of the mean squared error, due both to the model and to the randomization distribution, provide insights into aspects of the optimal designs. In particular, the analysis suggests new notions of balance based on specific network quantities, in addition to classical covariate balance. The resulting balanced optimal restricted randomization strategies are still design-unbiased when the model used to derive them does not hold. We illustrate how the proposed treatment allocation strategies improve on allocations that ignore the network structure.","Causal inference,Degree distribution,Network balance,Network data,Optimal treatment allocation,Randomized experiment,Rerandomization","Basse, Guillaume W.@Harvard University::Airoldi, Edoardo M.@Pennsylvania Commonwealth System of Higher Education (PCSHE)@Temple University"
Continuous testing for Poisson process intensities: a new perspective on scanning statistics,BIOMETRIKA,35,"We propose a continuous testing framework to test the intensities of Poisson processes that allows a rigorous definition of the complete testing procedure, from an infinite number of hypotheses to joint error rates. Our work extends procedures based on scanning windows by controlling the familywise error rate and the false discovery rate in a non-asymptotic manner and in a continuous way. We introduce the p-value process on which the decision rule is based. Our method is applied in neuroscience via the standard homogeneity and two-sample tests.","False discovery rate,Familywise error rate,Multiple testing,Poisson process","Picard, Franck@Centre National de la Recherche Scientifique (CNRS)::Reynaud-Bouret, Patricia@Centre National de la Recherche Scientifique (CNRS)::Roquain, Etienne@Sorbonne Universite"
Covariate association eliminating weights: a unified weighting framework for causal effect estimation,BIOMETRIKA,37,"Weighting methods offer an approach to estimating causal treatment effects in observational studies. However, if weights are estimated by maximum likelihood, misspecification of the treatment assignment model can lead to weighted estimators with substantial bias and variance. In this paper, we propose a unified framework for constructing weights such that a set of measured pretreatment covariates is unassociated with treatment assignment after weighting. We derive conditions for weight estimation by eliminating the associations between these covariates and treatment assignment characterized in a chosen treatment assignment model after weighting. The moment conditions in covariate balancing weight methods for binary, categorical and continuous treatments in cross-sectional settings are special cases of the conditions in our framework, which extends to longitudinal settings. Simulation shows that our method gives treatment effect estimates with smaller biases and variances than the maximum likelihood approach under treatment assignment model misspecification. We illustrate our method with an application to systemic lupus erythematosus data.","Causal inference,Confounding,Continuous treatment,Covariate balance,Inverse probability weighting,Propensity function","Yiu, Sean@MRC Biostatistics Unit@University of Cambridge::Su, Li@MRC Biostatistics Unit@University of Cambridge"
Variance estimation in the particle filter,BIOMETRIKA,22,"This paper concerns numerical assessment of Monte Carlo error in particle filters. We show that by keeping track of certain key features of the genealogical structure arising from resampling operations, it is possible to estimate variances of a number of Monte Carlo approximations that particle filters deliver. All our estimators can be computed from a single run of a particle filter. We establish that, as the number of particles grows, our estimators are weakly consistent for asymptotic variances of the Monte Carlo approximations and some of them are also non-asymptotically unbiased. The asymptotic variances can be decomposed into terms corresponding to each time step of the algorithm, and we show how to estimate each of these terms consistently. When the number of particles may vary over time, this allows approximation of the asymptotically optimal allocation of particle numbers.","Allocation,Particle filter,Sequential Monte Carlo simulation,Variance estimation","Lee, A.@University of Bristol::Whiteley, N.@University of Bristol"
